##复习方法
    1、知道那个知识点为了解决什么问题？（使用场景）
    2、然后怎么解决的？（技术原理：过程（一般涉及到设计模式的应用，包含启动过程、生命周期、扩展点等内容）、底层技术细节
        （动态代理技术、字节码修改技术、零拷贝（减少用户态内核态切换）、NIO、多路复用等））
    3、还有哪些场景是无法覆盖的？有其他技术可以替代吗？同问题领域技术优缺点对比

## 0.jdk

###目录
    1.stream
    2.Fork/Join框架
    3.HashMap,ConcurrentHashMap
    4.String   s   =   new   String( "xyz "),创建了几个对象?
      首先在string池内找，找到？不创建string对象，否则创建，   这样就一个string对象 
      遇到new运算符号了，在内存上创建string对象，并将其返回给s，又一个对象 
      所以总共是2个对象 
    5. Java序列化原理
    6. TreeMap 和 LinkedHashMap
        TreeMap取出来的是排序后的键值对。但如果您要按自然顺序或自定义顺序遍历键，那么TreeMap会更好。 LinkedHashMap 是HashMap的一个子类，
        如果需要输出的顺序和输入的相同,那么用LinkedHashMap可以实现,它还可以按读取顺序来排列，像连接池中可以应用。
        LinkedHashMap(线程不安全,需要外部锁定):
            排序模式：
               LinkedHashMap定义了排序模式accessOrder，该属性为boolean型变量，对于访问顺序，为true；对于插入顺序，则为false。
               访问顺序下,配合removeEldestEntry(Map.Entry<K,V> eldest)方法,可以用来实现LRU.
               
                该哈希映射的迭代顺序就是最后访问其条目的顺序，这种映射很适合构建LRU缓存。LinkedHashMap提供了
                removeEldestEntry(Map.Entry<K,V> eldest)方法，在将新条目插入到映射后，put和 putAll将调用此方法。
                该方法可以提供在每次添加新条目时移除最旧条目的实现程序，默认返回false，这样，此映射的行为将类似于正常映射，
                即永远不能移除最旧的元素。
                
                https://www.cnblogs.com/ganchuanpu/p/8908093.html
                
        TreeMap:
            TreeMap实现了SotredMap接口，它是有序的集合。而且是一个红黑树结构，每个key-value都作为一个红黑树的节点。如果在调用TreeMap
            的构造函数时没有指定比较器，则根据key执行自然排序。这点会在接下来的代码中做说明，如果指定了比较器则按照比较器来进行排序。
            
            https://my.oschina.net/90888/blog/1626065
            红黑树:
                红黑树是一个更高效的检索二叉树，有如下特点：
                
                每个节点只能是红色或者黑色
                根节点永远是黑色的
                所有的叶子的子节点都是空节点，并且都是黑色的
                每个红色节点的两个子节点都是黑色的（不会有两个连续的红色节点）
                从任一个节点到其子树中每个叶子节点的路径都包含相同数量的黑色节点（叶子节点到根节点的黑色节点数量每条路径都相同）
        
        LFU实现了解

        
    *7. jdk 1.8 concurrentHashMap为什么去掉分段锁?
        减小锁的粒度,提高并发,如果需要锁,则直接锁单个桶的首节点.
        
        https://www.cnblogs.com/wenbochang/p/8484779.html
    
        https://blog.csdn.net/qiaoqiao0609/article/details/79082860
        我想有基础的同学知道在jdk1.7中是采用Segment + HashEntry + ReentrantLock的方式进行实现的，而1.8中放弃了Segment臃肿的设计，
        取而代之的是采用Node + CAS + Synchronized来保证并发安全进行实现
        在1.8中ConcurrentHashMap的get操作全程不需要加锁，这也是它比其他并发集合比如hashtable、用Collections.synchronizedMap()包装的hashmap;安全效率高的原因之一。
        get操作全程不需要加锁是因为Node的成员val是用volatile修饰的和数组用volatile修饰没有关系。
        数组用volatile修饰主要是保证在数组扩容的时候保证可见性。
        
        1.8 put()
            //计算键所对应的 hash 值
            //如果为空，那么以CAS无锁式向该位置添加一个节点
            //检测到桶结点是 ForwardingNode 类型，协助扩容
            //桶结点是普通的结点，锁住该桶头结点并试图在该链表的尾部添加一个节点
            
        
###结束

    stream
        https://www.cnblogs.com/Dorae/p/7779246.html
        Stream中的操作可以分为两大类：中间操作与结束操作，中间操作只是对操作进行了记录，只有结束操作才会触发实际的计算（即惰性求值），
        这也是Stream在迭代大集合时高效的原因之一。中间操作又可以分为无状态（Stateless）操作与有状态（Stateful）操作，
        前者是指元素的处理不受之前元素的影响；后者是指该操作只有拿到所有元素之后才能继续下去。结束操作又可以分为短路与非短路操作，
        这个应该很好理解，前者是指遇到某些符合条件的元素就可以得到最终结果；而后者是指必须处理所有元素才能得到最终结果。
    
        stream包
            其中各个部分的主要功能为：
            主要是各种操作的工厂类、数据的存储结构以及收集器的工厂类等；
            主要用于Stream的惰性求值实现；
            Stream的并行计算框架；
            存储并行流的中间结果；
            终结操作的定义
        影响并行流的因素
            数据大小；源数据结构（分割越容易越好），arraylist、数组比较好，hashSet、treeSet次之，linked最差；装箱；
            核的数量（可使用）；单元处理开销（越大越好）
            
        https://crossoverjie.top/categories/Java-%E8%BF%9B%E9%98%B6/    (推荐)
        
        
        
    Fork/Join框架详解
        fork/Join框架要完成两件事情：
        　　1.任务分割：首先Fork/Join框架需要把大的任务分割成足够小的子任务，如果子任务比较大的话还要对子任务进行继续分割
        　　2.执行任务并合并结果：分割的子任务分别放到双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果
            都放在另外一个队列里，启动一个线程从队列里取数据，然后合并这些数据。
            
            我们进一步了解ForkJoinTask，ForkJoinTask与一般任务的主要区别在于它需要实现compute方法，在这个方法里，首先需要判断
            任务是否足够小，如果足够小就直接执行任务。如果不足够小，就必须分割成两个子任务，每个子任务在调用fork方法时，又会进入
            compute方法，看看当前子任务是否需要继续分割成子任务，如果不需要继续分割，则执行当前子任务并返回结果。使用join方法会等待
            子任务执行完并得到其结果。
            
            extends RecursiveTask<Integer>
            
        https://www.jianshu.com/p/48dd153577cd  (整体)
        
        1.分割任务
            使用一个fork类来把大任务分割成子任务，当子任务还较大时，不停的分割，直到分割到子任务足够小。(类似于归并排序的分化阶段)
        
        2.执行任务合并结果
            分割后的子任务分别放在双端队列里，然后开启多线程分别从双端队列获取任务执行，子任务执行完的结果都放在一个队列里，然后启动
            一个线程从队列里获取数据，最后合并这些数据。(类似于归并排序的合并阶段)
        
        
        
    
    
    HashMap? ConcurrentHashMap
        HashMap1.7
            put 方法
                判断当前数组是否需要初始化。
                如果 key 为空，则 put 一个空值进去。
                根据 key 计算出 hashcode。
                根据计算出的 hashcode 定位出所在桶。
                如果桶是一个链表则需要遍历判断里面的 hashcode、key 是否和传入 key 相等，如果相等则进行覆盖，并返回原来的值。
                如果桶是空的，说明当前位置没有数据存入；新增一个 Entry 对象写入当前位置。
            get 方法
                首先也是根据 key 计算出 hashcode，然后定位到具体的桶中。
                判断该位置是否为链表。
                不是链表就根据 key、key 的 hashcode 是否相等来返回值。
                为链表则需要遍历直到 key 及 hashcode 相等时候就返回值。
                啥都没取到就直接返回 null 。  
                
        HashMap1.8
            当 Hash 冲突严重时，在桶上形成的链表会变的越来越长，这样在查询时的效率就会越来越低；时间复杂度为 O(N)。
            
            put 方法
                判断当前桶是否为空，空的就需要初始化（resize 中会判断是否进行初始化）。
                根据当前 key 的 hashcode 定位到具体的桶中并判断是否为空，为空表明没有 Hash 冲突就直接在当前位置创建一个新桶即可。
                如果当前桶有值（ Hash 冲突），那么就要比较当前桶中的 key、key 的 hashcode 与写入的 key 是否相等，相等就赋值给 e,在第 8 步的时候会统一进行赋值及返回。
                如果当前桶为红黑树，那就要按照红黑树的方式写入数据。
                如果是个链表，就需要将当前的 key、value 封装成一个新节点写入到当前桶的后面（形成链表）。
                接着判断当前链表的大小是否大于预设的阈值，大于时就要转换为红黑树。
                如果在遍历过程中找到 key 相同时直接退出遍历。
                如果 e != null 就相当于存在相同的 key,那就需要将值覆盖。
                最后判断是否需要进行扩容。 
            get 方法
                首先将 key hash 之后取得所定位的桶。
                如果桶为空则直接返回 null 。
                否则判断桶的第一个位置(有可能是链表、红黑树)的 key 是否为查询的 key，是就直接返回 value。
                如果第一个不匹配，则判断它的下一个是红黑树还是链表。
                红黑树就按照树的查找方式返回值。
                不然就按照链表的方式遍历匹配返回值。
                
            但是 HashMap 原有的问题也都存在，比如在并发场景下使用时容易出现死循环。
                看过上文的还记得在 HashMap 扩容的时候会调用 resize() 方法，就是这里的并发操作容易在一个桶上形成环形链表；
                这样当获取一个不存在的 key 时，计算出的 index 正好是环形链表的下标就会出现死循环。
               
               
        ConcurrentHashMap1.7
            是由 Segment 数组、HashEntry 组成，和 HashMap 一样，仍然是数组加链表。
            ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 
            还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。
            每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。
                
            put 方法
                将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry。
                遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。
                不为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。
                最后会解除在 1 中所获取当前 Segment 的锁。
                
            get 方法
                1.7 已经解决了并发问题，并且能支持 N 个 Segment 这么多次数的并发，但依然存在 HashMap 在 1.7 版本中的问题。
                那就是查询遍历链表效率太低。
                因此 1.8 做了一些数据结构上的调整。
           
           
         ConcurrentHashMap1.8
            其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。
            put 方法
                根据 key 计算出 hashcode 。
                判断是否需要进行初始化。
                f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。
                如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。
                如果都不满足，则利用 synchronized 锁写入数据。
                如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。     
            get 方法
                根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。
                如果是红黑树那就按照树的方式获取值。
                就不满足那就按照链表的方式遍历获取值。
                
            1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 
            synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。
                
    java序列化
         Serialization（序列化）是一种将对象以一连串的字节描述的过程；反序列化deserialization是一种将这些字节重建成一个对象的过程。
         Java序列化API提供一种处理对象序列化的标准机制。在这里你能学到如何序列化一个对象，什么时候需要序列化以及Java序列化的算法，
         我们用一个实例来示范序列化以后的字节是如何描述一个对象的信息的。
        
         Java的序列化算法
         序列化算法一般会按步骤做如下事情：
             ◆将对象实例相关的类元数据输出。
             ◆递归地输出类的超类描述直到不再有超类。
             ◆类元数据完了以后，开始从最顶层的超类开始输出对象实例的实际数据值。
             ◆从上至下递归输出实例的数据
             
         serialVersionUID值的重要作用
               根据上面的分析,可以发现如果一个类可序列化,serialVersionUID建议给一个确定的值,不要由系统自动生成,否则在增减字段
               (不能修改字段类型及长度)时,如果两边的类的版本不同会导致反序列化失败.
    
                
      
## 1.java内存模型

        https://blog.csdn.net/tjiyu/article/details/53915869
        
        浅析java内存模型--JMM(Java Memory Model)
        https://www.cnblogs.com/lewis0077/p/5143268.html    (比较完善)
        
        *主内存和工作内存
        *原子性,可见性,顺序性,happens-before
        *Java的内存结构，也就是运行时的数据区域
            1/PC寄存器/程序计数器   (私有)
                严格来说是一个数据结构，用于保存当前正在执行的程序的内存地址，由于Java是支持多线程执行的，所以程序执行的轨迹不可能一直
                都是线性执行。当有多个线程交叉执行时，被中断的线程的程序当前执行到哪条内存地址必然要保存下来，以便用于被中断的线程恢复
                执行时再按照被中断时的指令地址继续执行下去。为了线程切换后能恢复到正确的执行位置，每个线程都需要有一个独立的程序计数器，
                各个线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存,这在某种程度上有点类似于“ThreadLocal”，
                是线程安全的。

            2/Java栈 Java Stack  (私有)
                由于Java栈是与线程对应起来的，Java栈数据不是线程共有的，所以不需要关心其数据一致性，也不会存在同步锁的问题。
            3/堆 Heap     (共享)
                堆是JVM所管理的内存中国最大的一块，是被所有Java线程锁共享的，不是线程安全的，在JVM启动时创建
            4/方法区Method Area    (共享)
                方法区是被Java线程锁共享的.不像Java堆中其他部分一样会频繁被GC回收，它存储的信息相对比较稳定，在一定条件下会被GC，
                当方法区要使用的内存超过其允许的大小时，会抛出OutOfMemory的错误信息。方法区也是堆中的一部分，就是我们通常所说的
                Java堆中的永久区 Permanet Generation，大小可以通过参数来设置,可以通过-XX:PermSize指定初始值，
                -XX:MaxPermSize指定最大值。
            5/常量池Constant Pool  (共享)
                常量池本身是方法区中的一个数据结构。常量池中存储了如字符串、final变量值、类名和方法名常量。常量池在编译期间就被确定，
                并保存在已编译的.class文件中。一般分为两类：字面量和应用量。字面量就是字符串、final变量等。类名和方法名属于引用量。
                引用量最常见的是在调用方法的时候，根据方法名找到方法的引用，并以此定为到函数体进行函数代码的执行。引用量包含：类和接口
                的权限定名、字段的名称和描述符，方法的名称和描述符。
            6/本地方法栈Native Method Stack  (native方法使用)
                本地方法栈和Java栈所发挥的作用非常相似，区别不过是Java栈为JVM执行Java方法服务，而本地方法栈为JVM执行
                Native方法服务。本地方法栈也会抛出StackOverflowError和OutOfMemoryError异常。
                


## 2.类加载

###目录
    1.三种类加载器
    2.类加载机制和对象生命周期
    3.双亲委派,为啥要这样,有违背这个原则的吗(tomcat)
    4.反射
    5.如何自定义自己的类加载器，自己的类加载器和Java自带的类加载器关系如何处理？
    5.获取class的三种方式
        第一种：通过类名获得
    　　  Class<?> class = ClassName.class;
    　　第二种：通过类名全路径获得：
    　　  Class<?> class = Class.forName("类名全路径");
            这种需要try catch 抓住ClassNotFoundException
    　　第三种：通过实例对象获得：
    　　  Class<?> class = object.getClass();
    
        建议：使用类名.class，这样做即简单又安全，因为在编译时就会受到检查，因此不需要置于try语句块中，
        并且它根除了对forName()方法的调用，所以也更高效。
    6.在Java的反射中，Class.forName和ClassLoader的区别
        根据运行结果得出Class.forName加载类是将类进了初始化，而ClassLoader的loadClass并没有对类进行初始化，只是把类加载到了虚拟机中。
###结束

     *JVM 类加载机制详解
     https://www.cnblogs.com/cxxjohnson/p/8653360.html
     
     启动类加载器(Bootstrap ClassLoader)：负责加载 JAVA_HOME\lib 目录中的，或通过-Xbootclasspath参数指定路径中的，
     且被虚拟机认可（按文件名识别，如rt.jar）的类。
     扩展类加载器(Extension ClassLoader)：负责加载 JAVA_HOME\lib\ext 目录中的，或通过java.ext.dirs系统变量指定路径中的类库。
     应用程序类加载器(Application ClassLoader)：负责加载用户路径（classpath）上的类库。

     *双亲委派模式
         当一个类加载器收到类加载任务，会先交给其父类加载器去完成，因此最终加载任务都会传递到顶层的启动类加载器，
         只有当父类加载器无法完成加载任务时，才会尝试执行加载任务。
         采用双亲委派的一个好处是比如加载位于rt.jar包中的类java.lang.Object，不管是哪个加载器加载这个类，
         最终都是委托给顶层的启动类加载器进行加载，这样就保证了使用不同的类加载器最终得到的都是同样一个Object对象。
          
         双亲委派模型是指：当一个类加载器收到类加载请求时，不会直接加载这个类，而是把这个加载请求委派给自己父加载器去完成。如果父加载器无法加载时，子加载器才会去尝试加载。
         采用双亲委派模型的原因：避免同一个类被多个类加载器重复加载。
     
     
     
     *JVM类加载机制与对象的生命周期
     https://www.cnblogs.com/cxxjohnson/p/8662370.html
     
     *类的生命周期
         类的生命周期包括7个部分：加载——验证——准备——解析——初始化——使用——卸载
         1.加载
            通过类名字获取类的二进制字节流——将字节流的内容转存到方法区——在内存中生成一个Class对象作为该类方法区数据的访问入
         2.验证
            确保class文件的二进制字节流中包含的信息符号虚拟机要求
         3.准备
            为类变量（静态变量）在方法区分配内存，并设置零值。注意：这里是类变量，不是实例变量，实例变量是对象分配到堆内存时根据运行时动态生成的。
         4.解析
            把常量池中的符号引用解析为直接引用：根据符号引用所作的描述，在内存中找到符合描述的目标并把目标指针指针返回。
         5.初始化
            真正开始执行Java程序代码，该步执行<clinit>方法根据代码赋值语句，对 类变量和其他资源  进行初始化赋值。
             在经历了上面5步“加载”阶段后，才真正地可以使用class对象或者使用实例对象。使用过后，不再需要用到该类的class对象或者实例对象时，
             就会把类卸载掉（发生在方法区的垃圾回收：无用类的卸载）。
         6.使用
         7.卸载
         
     类的初始化触发
         最常见的是前三种：实例化对象、读写静态对象、调用静态方法、反射机制调用类、调用子类触发父类初始化。
     
     *采用双亲委派模型的原因：避免同一个类被多个类加载器重复加载。
     
     反射
         反射的定义如下：java程序在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，
         都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。
     
     反射原理 
         总结起来说就是，反射是通过Class类和java.lang.reflect类库一起支持而实现的，其中每一个Class类的对象都对应了一个类，
         这些信息在编译时期就已经被存在了.class文件里面了，Class 对象是在加载类时由 Java 虚拟机以及通过调用类加载器中
         的defineClass方法自动构造的。也就是这不需要我们自己去处理创建，JVM已经帮我们创建好了。对于我们定义的每一个类，
         在虚拟机中都有一个应的Class对象。
         
         总结: java虚拟机帮我们生成了类的class对象,而通过类的全限定名，我们可以去获取这个类的字节码.class文件
         ，然后再获取这个类对应的class对象，再通过class对象提供的方法结合类Method,Filed,Constructor，
         就能获取到这个类的所有相关信息.
         
     Tomcat 类加载器之为何违背双亲委派模型
        https://blog.csdn.net/dangwanma6489/article/details/80244981
        tomcat 违背了java 推荐的双亲委派模型了吗？
        答案是：违背了。 我们前面说过：
        双亲委派模型要求除了顶层的启动类加载器之外，其余的类加载器都应当由自己的父类加载器加载。
        很显然，tomcat 不是这样实现，tomcat 为了实现隔离性，没有遵守这个约定，每个webappClassLoader加载自己的目录下的class文件，不会传递给父类加载器。
        我们扩展出一个问题：如果tomcat 的 Common ClassLoader 想加载 WebApp ClassLoader 中的类，该怎么办？
        看了前面的关于破坏双亲委派模型的内容，我们心里有数了，我们可以使用线程上下文类加载器实现，
        使用线程上下文加载器，可以让父类加载器请求子类加载器去完成类加载的动作。
        
     

## 3.jvm

###目录
    1.哪些内存需要回收？即如何判断对象已经死亡?
        判断对象可以回收算法
            引用计数算法  (很难解决对象之间相互循环引用的问题)
            可达性分析算法 (分析过程需要GC停顿（引用关系不能发生变化）)
        判断对象生存还是死亡
            第一次标记   (在可达性分析后发现到GC Roots没有任何引用链相连时，被第一次标记；)
            第二次标记   (GC将对F-Queue队列中的对象进行第二次小规模标记；如果对象在其finalize()方法中重新与引用链上任何一个对象建立关联，第二次标记时会将其移出"即将回收"的集合,否则就回收)
        安全点     (选取引用不咋变化的区域)
        安全区域    (指一段代码片段中，引用关系不会发生变化,在这个区域中的任意地方开始GC都是安全的；)
            
    2.什么时候回收？即GC发生在什么时候？需要了解GC策略，与垃圾回收器实现有关?
        常见的垃圾回收算法
            复制算法    (新生代使用)
            标记-清除算法 (标记清除后会产生大量不连续的内存碎片；)
            标记-整理算法 (主要是效率问题：除像标记-清除算法的标记过程外，还多了需要整理的过程，效率更低；)
            分代收集算法  (这样就可以根据各个年代的特点采用最适当的收集算法；)
            火车算法    (G1)
            
    3.如何回收？即需要了解垃圾回收算法，及算法的实现--垃圾回收器?
        7种垃圾回收器
            Serial、ParNew、Parallel Scavenge、Serial Old、Parallel Old、CMS、G1
        
        垃圾收集器组合
             新生代收集器：Serial、ParNew、Parallel Scavenge；
             老年代收集器：Serial Old、Parallel Old、CMS；
             整堆收集器：G1；
        吞吐量与收集器关注点说明
            停顿时间 
            吞吐量
            覆盖区
        常用组合
            Parallel Scavenge（新生代）+  Parallel Old（老年代）    吞吐量大
            ParNew（新生代）      +       CMS（老年代）             暂停时间短
            G1                                                   面向服务端应用，针对具有大内存、多处理器的机器；最主要的应用是为需要低GC延迟，并具有大堆的应用程序提供解决方案；如：在堆大小约6GB或更大时，可预测的暂停时间可以低于0.5秒；
                                                                 
    4.各版本默认垃圾收集器
        jdk1.7 默认垃圾收集器Parallel Scavenge（新生代）+Parallel Old（老年代）
        jdk1.8 默认垃圾收集器Parallel Scavenge（新生代）+Parallel Old（老年代）
        jdk1.9 默认垃圾收集器G1
        
        -XX:+PrintCommandLineFlagsjvm参数可查看默认设置收集器类型
        -XX:+PrintGCDetails亦可通过打印的GC日志的新生代、老年代名称判断
        
        bat推荐参数配置   (必看)
        https://juejin.im/post/5b091ee35188253892389683
    5.常用命令
        https://mp.weixin.qq.com/s/QNr8somjodyvU9dRAQG2oA
        jstat 
            -gc         垃圾回收堆的行为统计
            -gcutil     同-gc，不过输出的是已使用空间占总空间的百分比
            -gccause    垃圾收集统计概述（同-gcutil），附加最近两次垃圾回收事件的原因
            
        jmap
            option参数
            dump : 生成堆转储快照
            finalizerinfo : 显示在F-Queue队列等待Finalizer线程执行finalizer方法的对象
            heap : 显示Java堆详细信息
            histo : 显示堆中对象的统计信息
            permstat : to print permanent generation statistics
            F : 当-dump没有响应时，强制生成dump快照
        jhat
        jstack
            -F : 当正常输出请求不被响应时，强制输出线程堆栈
            -l : 除堆栈外，显示关于锁的附加信息    (常用)
            -m : 如果调用到本地方法的话，可以显示C/C++的堆栈
            
    6.频繁GC问题或内存溢出问题 
      一、使用jps查看线程ID 
      二、使用jstat -gc 3331 250 20 查看gc情况，一般比较关注PERM区的情况，查看GC的增长情况。 
      三、使用jstat -gccause：额外输出上次GC原因 
      四、使用jmap -dump:format=b,file=heapDump 3331生成堆转储文件 
      五、使用jhat或者可视化工具（Eclipse Memory Analyzer 、IBM HeapAnalyzer）分析堆情况。 
      六、结合代码解决内存溢出或泄露问题。 
      
      死锁问题 
      一、使用jps查看线程ID 
      二、使用jstack 3331：查看线程情况
      
        
###结束
    

     *Java虚拟机垃圾回收(一) 基础 回收哪些内存/对象 
        程序计数器、虚拟机栈、本地方法栈这3个区域是随线程而生而灭的，内存分配和回收都具备确定性，而Java堆和方法区则不一样，
        各线程共享，在运行时内存的分配与回收都是动态的，垃圾收集器所关注的是这部分内存。
    
     引用计数算法 可达性分析算法 finalize()方法 HotSpot实现分析
        https://blog.csdn.net/tjiyu/article/details/53982412
        
        *判断对象可以回收
            引用计数算法  
            可达性分析算法 
            
                思路
                    给对象添加一个引用计数器，每当有一个地方引用它，计数器加1；
                    当引用失效，计数器值减1；
                    任何时刻计数器值为0，则认为对象是不再被使用的；
                缺点
                    很难解决对象之间相互循环引用的问题
                
            *可达性分析算法
                思路
                    通过一系列"GC Roots"对象作为起始点，开始向下搜索；
                    搜索所走过和路径称为引用链（Reference Chain）；
                    *当一个对象到GC Roots没有任何引用链相连时（从GC Roots到这个对象不可达），则证明该对象是不可用的；
                GC Roots对象
                    （1）、虚拟机栈（栈帧中本地变量表）中引用的对象；
                    （2）、方法区中类静态属性引用的对象；
                    （3）、方法区中常量引用的对象；
                    （4）、本地方法栈中JNI（Native方法）引用的对象；
                缺点
                    实现比较复杂；
                    需要分析大量数据，消耗大量时间；
                    *分析过程需要GC停顿（引用关系不能发生变化），即停顿所有Java执行线程（称为"Stop The World"，是垃圾回收重点关注的问题）；
            
            引用的种类
                JDK1.2后，对引用概念进行了扩充
                （1）、强引用（Strong Reference）
                    只要强引用还存在，GC永远不会回收被引用的对象；
                （2）、软引用（Soft Reference）
                    直到内存空间不够时（抛出OutOfMemoryError之前），才会被垃圾回收；
                （3）、弱引用（Weak Reference）
                    只能生存到下一次垃圾回收之前，无论内存是否足够；
                （4）、虚引用（Phantom Reference）
            
            
        *判断对象生存还是死亡
            要真正宣告一个对象死亡，至少要经历两次标记过程。
            
            第一次标记
                 在可达性分析后发现到GC Roots没有任何引用链相连时，被第一次标记；
                 并且进行一次筛选：此对象是否必要执行finalize()方法；
                 （A）、没有必要执行
                       没有必要执行的情况：
                       （1）、对象没有覆盖finalize()方法；
                       （2）、finalize()方法已经被JVM调用过；
                       这两种情况就可以认为对象已死，可以回收；
                 
                 （B）、有必要执行
                       对有必要执行finalize()方法的对象，被放入F-Queue队列中；
                       稍后在JVM自动建立、低优先级的Finalizer线程（可能多个线程）中触发这个方法；
                
            第二次标记
                 GC将对F-Queue队列中的对象进行第二次小规模标记；
                      finalize()方法是对象逃脱死亡的最后一次机会：
                      （A）、如果对象在其finalize()方法中重新与引用链上任何一个对象建立关联，第二次标记时会将其移出"即将回收"的集合；
                      （B）、如果对象没有，也可以认为对象已死，可以回收了；                    
                      一个对象的finalize()方法只会被系统自动调用一次，经过finalize()方法逃脱死亡的对象，第二次不会再调用；
         
        *finalize()方法
            充当"安全网"
            与对象的本地对等体有关
         
        *安全点
            为什么需要安全点
                运行中，非常多的指令都会导致引用关系变化；
                如果为这些指令都生成对应的OopMap，需要的空间成本太高；     
            解决办法
                只在特定的位置记录OopMap引用关系，这些位置称为安全点（Safepoint）；
                即程序执行时并非所有地方都能停顿下来开始GC；
            安全点的选定
                不能太少，否则GC等待时间太长；也不能太多，否则GC过于频繁，增大运行时负荷；
                所以，基本上是以程序"是否具有让程序长时间执行的特征"为标准选定；
                "长时间执行"最明显的特征就是指令序列复用，如：方法调用、循环跳转、循环的末尾、异常跳转等；
                只有具有这些功能的指令才会产生Safepoint；
            如何在安全点上停顿
                抢先式中断   (不推荐)
                主动式中断   
                    （1）、在GC发生时，不直接操作线程中断，而是仅简单设置一个标志；
                    （2）、让各线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起；
                    
            
        *安全区域(解决安全点问题->线程sleep或者系统阻塞)
            为什么需要安全区域
                对于上面的Safepoint还有一个问题：
                程序不执行时没有CPU时间（Sleep或Blocked状态），无法运行到Safepoint上再中断挂起； 
            什么是安全区域（Safe Region）
                指一段代码片段中，引用关系不会发生变化；
                在这个区域中的任意地方开始GC都是安全的；  
            
            
            
        
     *Java虚拟机垃圾回收(二) 垃圾回收算法 
      标记-清除算法 复制算法 标记-整理算法 分代收集算法 火车算法
      https://blog.csdn.net/tjiyu/article/details/53983064
      
        常见的垃圾回收算法
            *标记-清除算法
                算法思路
                    "标记-清除"算法，分为两个阶段：
                    （A）、标记
                          首先标记出所有需要回收的对象；
                          （1）、第一次标记
                          （2）、第二次标记
                          对第一次被标记，且第二次还被标记（如果需要，但没有移出"即将回收"的集合），就可以认为对象已死，可以进行回收。
                    （B）、清除
                          两次标记后，还在"即将回收"集合的对象将被统一回收；
                      
                缺点
                    （A）、效率问题
                           标记和清除两个过程的效率都不高；
                    （B）、空间问题
                           标记清除后会产生大量不连续的内存碎片；
                           这会导致分配大内存对象时，无法找到足够的连续内存；
                           从而需要提前触发另一次垃圾收集动作；
                应用场景
                      针对老年代的CMS收集器； 
                          
                          
            *复制算法
                算法思路
                    （A）、把内存划分为大小相等的两块，每次只使用其中一块；
                    （B）、当一块内存用完了，就将还存活的对象复制到另一块上（而后使用这一块）；
                    （C）、再把已使用过的那块内存空间一次清理掉，而后重复步骤2；    
                优点
                    这使得每次都是只对整个半区进行内存回收；
                    内存分配时也不用考虑内存碎片等问题（可使用"指针碰撞"的方式分配内存）；
                    实现简单，运行高效；
                缺点
                    （A）、空间浪费
                          可用内存缩减为原来的一半，太过浪费（解决：可以改良，不按1:1比例划分）；
                    （B）、效率随对象存活率升高而变低
                          当对象存活率较高时，需要进行较多复制操作，效率将会变低（解决：后面的标记-整理算法）；
                应用场景
                      现在商业JVM都采用这种算法（通过改良缺点1）来回收新生代；
                      如Serial收集器、ParNew收集器、Parallel Scavenge收集器、、G1（从局部看）；
                HotSpot虚拟机的改良算法
                    （A）、弱代理论
                        （1）、大多数分配了内存的对象并不会存活太长时间，在处于年轻代时就会死掉；
                        （2）、很少有对象会从老年代变成年轻代；
                         其中IBM研究表明：新生代中98%的对象都是"朝生夕死"； 所以并不需要按1:1比例来划分内存（解决了缺点1）；
                    （B）、HotSpot虚拟机新生代内存布局及算法
                           （1）、将新生代内存分为一块较大的Eden空间和两块较小的Survivor空间；
                           （2）、每次使用Eden和其中一块Survivor；
                           （3）、当回收时，将Eden和使用中的Survivor中还存活的对象一次性复制到另外一块Survivor；
                           （4）、而后清理掉Eden和使用过的Survivor空间；
                           （5）、后面就使用Eden和复制到的那一块Survivor空间，重复步骤3；
                              默认Eden：Survivor=8:1，即每次可以使用90%的空间，只有一块Survivor的空间被浪费；
                    （C）、分配担保
                           如果另一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，
                           这些对象将直接通过分配担保机制（Handle Promotion）进入老年代；
                        
            *标记-整理算法
                "标记-整理"（Mark-Compact）算法是根据老年代的特点提出的。
                算法思路
                    （1）、标记
                          标记过程与"标记-清除"算法一样；
                    （2）、整理
                           但后续不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动；
                           然后直接清理掉端边界以外的内存；
                优点
                    （A）、不会像复制算法，效率随对象存活率升高而变低
                           老年代特点：
                           对象存活率高，没有额外的空间可以分配担保；
                           所以老年代一般不能直接选用复制算法算法；
                           而选用标记-整理算法；
                    （B）、不会像标记-清除算法，产生内存碎片
                           因为清除前，进行了整理，存活对象都集中到空间一侧；
                
                缺点
                       主要是效率问题：除像标记-清除算法的标记过程外，还多了需要整理的过程，效率更低；
                应用场景
                       很多垃圾收集器采用这种算法来回收老年代；
                      如Serial Old收集器、G1（从整体看）；


            *分代收集算法
                算法思路
                       基于前面说的弱代理论，其实并没有什么新的思想；
                       只是根据对象存活周期的不同将内存划分为几块；
                       这样就可以根据各个年代的特点采用最适当的收集算法；
                       一般把Java堆分为新生代和老年代；
                    （A）、新生代
                           每次垃圾收集都有大批对象死去，只有少量存活；
                           所以可采用复制算法；
                    
                    （B）、老年代
                          对象存活率高，没有额外的空间可以分配担保；
                          使用"标记-清理"或"标记-整理"算法；
                          结合上面对新生代的内存划分介绍和上篇文章对Java堆的介绍，可以得出HotSpot虚拟机一般的年代内存划分，如下图：
                    默认值:
                        新生代 : 老年代   = 1 : 2
                        eden : surivor = 8 : 1
                        
                优点      
                       可以根据各个年代的特点采用最适当的收集算法；
                        
            *火车算法
                火车算法也称列车算法，是一种更彻底的分区域处理收集算法，是对分代收集算法的一个有力补充。
                
                算法思路
                    在火车算法中，内存被分为块，多个块组成一个集合。为了形象化，一节车厢代表一个块，一列火车代表一个集合
                    （1）、选择标号最小的火车；
                    （2）、如果火车的记忆集合是空的, 释放整列火车并终止, 否则进行第三步操作；
                    （3）、选择火车中标号最小的车厢；
                    （4）、对于车厢记忆集合的每个元素：
                    （5）、释放车厢并且终止；
                优点
                      可以在成熟对象空间提供限定时间的渐近收集；
                      而不需要每次都进行一个大区域的垃圾回收过程；
                      即可以控制垃圾回收的时间，在指定时间内进行一些小区域的回收；
                
                缺点
                      实现较为复杂，如采用类似的算法的G1收集器在JDK7才实现；
                      一些场景下可能性价比不高；
                
                应用场景
                      JDK7后HotSpot虚拟机G1收集器采用类似的算法，能建立可预测的停顿时间模型；
                
                
     *Java虚拟机垃圾回收(三) 7种垃圾收集器 
      主要特点 应用场景 设置参数 基本运行原理
      https://blog.csdn.net/tjiyu/article/details/53983650
      
         垃圾收集器组合
            （A）、7种不同分代的收集器
                Serial、ParNew、Parallel Scavenge、Serial Old、Parallel Old、CMS、G1；
            （B）、而它们所处区域，则表明其是属于新生代收集器还是老年代收集器：
                新生代收集器：Serial、ParNew、Parallel Scavenge；
                老年代收集器：Serial Old、Parallel Old、CMS；
                整堆收集器：G1；
            （C）、两个收集器间有连线，表明它们可以搭配使用：
                   Serial/Serial Old、Serial/CMS、ParNew/Serial Old、ParNew/CMS、Parallel Scavenge/Serial Old、Parallel Scavenge/Parallel Old、G1；
            （D）、其中Serial Old作为CMS出现"Concurrent Mode Failure"失败的后备预案（后面介绍）；
            
         并发垃圾收集和并行垃圾收集的区别
            （A）、并行（Parallel）
                   指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态；
                   如ParNew、Parallel Scavenge、Parallel Old；
            
            （B）、并发（Concurrent）
                   指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行）；
                  用户程序在继续运行，而垃圾收集程序线程运行于另一个CPU上；    
                   如CMS、G1（也有并行）；
         
         Minor GC和Full GC的区别
             （A）、Minor GC
                    又称新生代GC，指发生在新生代的垃圾收集动作；
                    因为Java对象大多是朝生夕灭，所以Minor GC非常频繁，一般回收速度也比较快；
             
             （B）、Full GC
                    又称Major GC或老年代GC，指发生在老年代的GC；
                    出现Full GC经常会伴随至少一次的Minor GC（不是绝对，Parallel Sacvenge收集器就可以选择设置Major GC策略）；
                   Major GC速度一般比Minor GC慢10倍以上；

         各种收集器
            Serial收集器
                特点
                      针对新生代；
                      采用复制算法；
                      单线程收集；
                       进行垃圾收集时，必须暂停所有工作线程，直到完成；            
                       即会"Stop The World"；

            ParNew收集器
                ParNew垃圾收集器是Serial收集器的多线程版本。
                特点
                      除了多线程外，其余的行为、特点和Serial收集器一样；
                      如Serial收集器可用控制参数、收集算法、Stop The World、内存分配规则、回收策略等；
                      两个收集器共用了不少代码；
                      ParNew/Serial Old组合收集器运行示意图如下：
                
                应用场景
                      *在Server模式下，ParNew收集器是一个非常重要的收集器，因为除Serial外，目前只有它能与CMS收集器配合工作；
                      但在单个CPU环境中，不会比Serail收集器有更好的效果，因为存在线程交互开销。
            
                设置参数
                      "-XX:+UseConcMarkSweepGC"：指定使用CMS后，会默认使用ParNew作为新生代收集器；
                      "-XX:+UseParNewGC"：强制指定使用ParNew；    
                      "-XX:ParallelGCThreads"：指定垃圾收集的线程数量，ParNew默认开启的收集线程与CPU的数量相同；

                为什么只有ParNew能与CMS收集器配合
                      CMS是HotSpot在JDK1.5推出的第一款真正意义上的并发（Concurrent）收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作；
                      CMS作为老年代收集器，但却无法与JDK1.4已经存在的新生代收集器Parallel Scavenge配合工作；
                      因为Parallel Scavenge（以及G1）都没有使用传统的GC收集器代码框架，而另外独立实现；而其余几种收集器则共用了部分的框架代码；
                      关于CMS收集器后面会详细介绍。

            Parallel Scavenge收集器
                 Parallel Scavenge垃圾收集器因为与吞吐量关系密切，也称为吞吐量收集器（Throughput Collector）。
                
                特点
                （A）、有一些特点与ParNew收集器相似
                      新生代收集器；
                      采用复制算法；
                      多线程收集；
                
                （B）、主要特点是：它的关注点与其他收集器不同
                      CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间；
                      而Parallel Scavenge收集器的目标则是达一个可控制的吞吐量（Throughput）；
                      关于吞吐量与收集器关注点说明详见本节后面；
                应用场景
                      高吞吐量为目标，即减少垃圾收集时间，让用户代码获得更长的运行时间；
                      当应用程序运行在具有多个CPU上，对暂停时间没有特别高的要求时，即程序主要在后台进行计算，而不需要与用户进行太多交互；
                      例如，那些执行批量处理、订单处理、工资支付、科学计算的应用程序；
                
                设置参数
                      Parallel Scavenge收集器提供两个参数用于精确控制吞吐量：
                
                    （A）、"-XX:MaxGCPauseMillis"
                          控制最大垃圾收集停顿时间，大于0的毫秒数；
                          MaxGCPauseMillis设置得稍小，停顿时间可能会缩短，但也可能会使得吞吐量下降；
                          因为可能导致垃圾收集发生得更频繁；
                    
                    （B）、"-XX:GCTimeRatio"
                          设置垃圾收集时间占总时间的比率，0<n<100的整数；
                          GCTimeRatio相当于设置吞吐量大小；
                          垃圾收集执行时间占应用程序执行时间的比例的计算方法是：
                          1 / (1 + n)
                          例如，选项-XX:GCTimeRatio=19，设置了垃圾收集时间占总时间的5%--1/(1+19)；
                          默认值是1%--1/(1+99)，即n=99；
                    
                        垃圾收集所花费的时间是年轻一代和老年代收集的总时间；
                        如果没有满足吞吐量目标，则增加代的内存大小以尽量增加用户程序运行的时间；
                          此外，还有一个值得关注的参数：
                     
                     （C）、"-XX:+UseAdptiveSizePolicy"
                     
                           开启这个参数后，就不用手工指定一些细节参数，如：
                           新生代的大小（-Xmn）、Eden与Survivor区的比例（-XX:SurvivorRation）、晋升老年代的对象年龄（-XX:PretenureSizeThreshold）等；
                           JVM会根据当前系统运行情况收集性能监控信息，动态调整这些参数，以提供最合适的停顿时间或最大的吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomiscs）；    
                           这是一种值得推荐的方式：
                     
                           (1)、只需设置好内存数据大小（如"-Xmx"设置最大堆）；
                           (2)、然后使用"-XX:MaxGCPauseMillis"或"-XX:GCTimeRatio"给JVM设置一个优化目标；
                           (3)、那些具体细节参数的调节就由JVM自适应完成；        
                     
                           这也是Parallel Scavenge收集器与ParNew收集器一个重要区别；    
                     
                           更多目标调优和GC自适应的调节策略说明请参考：    

                吞吐量与收集器关注点说明
                    （A）、吞吐量（Throughput）
                         即吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间）；    
                         高吞吐量即减少垃圾收集时间，让用户代码获得更长的运行时间；
                    （B）、垃圾收集器期望的目标（关注点）
                         停顿时间越短就适合需要与用户交互的程序；
                    （3）、覆盖区（Footprint）
                         在达到前面两个目标的情况下，尽量减少堆的内存空间；

         *吞吐量与收集器关注点说明
         
            吞吐量
            CPU用于运行用户代码的时间与CPU总消耗时间的比值；
            即吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间）；
            
            垃圾收集器期望的目标
            停顿时间 
            吞吐量
            覆盖区

         *Parallel Old收集器
            Parallel Old垃圾收集器是Parallel Scavenge收集器的老年代版本；
            特点
                  针对老年代；
                  采用"标记-整理"算法；
                  多线程收集；
      
            应用场景
                  JDK1.6及之后用来代替老年代的Serial Old收集器；
                  特别是在Server模式，多CPU的情况下；
                  这样在注重吞吐量以及CPU资源敏感的场景，就有了Parallel Scavenge加Parallel Old收集器的"给力"应用组合；
            
            设置参数
                  "-XX:+UseParallelOldGC"：指定使用Parallel Old收集器；
            
         *CMS收集器
            并发标记清理（Concurrent Mark Sweep，CMS）收集器也称为并发低停顿收集器（Concurrent Low Pause Collector）
            或低延迟（low-latency）垃圾收集器；
            
            特点
                  针对老年代；
                  基于"标记-清除"算法(不进行压缩操作，产生内存碎片)；            
                  以获取最短回收停顿时间为目标；
                  并发收集、低停顿；
                  需要更多的内存（看后面的缺点）；
            
            应用场景
                  与用户交互较多的场景；        
                  希望系统停顿时间最短，注重服务的响应速度；
                  以给用户带来较好的体验；
                  如常见WEB、B/S系统的服务器上的应用；
            
            设置参数
                  "-XX:+UseConcMarkSweepGC"：指定使用CMS收集器；
            
            CMS收集器运作过程
                  比前面几种收集器更复杂，可以分为4个步骤:
                （A）、初始标记（CMS initial mark）
                      仅标记一下GC Roots能直接关联到的对象；
                      速度很快；
                      但需要"Stop The World"；
                （B）、并发标记（CMS concurrent mark）
                      进行GC Roots Tracing的过程；
                      刚才产生的集合中标记出存活对象；
                      应用程序也在运行；
                      并不能保证可以标记出所有的存活对象；
                （C）、重新标记（CMS remark）
                      为了修正并发标记期间因用户程序继续运作而导致标记变动的那一部分对象的标记记录；
                      需要"Stop The World"，且停顿时间比初始标记稍长，但远比并发标记短；
                      采用多线程并行执行来提升效率；
                （D）、并发清除（CMS concurrent sweep）
                
                      回收所有的垃圾对象；

            CMS收集器3个明显的缺点
                （A）、对CPU资源非常敏感
                     并发收集虽然不会暂停用户线程，但因为占用一部分CPU资源，还是会导致应用程序变慢，总吞吐量降低。
                     
                （B）、无法处理浮动垃圾,可能出现"Concurrent Mode Failure"失败
                    （1）、浮动垃圾（Floating Garbage）
                          在并发清除时，用户线程新产生的垃圾，称为浮动垃圾；
                          这使得并发清除时需要预留一定的内存空间，不能像其他收集器在老年代几乎填满再进行收集；
                          也要可以认为CMS所需要的空间比其他垃圾收集器大；
                          "-XX:CMSInitiatingOccupancyFraction"：设置CMS预留内存空间；
                          JDK1.5默认值为68%；
                          JDK1.6变为大约92%；  
                                 
                    （2）、"Concurrent Mode Failure"失败
                          如果CMS预留内存空间无法满足程序需要，就会出现一次"Concurrent Mode Failure"失败；
                          这时JVM启用后备预案：临时启用Serail Old收集器，而导致另一次Full GC的产生；
                          这样的代价是很大的，所以CMSInitiatingOccupancyFraction不能设置得太大。
                   
                （C）、产生大量内存碎片
                       由于CMS基于"标记-清除"算法，清除后不进行压缩操作；
                       前面《Java虚拟机垃圾回收(二) 垃圾回收算法》"标记-清除"算法介绍时曾说过：
                       产生大量不连续的内存碎片会导致分配大内存对象时，无法找到足够的连续内存，从而需要提前触发另一次Full GC动作。
                       解决方法：                
                         （1）、"-XX:+UseCMSCompactAtFullCollection"
                               使得CMS出现上面这种情况时不进行Full GC，而开启内存碎片的合并整理过程；
                               但合并整理过程无法并发，停顿时间会变长；
                               默认开启（但不会进行，结合下面的CMSFullGCsBeforeCompaction）；
                         
                         （2）、"-XX:+CMSFullGCsBeforeCompaction"
                               设置执行多少次不压缩的Full GC后，来一次压缩整理；
                               为减少合并整理过程的停顿时间；
                               默认为0，也就是说每次都执行Full GC，不会进行压缩整理；

            *G1收集器
                特点
                    （A）、并行与并发
                    （B）、分代收集，收集范围包括新生代和老年代    
                          能独立管理整个GC堆（新生代和老年代），而不需要与其他收集器搭配；
                          能够采用不同方式处理不同时期的对象；
                          虽然保留分代概念，但Java堆的内存布局有很大差别；
                          将整个堆划分为多个大小相等的独立区域（Region）；
                          新生代和老年代不再是物理隔离，它们都是一部分Region（不需要连续）的集合；
                          更多G1内存布局信息请参考：
                   （C）、结合多种垃圾收集算法，空间整合，不产生碎片 
                          从整体看，是基于标记-整理算法；
                          从局部（两个Region间）看，是基于复制算法；
                          这是一种类似火车算法的实现；
                          都不会产生内存碎片，有利于长时间运行；
                          
                   （D）、可预测的停顿：低停顿的同时实现高吞吐量
                        G1除了追求低停顿处，还能建立可预测的停顿时间模型；
                        可以明确指定M毫秒时间片内，垃圾收集消耗的时间不超过N毫秒；
                        
                应用场景
                      面向服务端应用，针对具有大内存、多处理器的机器；
                      最主要的应用是为需要低GC延迟，并具有大堆的应用程序提供解决方案；
                      如：在堆大小约6GB或更大时，可预测的暂停时间可以低于0.5秒；
                      
                      用来替换掉JDK1.5中的CMS收集器；
                            在下面的情况时，使用G1可能比CMS好：
                            （1）、超过50％的Java堆被活动数据占用；
                            （2）、对象分配频率或年代提升频率变化很大；
                            （3）、GC停顿时间过长（长于0.5至1秒）。
                            
                设置参数
                      "-XX:+UseG1GC"：指定使用G1收集器；
                      "-XX:InitiatingHeapOccupancyPercent"：当整个Java堆的占用率达到参数值时，开始并发标记阶段；默认为45；
                      "-XX:MaxGCPauseMillis"：为G1设置暂停时间目标，默认值为200毫秒；
                      "-XX:G1HeapRegionSize"：设置每个Region大小，范围1MB到32MB；目标是在最小Java堆时可以拥有约2048个Region；
                
                      更多关于G1参数设置请参考：

                G1收集器运作过程
                    （A）、初始标记（Initial Marking）
                    （B）、并发标记（Concurrent Marking）
                    （C）、最终标记（Final Marking）
                    （D）、筛选回收（Live Data Counting and Evacuation）
                    
      
     *Java虚拟机垃圾回收(四) 总结：
      内存分配与回收策略 方法区垃圾回收 以及 JVM垃圾回收的调优方法
      https://blog.csdn.net/tjiyu/article/details/54588494
      
         *内存分配与回收策略
            对象优先在Eden分配
            大对象直接进入老年代
            长期存活的对象将进入老年代(默认15)
            动态对象年龄判定
         回收方法区
            
         *JVM垃圾回收的调优方法
            明确期望的目标（关注点）
                吞吐量
                停顿时间
                覆盖区
                总结就是：低停顿、高吞吐量、少用内存资源；
                        一般这些目标都相互影响的，增大堆内存获得高吞吐量但会增长停顿时间，反之亦然，有时需折中处理。
                
            JVM自适应调整（Ergonomics）
                
            *实践调优：选择垃圾收集器，并进行相关设置
                
                  例如，使用Parallel Scavenge/Parallel Old组合，这是一种值得推荐的方式：
                       1、只需设置好内存数据大小（如"-Xmx"设置最大堆）；
                       2、然后使用"-XX:MaxGCPauseMillis"或"-XX:GCTimeRatio"给JVM设置一个优化目标；
                       3、那些具体细节参数的调节就由JVM自适应完成；   


            *bat 推荐配置
            https://juejin.im/post/5b091ee35188253892389683
            
            
       
## 4.tomcat
        
     *Tomcat(一) Tomcat是什么：Tomcat与Java技术 Tomcat与Web应用 以及 Tomcat基本框架及相关配置
     https://blog.csdn.net/tjiyu/article/details/54590258
     
     *Tomcat(二) Tomcat实现：
      Servlet与web.xml介绍 以及 源码分析Tomcat实现细节
      https://blog.csdn.net/tjiyu/article/details/54590259
      
     *Tomcat(三) Tomcat安装配置：
      Tomcat+Nginx+keepalived 实现动静分离、Session会话保持的高可用集群
      https://blog.csdn.net/tjiyu/article/details/54591126
      
      

     
## 5.linux命令

     *Linux常用命令大全
     https://www.cnblogs.com/cxxjohnson/p/4973161.html
     

## 6.多线程

###目录
    0.多线程特点
        1.可见性
        2.原子性
        3.可见性
    1.线程的状态
        new -> runable -> running -> Blocked -> Dead
    2.如何停止一个线程
        https://www.jianshu.com/p/e0ff2e420ab6
        isInterrupted() + return
            在正常运行任务时，经常检查本线程的中断标志位，如果被设置了中断标志就自行停止线程。
            在调用阻塞方法时正确处理InterruptedException异常。（例如：catch异常后就结束线程。）
            
    3.如何获取线程中的异常
         setUncaughtExceptionHandler
    4.线程组
        但如果在不同的线程组中，那么就不能“跨线程组”修改数据，可以从一定程度上保证数据安全。
    5.ThreadLocal
         每个Thread的对象都有一个ThreadLocalMap，当创建一个ThreadLocal的时候，就会将该ThreadLocal对象添加到该Map中，
         其中键就是ThreadLocal，值可以是任意类型
         每个线程用完了以后最好自己remove,不然长期存在的线程可能内存泄漏或者取到脏数据
    6.Atomic原子类
        ABA问题,用AtomicStampedReference解决,加版本号,CAS Unsage.compareAndSet
    7.volatile
        每次针对该变量的操作都激发一次load and save,不保证原子行
    8.AQS框架 (AbstractQueuedSynchronizer)
        CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。
         AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node）来实现锁的分配。
         用CAS操作来管理这个同步状态
         内部内有Node和Condition,一个AQS一般包括一个同步队列和多个等待队列(多condition)
         
         AQS使用一个FIFO的队列表示排队等待锁的线程，队列头节点称作“哨兵节点”或者“哑节点”，它不与任何线程关联。其他的节点与等待线程关联，
         每个节点维护一个等待状态waitStatus
         http://ifeve.com/java-special-troops-aqs/
         
         总结一下acquire()的流程：
             调用自定义同步器的tryAcquire()尝试直接去获取资源，如果成功则直接返回；
             没成功，则addWaiter()将该线程加入等待队列的尾部，并标记为独占模式；
             acquireQueued()使线程在等待队列中休息，有机会时（轮到自己，会被unpark()）会去尝试获取资源。获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false。
             如果线程在等待过程中被中断过，它是不响应的。只是获取资源后才再进行自我中断selfInterrupt()，将中断补上。
         
         
    9.lock
        ReentrantReadWriteLock,Reentrant,多condition,sign(),await()
        与synchronized的区别:
            lock更灵活，可以自由定义多把锁的枷锁解锁顺序（synchronized要按照先加的后解顺序）
            提供多种加锁方案，lock 阻塞式, trylock 无阻塞式, lockInterruptily 可打断式， 还有trylock的带超时时间版本。
    10.synchronized
        wait(),notify()
        1> 某个对象实例内，synchronized aMethod(){}关键字可以防止多个线程访问对象的synchronized方法
        （如果一个对象有多个synchronized方法，只要一个线程访问了其中的一个synchronized方法，其它线程不能
        同时访问这个对象中任何一个synchronized方法）。这时，不同的对象实例的synchronized方法是不相干扰的。
        也就是说，其它线程照样可以同时访问相同类的另一个对象实例中的synchronized方法.
        
        2> 是某个类的范围，synchronized static aStaticMethod{}防止多个线程同时访问这个类中的synchronized
         static 方法。它可以对类的所有对象实例起作用。
    11.CountDownLatch,CyclicBarrier,Semaphore(信号量)
        使用Semaphore可以控制同时访问资源的线程个数（即允许n个任务同时访问这个资源）
        单个信号量的Semaphore对象可以实现互斥锁的功能，并且可以是由一个线程获得了“锁”，再由另一个线程释放“锁”，这可应用于死锁恢复的一些场合。
        
    12.Volatile和Synchronized四个不同点： 
        1 粒度不同，前者针对变量 ，后者锁对象和类 
        2 syn阻塞，volatile线程不阻塞 
        3 syn保证三大特性，volatile不保证原子性 
        4 syn编译器优化，volatile不优化 
    13.happens-before
        如果两个操作之间具有happens-before 关系，那么前一个操作的结果就会对后面一个操作可见。 
            1.程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 
            2.监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 
            3.volatile变量规则：对一个volatile域的写，happens- before于任意后续对这个volatile域的读。 
            4.传递性：如果A happens- before B，且B happens- before C，那么A happens- before C。 
            5.线程启动规则：Thread对象的start()方法happens- before于此线程的每一个动作。
    14.java线程阻塞的代价
        java的线程是映射到操作系统原生线程之上的，如果要阻塞或唤醒一个线程就需要操作系统介入，需要在户态与核心态之间切换，这种切换
        会消耗大量的系统资源，因为用户态与内核态都有各自专用的内存空间，专用的寄存器等，用户态切换至内核态需要传递给许多变量、参数
        给内核，内核也需要保护好用户态在切换时的一些寄存器值、变量等，以便内核态调用结束后切换回用户态继续工作。
    15.自旋锁,偏向锁,轻量锁
        cas 用到了自旋锁,synchronized 先用cas ,不行再用重量级锁
        JVM从1.5开始，引入了轻量锁与偏向锁，默认启用了自旋锁，他们都属于乐观锁。1.6优化
    16.start() 和 run()的区别
        调用start方法方可启动线程，而run方法只是thread的一个普通方法调用，还是在主线程里执行。
    17.LongAdder
        原理就是降低对value更新的并发数，也就是将对单一value的变更压力分散到多个value值上，降低单个value的“热度”
    18.synchronized 的实现原理以及锁优化？
        偏向锁-->轻量级锁 --> 自旋,如果自旋失败 -->重量级锁
        锁粗化 (就是将多次连接在一起的加锁、解锁操作合并为一次，将多个连续的锁扩展成一个范围更大的锁)
        锁消除 (如果判断到一段代码中，堆上的数据不会逃逸出当前线程，那么可以认为这段代码是线程安全的，不必要加锁)
        适应性自旋   (线程如果自旋成功了，则下次自旋的次数会更多，如果自旋失败了，则自旋的次数就会减少。)
    19.lock和synchronized的区别
    20.Hashtable 是怎么加锁的 ？  基本所有方法都加synchronized
    21.HashMap 的并发问题    (put 和 Rehash) 
    22.ConcurrenHashMap 介绍？1.8 中为什么要用红黑树？   (1.8 先用cas,再用synchronized)
    23.如何检测死锁？怎么预防死锁？   (四个必要条件,破坏其中一个,画图,无环)  jstack -l 查看死锁
    24.ThreadLocal原理，用的时候需要注意什么？
        *在线程池环境下，由于线程是一直运行且复用的，使用ThreadLocal<T>时会出现这个任务看到上个任务ThreadLocal变量值以及内存泄露等问题，
         解决方法就是在当前任务执行完后将ThreadLocal变量remove或设置为初始值
         调用get set 方法会清理key为null的值,线程池没调用时就会内存泄漏
    25.并发容器     (Condition notEmpty  notFull)
    26.Lock，tryLock，lockInterruptibly区别
    27.CyclicBarrier底层实现和原理
        ReentrantLock + Condition实现,循环使用分代实现,屏障使用condition+count 如果count=0,说明都到达了await,
        最后一个到达的线程唤醒其他所有线程,不再进入await,直接return
        https://www.cnblogs.com/200911/p/6060195.html   (讲解比较详细)
    28.CountDownLatch实现原理
         finally {
            countDownLatch.countDown();
         }
         最好放在finnally里面
         
         继承AQS,初始化state为count,countDown state减少1,await是进入一个死循环,检查state的值是否为0,为0跳出循环
    29.独占锁的获取和释放流程
        获取
            调用入口方法acquire(arg)
            调用模版方法tryAcquire(arg)尝试获取锁，若成功则返回，若失败则走下一步
            将当前线程构造成一个Node节点，并利用CAS将其加入到同步队列到尾部，然后该节点对应到线程进入自旋状态
            自旋时，首先判断其前驱节点释放为头节点&是否成功获取同步状态，两个条件都成立，则将当前线程的节点设置为头节点，如果不是，则利用LockSupport.park(this)将当前线程挂起 ,等待被前驱节点唤醒
        释放
            调用入口方法release(arg)
            调用模版方法tryRelease(arg)释放同步状态
            获取当前节点的下一个节点
            利用LockSupport.unpark(currentNode.next.thread)唤醒后继节点（接获取的第四步） 
    30.共享锁的获取和释放流程
        获取锁
            调用acquireShared(arg)入口方法
            进入tryAcquireShared(arg)模版方法获取同步状态，如果返返回值>=0，则说明同步状态(state)有剩余，获取锁成功直接返回
            如果tryAcquireShared(arg)返回值<0，说明获取同步状态失败，向队列尾部添加一个共享类型的Node节点，随即该节点进入自旋状态
            自旋时，首先检查前驱节点释放为头节点&tryAcquireShared()是否>=0(即成功获取同步状态)
            如果是，则说明当前节点可执行，同时把当前节点设置为头节点，并且唤醒所有后继节点
            如果否，则利用LockSupport.unpark(this)挂起当前线程，等待被前驱节点唤醒
        释放锁
            调用releaseShared(arg)模版方法释放同步状态
            如果释放成，则遍历整个队列，利用LockSupport.unpark(nextNode.thread)唤醒所有后继节点
        
    31.独占锁和共享锁在实现上的区别
        独占锁的同步状态值为1，即同一时刻只能有一个线程成功获取同步状态
        共享锁的同步状态>1，取值由上层同步组件确定
        独占锁队列中头节点运行完成后释放它的直接后继节点
        共享锁队列中头节点运行完成后释放它后面的所有节点
        共享锁中会出现多个线程（即同步队列中的节点）同时成功获取同步状态的情况


        

###结束

     https://github.com/Snailclimb/JavaGuide/blob/master/Java%E7%9B%B8%E5%85%B3/Multithread/AQS.md
     https://blog.csdn.net/sinat_35512245/article/details/59056120  (常用题)
     
     如何停止一个线程
        https://www.jianshu.com/p/e0ff2e420ab6  (讲的很清楚)
        interrupted + return
        
        interrupt() 它基于「一个线程不应该由其他线程来强制中断或停止，而是应该由线程自己自行停止。」思想，是一个比较温柔的做法，
                    它更类似一个标志位。其实作用不是中断线程，而是「通知线程应该中断了」，具体到底中断还是继续运行，应该由被通知的线程自己处理。
                    interrupt() 并不能真正的中断线程，这点要谨记。需要被调用的线程自己进行配合才行。也就是说，一个线程如果有被中断的需求，
                    那么就需要这样做：
                        在正常运行任务时，经常检查本线程的中断标志位，如果被设置了中断标志就自行停止线程。
                        在调用阻塞方法时正确处理InterruptedException异常。（例如：catch异常后就结束线程。）
                    
                    首先讲 interrupt() 方法：
                        interrupt 中断操作时，非自身打断需要先检测是否有中断权限，这由jvm的安全机制配置；
                        如果线程处于sleep, wait, join 等状态，那么线程将立即退出被阻塞状态，并抛出一个InterruptedException异常；
                        如果线程处于I/O阻塞状态，将会抛出ClosedByInterruptException（IOException的子类）异常；
                        如果线程在Selector上被阻塞，select方法将立即返回；
                        如果非以上情况，将直接标记 interrupt 状态；
                        
                        注意：interrupt 操作不会打断所有阻塞，只有上述阻塞情况才在jvm的打断范围内，如处于锁阻塞的线程，不会受 interrupt 中断；
        
     守护线程
        用户线程：运行在前台，执行具体的任务，如程序的主线程、连接网络的子线程等都是用户线程
        守护线程：运行在后台，为其他前台线程服务.也可以说守护线程是JVM中非守护线程的 “佣人”。
        特点：一旦所有用户线程都结束运行，守护线程会随JVM一起结束工作
        应用：数据库连接池中的检测线程，JVM虚拟机启动后的检测线程
        最常见的守护线程：垃圾回收线程
        
     对于CountDownLatch来说，重点是“一个线程（多个线程）等待”，而其他的N个线程在完成“某件事情”之后，可以终止，
     也可以等待。而对于CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。
     CountDownLatch是计数器，线程完成一个记录一个，只不过计数不是递增而是递减，而CyclicBarrier更像是一个阀门，需要所有线程都到达，
     阀门才能打开，然后继续执行。
     
     AQS的全称为（AbstractQueuedSynchronizer）
        https://blog.csdn.net/zhangdong2012/article/details/79983404    (写得比较明白)
         AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。
         如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，
         即将暂时获取不到锁的线程加入到队列中。
         CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。
         AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node）来实现锁的分配。
     
        什么是AQS
            AQS是AbustactQueuedSynchronizer的简称，它是一个Java提高的底层同步工具类，用一个int类型的变量表示同步状态，并提供了一系列的
            CAS操作来管理这个同步状态。AQS的主要作用是为Java中的并发同步组件提供统一的底层支持，例如ReentrantLock，CountdowLatch
            就是基于AQS实现的，用法是通过继承AQS实现其模版方法，然后将子类作为同步组件的内部类。
        
        同步队列
            同步队列是AQS很重要的组成部分，它是一个双端队列，遵循FIFO原则，主要作用是用来存放在锁上阻塞的线程，当一个线程尝试获取锁时，
            如果已经被占用，那么当前线程就会被构造成一个Node节点假如到同步队列的尾部，队列的头节点是成功获取锁的节点，当头节点线程是否锁时，
            会唤醒后面的节点并释放当前头节点的引用。 
        
        独占锁和共享锁在实现上的区别
            独占锁的同步状态值为1，即同一时刻只能有一个线程成功获取同步状态
            共享锁的同步状态>1，取值由上层同步组件确定
            独占锁队列中头节点运行完成后释放它的直接后继节点
            共享锁队列中头节点运行完成后释放它后面的所有节点
            共享锁中会出现多个线程（即同步队列中的节点）同时成功获取同步状态的情况
        
        重入锁的最主要逻辑就锁判断上次获取锁的线程是否为当前线程。
        
        非公平锁
            非公平锁是指当锁状态为可用时，不管在当前锁上是否有其他线程在等待，新近线程都有机会抢占锁。
            上述代码即为非公平锁和核心实现，可以看到只要同步状态为0，任何调用lock的线程都有可能获取到锁，而不是按照锁请求的FIFO原则来进行的。
        公平锁
            从上面的代码中可以看出，公平锁与非公平锁的区别仅在于是否判断当前节点是否存在前驱节点!hasQueuedPredecessors() &&，由AQS可知，
            如果当前线程获取锁失败就会被加入到AQS同步队列中，那么，如果同步队列中的节点存在前驱节点，也就表明存在线程比当前节点线程更早
            的获取锁，故只有等待前面的线程释放锁后才能获取锁。
        
     
     *ReentrantLock 实现原理
     https://crossoverjie.top/%2F2018%2F01%2F25%2FReentrantLock%2F

     https://www.cnblogs.com/wxd0108/p/5479442.html
     目的，那就是更好的利用cpu的资源
     
     并行：多个cpu实例或者多台机器同时执行一段处理逻辑，是真正的同时。
     并发：通过cpu调度算法，让用户看上去同时执行，实际上从cpu操作层面不是真正的同时。并发往往在场景中有公用的资源，
     那么针对这个公用的资源往往产生瓶颈，我们会用TPS或者QPS来反应这个系统的处理能力。
     
     *线程的状态
     
        new -> runable -> running -> Blocked -> Dead
        
        //当前线程可转让cpu控制权，让别的就绪状态线程运行（切换）
        public static Thread.yield() 
        //暂停一段时间
        public static Thread.sleep()  
        //在一个线程中调用other.join(),将等待other执行完后才继续本线程。　　　　
        public join()
        //后两个函数皆可以被打断
        public interrupte()
        
        Synchronized块中
            wait()
            notify()
            
     *volatile
         多线程的内存模型：main memory（主存）、working memory（线程栈），在处理数据时，线程会把值从主存load到本地栈，
         完成操作后再save回去(volatile关键词的作用：每次针对该变量的操作都激发一次load and save)。
         
         针对多线程使用的变量如果不是volatile或者final修饰的，很有可能产生不可预知的结果（另一个线程修改了这个值，
         但是之后在某线程看到的是修改之前的值）。其实道理上讲同一实例的同一属性本身只有一个副本。但是多线程是会缓存值的，
         本质上，volatile就是不去缓存，直接取值。在线程安全的情况下加volatile会牺牲性能。
     
     
     *如何获取线程中的异常
     setUncaughtExceptionHandler
     
     *线程组：
         线程组存在的意义，首要原因是安全。java默认创建的线程都是属于系统线程组，
         而同一个线程组的线程是可以相互修改对方的数据的。但如果在不同的线程组中，那么就不能“跨线程组”修改数据，
         可以从一定程度上保证数据安全。

     *Runnable
     *Callable
          future模式：并发模式的一种，可以有两种形式，即无阻塞和阻塞，分别是isDone和get。其中Future对象用来存放该线程的返回值以及状态
          
          ExecutorService e = Executors.newFixedThreadPool(3);
           //submit方法有多重参数版本，及支持callable也能够支持runnable接口类型.
          Future future = e.submit(new myCallable());
          future.isDone() //return true,false 无阻塞
          future.get() // return 返回值，阻塞直到该线程运行结束
           
     submit和execute区别
        1.对返回值的处理不同
            execute方法不关心返回值。
            submit方法有返回值，Future.
        2.对异常的处理不同
            excute方法会抛出异常。
            sumbit方法不会抛出异常。除非你调用Future.get()
               
     *ThreadLocal类
         用处：保存线程的独立变量。对一个线程类（继承自Thread)
         当使用ThreadLocal维护变量时，ThreadLocal为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，
         而不会影响其它线程所对应的副本。常用于用户登录控制，如记录session信息。
     
        每个Thread的对象都有一个ThreadLocalMap，当创建一个ThreadLocal的时候，就会将该ThreadLocal对象添加到该Map中，
        其中键就是ThreadLocal，值可以是任意类型
        
        *在线程池环境下，由于线程是一直运行且复用的，使用ThreadLocal<T>时会出现这个任务看到上个任务ThreadLocal变量值以及内存泄露等问题，
        解决方法就是在当前任务执行完后将ThreadLocal变量remove或设置为初始值
        
        ThreadLocalMap里面对Key的引用是弱引用。那么，就存在这样的情况：当释放掉对threadlocal对象的强引用后，map里面的value没有被回收，
        但却永远不会被访问到了，因此ThreadLocal存在着内存泄露问题
        Java为了最小化减少内存泄露的可能性和影响，在ThreadLocal进行get、set操作时会清除线程Map里所有key为null的value。所以最怕的情况就是，
        ThreadLocal对象设null了，开始发生“内存泄露”，然后使用线程池，线程结束后被放回线程池中而不销毁，那么如果这个线程一直不被使用或者分配
        使用了又不再调用get/set方法，那么这个期间就会发生真正的内存泄露。因此，最好的做法是：在不使用该ThreadLocal对象时，及时调用该对象的
        remove方法去移除ThreadLocal.ThreadLocalMap中的对应Entry.
        
        
     
     *原子类（AtomicInteger、AtomicBoolean……）
        如果使用atomic wrapper class如atomicInteger，或者使用自己保证原子的操作，则等同于synchronized
     
         //返回值为boolean
         AtomicInteger.compareAndSet(int expect,int update)
         该方法可用于实现乐观锁
         ABA问题
         用AtomicStampedReference解决
         https://www.cnblogs.com/java20130722/p/3206742.html
     
     
     
     *Lock类　
          lock: 在java.util.concurrent包内。共有三个实现：
          
          ReentrantLock
          ReentrantReadWriteLock.ReadLock
          ReentrantReadWriteLock.WriteLock
          jdk 1.8 StampedLock(解决读写锁,写锁饥饿问题)
          https://segmentfault.com/a/1190000015808032?utm_source=tag-newest
          
          多condition,sign(),await()
          https://www.cnblogs.com/Wanted-Tao/p/6378942.html
          
          主要目的是和synchronized一样， 两者都是为了解决同步问题，处理资源争端而产生的技术。功能类似但有一些区别。
          
          
     *lock和synchronized的区别
          1.提供多种加锁方案，lock 阻塞式, trylock 无阻塞式, lockInterruptily 可打断式， 还有trylock的带超时时间版本。
          2.首先synchronized是java内置关键字，在jvm层面，Lock是个java类；
          3.synchronized无法判断是否获取锁的状态，Lock可以判断是否获取到锁；
          4.synchronized会自动释放锁(a 线程执行完同步代码会释放锁 ；b 线程执行过程中发生异常会释放锁)，Lock需在finally中手工释放锁（unlock()方法释放锁），否则容易造成线程死锁；
          5.用synchronized关键字的两个线程1和线程2，如果当前线程1获得锁，线程2线程等待。如果线程1阻塞，线程2则会一直等待下去，而Lock锁就不一定会等待下去，如果尝试获取不到锁，线程可以不用一直等待就结束了；
          6.synchronized的锁可重入、不可中断、非公平，而Lock锁可重入、可判断、可公平（两者皆可）
          7.Lock锁适合大量同步的代码的同步问题，synchronized锁适合代码少量的同步问题。
     
     ReentrantReadWriteLock
         可重入读写锁（读写锁的一个实现）　
         
         　ReentrantReadWriteLock lock = new ReentrantReadWriteLock()
         　　ReadLock r = lock.readLock();
         　　WriteLock w = lock.writeLock();
         两者都有lock,unlock方法。写写，写读互斥；读读不互斥。可以实现并发读的高效线程安全代码
         
     synchronized 关键字有两种作用域：
     https://www.cnblogs.com/yangfei-beijing/p/5952934.html
        1> 某个对象实例内，synchronized aMethod(){}关键字可以防止多个线程访问对象的synchronized方法
        （如果一个对象有多个synchronized方法，只要一个线程访问了其中的一个synchronized方法，其它线程不能
        同时访问这个对象中任何一个synchronized方法）。这时，不同的对象实例的synchronized方法是不相干扰的。
        也就是说，其它线程照样可以同时访问相同类的另一个对象实例中的synchronized方法.
        
        2> 是某个类的范围，synchronized static aStaticMethod{}防止多个线程同时访问这个类中的synchronized
         static 方法。它可以对类的所有对象实例起作用。
         
     *JVM中锁的优化
        锁机制升级流程
        偏向锁--》轻量级锁--》重量级锁
     
     
     *容器类
         这里就讨论比较常用的两个：
         
         BlockingQueue
         ConcurrentHashMap
     
     *管理类
         管理类的概念比较泛，用于管理线程，本身不是多线程的，但提供了一些机制来利用上述的工具做一些封装。
         了解到的值得一提的管理类：ThreadPoolExecutor和 JMX框架下的系统级管理类 ThreadMXBean
         ThreadPoolExecutor
         如果不了解这个类，应该了解前面提到的ExecutorService，开一个自己的线程池非常方便：
         
     复制代码
     ExecutorService e = Executors.newCachedThreadPool();
         ExecutorService e = Executors.newSingleThreadExecutor();
         ExecutorService e = Executors.newFixedThreadPool(3);
         // 第一种是可变大小线程池，按照任务数来分配线程，
         // 第二种是单线程池，相当于FixedThreadPool(1)
         // 第三种是固定大小线程池。
         // 然后运行
         e.execute(new MyRunnableImpl());
         
     
     *读写锁
     https://blog.csdn.net/zwjyyy1203/article/details/80231303
     
     *CountDownLatch
     　　CountDownLatch可以理解为一个计数器在初始化时设置初始值，当一个线程需要等待某些操作先完成时，需要调用await()方法。
        这个方法让线程进入休眠状态直到等待的所有线程都执行完成。每调用一次countDown()方法，内部计数器减1，直到计数器为0时唤醒。
        这个可以理解为特殊的CyclicBarrier。
     
        使用场景
        
        有时候会有这样的需求，多个线程同时工作，然后其中几个可以随意并发执行，但有一个线程需要等其他线程工作结束后，
        才能开始。举个例子，开启多个线程分块下载一个大文件，每个线程只下载固定的一截，最后由另外一个线程来拼接所有的分段，
        那么这时候我们可以考虑使用CountDownLatch来控制并发。
        
     LockSupport
     https://www.jianshu.com/p/9677a754cf60
        通过分别调用这两个方法然后dump线程信息可以看出，带Object的park方法相较于无参的park方法会增加 
        parking to wait for  <0x048c2d18> (a java.lang.String）的信息，这种信息就类似于记录“案发现场”，有助于工程人员能够迅速
        发现问题解决问题。有个有意思的事情是，我们都知道如果使用synchronzed阻塞了线程dump线程时都会有阻塞对象的描述，在java 5
        推出LockSupport时遗漏了这一点，在java 6时进行了补充。还有一点需要需要的是：synchronzed致使线程阻塞，线程会进入到BLOCKED状态，
        而调用LockSupprt方法阻塞线程会致使线程进入到WAITING状态。
        
     **Condition接口及其实现原理
     https://www.jianshu.com/p/c7af7f3fa135     (必看,重要)
        ConditionObject是同步器AbstractQueuedSynchronizer的内部类，因为Condition的操作需要获取相关联的锁，所以作为同步器的内部类
        也较为合理。每个Condition对象都包含着一个队列（以下称为等待队列），该队列是Condition对象实现等待/通知功能的关键。
         
        下面将分析Condition的实现，主要包括：等待队列、等待和通知，下面提到的Condition如果不加说明均指的是ConditionObject。
        
        等待队列
            等待队列是一个FIFO的队列，在队列中的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程，如果一个线程调用
            了Condition.await()方法，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态。事实上，节点的定义复用了同步器中节点
            的定义，也就是说，同步队列和等待队列中节点类型都是同步器的静态内部类AbstractQueuedSynchronizer.Node。
            
            一个Condition包含一个等待队列，Condition拥有首节点（firstWaiter）和尾节点（lastWaiter）。当前线程调用
            Condition.await()方法，将会以当前线程构造节点，并将节点从尾部加入等待队列
            
            如图所示，Condition拥有首尾节点的引用，而新增节点只需要将原有的尾节点nextWaiter指向它，并且更新尾节点即可。上述节点
            引用更新的过程并没有使用CAS保证，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的
        
            在Object的监视器模型上，一个对象拥有一个同步队列和等待队列，而并发包中的Lock（更确切地说是同步器）拥有一个同步队列和多个
            等待队列，其对应关系如图所示。
            
            Condition的实现是同步器的内部类，因此每个Condition实例都能够访问同步器提供的方法，相当于每个Condition都拥有所属同步器的引用。
            
        等待
             调用Condition的await()方法（或者以await开头的方法），会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。当从
            await()方法返回时，当前线程一定获取了Condition相关联的锁。
            
             如果从队列（同步队列和等待队列）的角度看await()方法，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的等待队列中。
            
             调用该方法的线程成功获取了锁的线程，也就是同步队列中的首节点，该方法会将当前线程构造成节点并加入等待队列中，然后释放同步状态，
             唤醒同步队列中的后继节点，然后当前线程会进入等待状态。
             
              当等待队列中的节点被唤醒，则唤醒节点的线程开始尝试获取同步状态。如果不是通过其他线程调用Condition.signal()方法唤醒，
             而是对等待线程进行中断，则会抛出InterruptedException。
             
             同步队列的首节点并不会直接加入等待队列，而是通过addConditionWaiter()方法把当前线程构造成一个新的节点并将其加入等待队列中。
             
        通知
            调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。
            
            调用该方法的前置条件是当前线程必须获取了锁，可以看到signal()方法进行了isHeldExclusively()检查，也就是当前线程必须是获取
            了锁的线程。接着获取等待队列的首节点，将其移动到同步队列并使用LockSupport唤醒节点中的线程。
        
            通过调用同步器的enq(Node node)方法，等待队列中的头节点线程安全地移动到同步队列。当节点移动到同步队列后，当前线程再使用LockSupport唤醒该节点的线程。
            
             被唤醒后的线程，将从await()方法中的while循环中退出（isOnSyncQueue(Node node)方法返回true，节点已经在同步队列中），
            进而调用同步器的acquireQueued()方法加入到获取同步状态的竞争中。
            
             成功获取同步状态（或者说锁）之后，被唤醒的线程将从先前调用的await()方法返回，此时该线程已经成功地获取了锁。
            
             Condition的signalAll()方法，相当于对等待队列中的每个节点均执行一次signal()方法，效果就是将等待队列中所有节点全部移动到
            同步队列中，并唤醒每个节点的线程。
            
        
     并发容器
     https://github.com/Snailclimb/JavaGuide/blob/master/Java%E7%9B%B8%E5%85%B3/Multithread/%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%E6%80%BB%E7%BB%93.md
         ConcurrentHashMap： 线程安全的HashMap
         CopyOnWriteArrayList: 线程安全的List，在读多写少的场合性能非常好，远远好于Vector.
         **ConcurrentLinkedQueue：**高效的并发队列，使用链表实现。可以看做一个线程安全的 LinkedList，这是一个非阻塞队列。
         BlockingQueue: 这是一个接口，JDK内部通过链表、数组等方式实现了这个接口。表示阻塞队列，非常适合用于作为数据共享的通道。
         ConcurrentSkipListMap: 跳表的实现。这是一个Map，使用跳表的数据结构进行快速查找。
         
         1、ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列
         2、LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列
         3、PriorityBlockingQueue：一个支持优先级排序的无解阻塞队列
         4、DelayQueue：一个使用有限级队列实现的无界阻塞队列
         5、SynchronousQueue：一个不存储元素的阻塞队列
         6、LinkedTransferQueue：一个由链表结构组成的无界阻塞队列
         7、LinkedBlockingDeque：一个由链表结构组成的双阻塞队列

    
     如果让你设计一个计数器，你怎么实现？
     http://www.cnblogs.com/williamjie/p/9556079.html
        高性能计数器总结
        AtomicLong ：并发场景下读性能优秀，写性能急剧下降，不适合作为高性能的计数器方案。内存需求量少。
        
        LongAdder ：并发场景下写性能优秀，读性能由于组合求值的原因，不如直接读值的方案，但由于计数器场景写多读少的缘故，
        整体性能在几个方案中最优，是高性能计数器的首选方案。由于 Cells 数组以及缓存行填充的缘故，占用内存较大。
        
        ConcurrentAutoTable ：拥有和 LongAdder 相近的写入性能，读性能则更加不如 LongAdder。它的使用需要引入 JCTools 依赖，相比 Jdk 自带的 LongAdder 并没有优势。但额外说明一点，
        ConcurrentAutoTable 的使用并非局限于计数器场景，其仍然存在很大的价值。
    
        LongAdder newCounter = new LongAdder();
        newCounter.increment();
        System.out.println(newCounter.sum());
        
        LongAdder基本原理
            我们都知道AtomicLong是通过无限循环不停的采取CAS的方法去设置value，直到成功为止。那么当并发数比较多或出现更新热点时，
            就会导致CAS的失败机率变高，重试次数更多，越多的线程重试，CAS失败的机率越高，形成恶性循环，从而降低了效率。而LongAdder
            的原理就是降低对value更新的并发数，也就是将对单一value的变更压力分散到多个value值上，降低单个value的“热度”
             我们知道LongAdder的大致原理之后，再来详细的了解一下它的具体实现，其中也有很多值得借鉴的并发编程的技巧。
            
            
    
     CountDownLatch实现原理
     https://cloud.tencent.com/developer/article/1038486
    
     fail-fast：
        机制是java集合(Collection)中的一种错误机制。当多个线程对同一个集合的内容进行操作时，就可能会产生fail-fast事件。 
        例如：当某一个线程A通过iterator去遍历某集合的过程中，若该集合的内容被其他线程所改变了；那么线程A访问集合时，
        就会抛出ConcurrentModificationException异常，产生fail-fast事件
    
     happens-before:如果两个操作之间具有happens-before 关系，那么前一个操作的结果就会对后面一个操作可见。 
         1.程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 
         2.监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 
         3.volatile变量规则：对一个volatile域的写，happens- before于任意后续对这个volatile域的读。 
         4.传递性：如果A happens- before B，且B happens- before C，那么A happens- before C。 
         5.线程启动规则：Thread对象的start()方法happens- before于此线程的每一个动作。

     Volatile和Synchronized四个不同点： 
         1 粒度不同，前者针对变量 ，后者锁对象和类 
         2 syn阻塞，volatile线程不阻塞 
         3 syn保证三大特性，volatile不保证原子性 
         4 syn编译器优化，volatile不优化 
         volatile具备两种特性： 
         1. 保证此变量对所有线程的可见性，指一条线程修改了这个变量的值，新值对于其他线程来说是可见的，但并不是多线程安全的。 
         2. 禁止指令重排序优化。 
         Volatile如何保证内存可见性: 
         1.当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。 
         2.当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。
         
         同步：就是一个任务的完成需要依赖另外一个任务，只有等待被依赖的任务完成后，依赖任务才能完成。 
         异步：不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，只要自己任务完成了就算完成了，
         被依赖的任务是否完成会通知回来。（异步的特点就是通知）。 
         打电话和发短信来比喻同步和异步操作。 
         阻塞：CPU停下来等一个慢的操作完成以后，才会接着完成其他的工作。 
         非阻塞：非阻塞就是在这个慢的执行时，CPU去做其他工作，等这个慢的完成后，CPU才会接着完成后续的操作。 
         非阻塞会造成线程切换增加，增加CPU的使用时间能不能补偿系统的切换成本需要考虑。
     
     
     CAS（Compare And Swap） 无锁算法： 
        CAS是乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，
        而是被告知这次竞争中失败，并可以再次尝试。CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，
        将内存值V修改为B，否则什么都不做。

     https://www.cnblogs.com/lzh-blogs/p/7477157.html   (比较底层,自旋锁)
     理解锁的基础知识
     
     *锁基础知识
        基础知识之一：锁的类型
            乐观锁
                CAS
            悲观锁
                悲观锁是就是悲观思想，即认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候
                都会上锁，这样别人想读写这个数据就会block直到拿到锁。java中的悲观锁就是Synchronized,AQS框架下的锁则是先尝试cas
                乐观锁去获取锁，获取不到，才会转换为悲观锁，如RetreenLock。

        基础知识之二：java线程阻塞的代价
            java的线程是映射到操作系统原生线程之上的，如果要阻塞或唤醒一个线程就需要操作系统介入，需要在户态与核心态之间切换，这种切换
            会消耗大量的系统资源，因为用户态与内核态都有各自专用的内存空间，专用的寄存器等，用户态切换至内核态需要传递给许多变量、参数
            给内核，内核也需要保护好用户态在切换时的一些寄存器值、变量等，以便内核态调用结束后切换回用户态继续工作。
            
            如果线程状态切换是一个高频操作时，这将会消耗很多CPU处理时间；
            如果对于那些需要同步的简单的代码块，获取锁挂起操作消耗的时间比用户代码执行的时间还要长，这种同步策略显然非常糟糕的。
            synchronized会导致争用不到锁的线程进入阻塞状态，所以说它是java语言中一个重量级的同步操纵，被称为重量级锁，为了缓解
            上述性能问题，JVM从1.5开始，引入了轻量锁与偏向锁，默认启用了自旋锁，他们都属于乐观锁。
            
            明确java线程切换的代价，是理解java中各种锁的优缺点的基础之一
            
                
     **Synchronized实现原理及锁的优化
            https://www.jianshu.com/p/1ea87c152413  (*必读)
            Synchronized实现原理
                1.同步代码块是使用monitorenter和monitorexit指令实现的，会在同步块的区域通过监听器对象去获取锁和释放锁，从而在字节码层面
                来控制同步scope.
                2.同步方法和静态同步方法依靠的是方法修饰符上的ACC_SYNCHRONIZED实现。JVM根据该修饰符来实现方法的同步。当方法调用时，
                调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取monitor，获取成功之后才能执行
                方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个monitor对象
                
            同步的原理
                Java对象头
                    HotSpot虚拟机中，对象在内存中存储分为三块区域：对象头、实例数据和对齐填充
                    HotSpot虚拟机的对象头(Object Header)包括两部分信息:
                        第一部分"Mark Word": 用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、
                        偏向线程ID、偏向时间戳等等.
                        第二部分"Klass Pointer": 对象指向它的类的元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。
                        (数组，对象头中还须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，
                        但是从数组的元数据中无法确定数组的大小。 )
            
            *JVM中锁的优化
                锁机制升级流程
                偏向锁--》轻量级锁--》重量级锁
                    
                    偏向锁
                        大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。
                        当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时
                        不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试
                        成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：
                        如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。
                        注意：当锁有竞争关系的时候，需要解除偏向锁，进入轻量级锁。
                    
                    轻量级锁
                        1>加锁
                            线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，
                            官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，
                            当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。
                        2>解锁
                            轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。
                            如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。
                    
                        自旋的线程在自旋过程中，成功获得资源(即之前获的资源的线程执行完成并释放了共享资源)，则整个状态依然处于轻量级锁的状态，
                        如果自旋失败进入重量级锁的状态，这个时候，自旋的线程进行阻塞，等待之前线程执行完成并唤醒自己。因为自旋会消耗CPU，
                        为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。
                        当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就
                        会进行新一轮的夺锁之争。
                        
                    锁粗化:就是将多次连接在一起的加锁、解锁操作合并为一次，将多个连续的锁扩展成一个范围更大的锁。
                        这里每次调用stringBuffer.append方法都需要加锁和解锁，如果虚拟机检测到有一系列连串的对同一个对象加锁和解锁操作，
                        就会将其合并成一次范围更大的加锁和解锁操作，即在第一次append方法时进行加锁，最后一次append方法结束后进行解锁。
                        
                    锁消除
                        锁消除即删除不必要的加锁操作。根据代码逃逸技术，如果判断到一段代码中，堆上的数据不会逃逸出当前线程，那么可以认为
                        这段代码是线程安全的，不必要加锁。逃逸分析和锁消除分别可以使用参数-XX:+DoEscapeAnalysis和-XX:+EliminateLocks
                        (锁消除必须在-server模式下)开启
                        
                    适应性自旋
                        当前锁处于膨胀，会进行自旋。自旋是需要消耗CPU的，如果一直获取不到锁的话，那线程一直处在自旋状态，消耗CPU资源。
                        为了解决这个问题JDK采用—适应性自旋，线程如果自旋成功了，则下次自旋的次数会更多，如果自旋失败了，则自旋的次数就会减少。
                        另外自旋虽然会占用CPU资源，但不会一直占用CPU资源，每隔一段时间会通过os::NakedYield方法放弃CPU资源，
                        或通过park方法挂起；如果其他线程完成锁的膨胀操作，则退出自旋并返回
                    

     start() 和 run()的区别
         1。start():
             我们先来看看API中对于该方法的介绍：
                  使该线程开始执行；Java 虚拟机调用该线程的 run 方法。
                  结果是两个线程并发地运行；当前线程（从调用返回给 start 方法）和另一个线程（执行其 run 方法）。
                  多次启动一个线程是非法的。特别是当线程已经结束执行后，不能再重新启动。
             用start方法来启动线程，真正实现了多线程运行，这时无需等待run方法体代码执行完毕而直接继续执行下面的代码。通过调用Thread类的 start()
             方法来启动一个线程，这时此线程处于就绪（可运行）状态，并没有运行，一旦得到cpu时间片，就开始执行run()方法，这里方法 run()称为线程体，
             它包含了要执行的这个线程的内容，Run方法运行结束，此线程随即终止。
          
         2。run():
             我们还是先看看API中对该方法的介绍：
                   如果该线程是使用独立的 Runnable 运行对象构造的，则调用该 Runnable 对象的 run 方法；否则，该方法不执行任何操作并返回。
                 Thread 的子类应该重写该方法。
             run()方法只是类的一个普通方法而已，如果直接调用Run方法，程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，
             还是要等待run方法体执行完毕后才可继续执行下面的代码，这样就没有达到写线程的目的。
         
         3。总结：
            调用start方法方可启动线程，而run方法只是thread的一个普通方法调用，还是在主线程里执行。
            
     如何检测死锁？怎么预防死锁？
        死锁的定义：
            多个进行相互等待对方资源，在得到所有资源继续运行之前，都不会释放自己已有的资源，这样造成了循环等待的现象，称为死锁。
        
        产生死锁的四大必要条件：
            ①资源互斥/资源不共享
            ②占有和等待/请求并保持
            ③资源不可剥夺
            ④环路等待
        防止死锁的方法
            1、破坏互斥条件
            2、破坏请求并保持条件
            3、破坏不可剥夺条件
            4、破坏循环等待条件
        避免死锁的算法
            
        死锁的检测
            使用死锁定理判断
            死锁定理： 
             ①如果资源分配图中没有环路，则系统没有死锁； 
             ②如果资源分配图中出现了环路，则系统可能有死锁
             
    
        
    


        
     *总结
        你讲讲线程池的实现原理?比如现在设置coreSize=5，maxSize=10，blockQueueSize=10，依次提交6个比较耗时的任务，线程池是如何执行的？
        如果让你设计一个计数器，你怎么实现？
        多线程同步和互斥有哪几种实现方法？
        悲观锁和乐观锁有什么区别？
        Java的锁实现机制，使用场景分析
        ReentranLock源码，设计原理，整体过程
        volatile的实现原理
        AQS的实现过程
        CountDownLatch实现原理
        start和run的区别
    
## 7.线程池

###目录
    1.线程池参数,及计算方法
    2.分析线程池的实现原理和线程的调度过程？
        待执行的线程放入阻塞队列,不停从队列中take,同时判断参数,决定是否创建新线程来处理
    3.线程池拒绝策略
    
    
###结束

    线程池的优点
        重用线程池中的线程,减少因对象创建,销毁所带来的性能开销;
        能有效的控制线程的最大并发数,提高系统资源利用率,同时避免过多的资源竞争,避免堵塞;
        能够多线程进行简单的管理,使线程的使用简单、高效。

    由浅入深理解Java线程池及线程池的如何使用
    https://www.cnblogs.com/superfj/p/7544971.html
    
    线程池参数及拒绝策略 
    https://blog.csdn.net/wang_rrui/article/details/78541786
    
    线程池的各种使用场景
        （1）高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换
        （2）并发不高、任务执行时间长的业务要区分开看：
               a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，
                    可以加大线程池中的线程数目，让CPU处理更多的业务
               b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，
                    减少线程上下文的切换
        （3）并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存
            是第一步，增加服务器是第二步，至于线程池的设置，设置参考（2）。最后，业务执行时间长的问题，也可能需要分析一下，
            看看能不能使用中间件对任务进行拆分和解耦
                
    *ThreadPoolExecutor执行顺序：
         线程池按以下行为执行任务
        当线程数小于核心线程数时，创建线程。
        当线程数大于等于核心线程数，且任务队列未满时，将任务放入任务队列。
        当线程数大于等于核心线程数，且任务队列已满
        若线程数小于最大线程数，创建线程
        若线程数等于最大线程数，抛出异常，拒绝任务
    
    *如何设置参数
        默认值
            corePoolSize=1
            queueCapacity=Integer.MAX_VALUE
            maxPoolSize=Integer.MAX_VALUE
            keepAliveTime=60s
            allowCoreThreadTimeout=false
            rejectedExecutionHandler=AbortPolicy()
        如何来设置
            tasks ：每秒的任务数，假设为500~1000
            taskcost：每个任务花费时间，假设为0.1s
            responsetime：系统允许容忍的最大响应时间，假设为1s
            
            corePoolSize = 每秒需要多少个线程处理？
                1/taskcost = 一个线程每秒处理多少个任务
                threadcount = tasks/(1/taskcost) =tasks*taskcout =  (500~1000)*0.1 = 50~100 个线程。corePoolSize设置应该大于50
                根据8020原则，如果80%的每秒任务数小于800，那么corePoolSize设置为80即可
            
            queueCapacity = (coreSizePool/taskcost)*responsetime
                计算可得 queueCapacity = 80/0.1*1 = 80。意思是队列里的线程可以等待1s，超过了的需要新开线程来执行
                切记不能设置为Integer.MAX_VALUE，这样队列会很大，线程数只会保持在corePoolSize大小，当任务陡增时，不能新开线程来执行，响应时间会随之陡增。
                
            maxPoolSize = (max(tasks)- queueCapacity)/(1/taskcost)
                计算可得 maxPoolSize = (1000-80)/10 = 92
                （最大任务数-队列容量）/每个线程每秒处理能力 = 最大线程数
                rejectedExecutionHandler：根据具体情况来决定，任务不重要可丢弃，任务重要则要利用一些缓冲机制来处理
                keepAliveTime和allowCoreThreadTimeout采用默认通常能满足
                
            以上都是理想值，实际情况下要根据机器性能来决定。如果在未达到最大线程数的情况机器cpu load已经满了，则需要通过升级硬件（呵呵）
            和优化代码，降低taskcost来处理。
             
    线程池实现的原理
    https://blog.csdn.net/aiengelangte/article/details/80397952 (后面讲原理部分推荐)
        如果只讲线程池的使用，那这篇博客没有什么大的价值，充其量也就是熟悉Executor相关API的过程。线程池的实现过程没有用到
        Synchronized关键字，用的都是Volatile,Lock和同步(阻塞)队列,Atomic相关类，FutureTask等等，因为后者的性能更优。
        理解的过程可以很好的学习源码中并发控制的思想。
        
        在ThreadPoolExecutor主要Worker类来控制线程的复用。看下Worker类简化后的代码，这样方便理解：
        
        private final class Worker implements Runnable {
         
        	final Thread thread;
         
        	Runnable firstTask;
         
        	Worker(Runnable firstTask) {
        		this.firstTask = firstTask;
        		this.thread = getThreadFactory().newThread(this);
        	}
         
        	public void run() {
        		runWorker(this);
        	}
         
        	final void runWorker(Worker w) {
        		Runnable task = w.firstTask;
        		w.firstTask = null;
        		while (task != null || (task = getTask()) != null){
        		task.run();
        	}
        }
        
        个人总结:待执行的线程放入阻塞队列,不停从队列中take,同事判断参数,决定是否创建新线程来处理
            
            
    
## 8.面试题集锦

###8.0 难题集中营
    todo 列表
        1.上千万数据的IP取前100个出现次数最多的     (todo)
        https://blog.csdn.net/Iloveyougirls/article/details/81814524
        2.Spring Boot 启动 流程(*)
        https://juejin.im/post/5b679fbc5188251aad213110#heading-0
        3.
        

###8.1 Java高级工程师面试题总结及参考答案
    https://www.cnblogs.com/cxxjohnson/p/10118067.html
    
        限流，分布式锁，UUID
        我现在要做一个限流功能, 怎么做?
        令牌桶
        这个限流要做成分布式的, 怎么做?
        令牌桶维护到 Redis 里，每个实例起一个线程抢锁，抢到锁的负责定时放令牌
        怎么抢锁?
        Redis setnx
        锁怎么释放?
        抢到锁后设置过期时间，线程本身退出时主动释放锁，假如线程卡住了，锁过期那么其它线程可以继续抢占
        加了超时之后有没有可能在没有释放的情况下, 被人抢走锁
        有可能，单次处理时间过长，锁泄露
        怎么解决?
        换 zk，用心跳解决
        不用 zk 的心跳, 可以怎么解决这个问题呢?
        每次更新过期时间时，Redis 用 MULTI 做 check-and-set 检查更新时间是否被其他线程修改了，假如被修改了，说明锁已经被抢走，放弃这把锁
        假如这个限流希望做成可配置的, 需要有一个后台管理系统随意对某个 api 配置全局流量, 怎么做？
        在 Redis 里存储每个 API 的令牌桶 key，假如存在这个 key，则需要按上述逻辑进行限流
        某一个业务中现在需要生成全局唯一的递增 ID, 并发量非常大, 怎么做
        snowflake (这个其实答得不好，snowflake 无法实现全局递增，只能实现全局唯一，单机递增，面试结束后就想到了类似 TDDL 那样一次取一个 ID 段，放在本地慢慢分配的策略）
        算法题, M*N 横向递增矩阵找指定数
        只想到 O(M+N)的解法
        
        Java 中 HashMap 的存储, 冲突, 扩容, 并发访问分别是怎么解决的
        Hash 表，拉链法（长度大于8变形为红黑树）,扩容*2 rehash，并发访问不安全
        
        拉链法中链表过长时变形为红黑树有什么优缺点?
        优点：O(LogN) 的读取速度更快；缺点：插入时有 Overhead，O(LogN) 插入，旋转维护平衡
        HashMap 的并发不安全体现在哪?
        拉链法解决冲突，插入链表时不安全，并发操作可能导致另一个插入失效
        https://www.cnblogs.com/qiumingcheng/p/5259892.html
        
        HashMap 在扩容时, 对读写操作有什么特殊处理?
        
        知道 CAS 吗? Java 中 CAS 是怎么实现的?
        Compare and Swap，一种乐观锁的实现，可以称为"无锁"(lock-free)，CAS 由于要保证原子性无法由 JVM 本身实现，
        需要调用对应 OS 的指令(这块其实我不了解细节)
    
###8.2 常规面试题2
        Java基础
            1、List 和 Set 的区别
            2、HashSet 是如何保证不重复的
            3、HashMap 是线程安全的吗，为什么不是线程安全的（最好画图说明多线程环境下不安全）?
            4、HashMap 的扩容过程
            5、HashMap 1.7 与 1.8 的 区别，说明 1.8 做了哪些优化，如何优化的？
            6、final finally finalize
            7、强引用 、软引用、 弱引用、虚引用
            8、Java反射
            9、Arrays.sort 实现原理和 Collection 实现原理
            10、LinkedHashMap的应用
            11、cloneable接口实现原理
            12、异常分类以及处理机制
            13、wait和sleep的区别
            14、数组在内存中如何分配
        Java 并发
            1、synchronized 的实现原理以及锁优化？
            2、volatile 的实现原理？
            3、Java 的信号灯？
            4、synchronized 在静态方法和普通方法的区别？
            5、怎么实现所有线程在等待某个事件的发生才会去执行？
            6、CAS？CAS 有什么缺陷，如何解决？
            7、synchronized 和 lock 有什么区别？
            8、Hashtable 是怎么加锁的 ？
            9、HashMap 的并发问题？
            10、ConcurrenHashMap 介绍？1.8 中为什么要用红黑树？
            11、AQS
            12、如何检测死锁？怎么预防死锁？
            13、Java 内存模型？
            14、如何保证多线程下 i++ 结果正确？
            15、线程池的种类，区别和使用场景？
            16、分析线程池的实现原理和线程的调度过程？
            17、线程池如何调优，最大数目如何确认？
            18、ThreadLocal原理，用的时候需要注意什么？
            19、CountDownLatch 和 CyclicBarrier 的用法，以及相互之间的差别?
            20、LockSupport工具
            21、Condition接口及其实现原理
            22、Fork/Join框架的理解
            23、分段锁的原理,锁力度减小的思考
            24、八种阻塞队列以及各个阻塞队列的特性
        Spring
            1、BeanFactory 和 FactoryBean？
            2、Spring IOC 的理解，其初始化过程？
            3、BeanFactory 和 ApplicationContext？
            4、Spring Bean 的生命周期，如何被管理的？
            5、Spring Bean 的加载过程是怎样的？
            6、如果要你实现Spring AOP，请问怎么实现？
            7、如果要你实现Spring IOC，你会注意哪些问题？
            8、Spring 是如何管理事务的，事务管理机制？
            9、Spring 的不同事务传播行为有哪些，干什么用的？
            10、Spring 中用到了那些设计模式？
            11、Spring MVC 的工作原理？
            12、Spring 循环注入的原理？
            13、Spring AOP的理解，各个术语，他们是怎么相互工作的？
            14、Spring 如何保证 Controller 并发的安全？
        Netty
            1、BIO、NIO和AIO
            2、Netty 的各大组件
            3、Netty的线程模型
            4、TCP 粘包/拆包的原因及解决方法
            5、了解哪几种序列化协议？包括使用场景和如何去选择
            6、Netty的零拷贝实现
            7、Netty的高性能表现在哪些方面
        分布式相关
            1、Dubbo的底层实现原理和机制
            2、描述一个服务从发布到被消费的详细过程
            3、分布式系统怎么做服务治理
            4、接口的幂等性的概念
            5、消息中间件如何解决消息丢失问题
            6、Dubbo的服务请求失败怎么处理
            7、重连机制会不会造成错误
            8、对分布式事务的理解
            9、如何实现负载均衡，有哪些算法可以实现？
            10、Zookeeper的用途，选举的原理是什么？
            11、数据的垂直拆分水平拆分。
            12、zookeeper原理和适用场景
            13、zookeeper watch机制
            14、redis/zk节点宕机如何处理
            15、分布式集群下如何做到唯一序列号
            16、如何做一个分布式锁
            17、用过哪些MQ，怎么用的，和其他mq比较有什么优缺点，MQ的连接是线程安全的吗
            18、MQ系统的数据如何保证不丢失
            19、列举出你能想到的数据库分库分表策略；分库分表后，如何解决全表查询的问题
            20、zookeeper的选举策略
            21、全局ID
        数据库
            1、mysql分页有什么优化
            2、悲观锁、乐观锁
            3、组合索引，最左原则
            4、mysql 的表锁、行锁
            5、mysql 性能优化
            6、mysql的索引分类：B+，hash；什么情况用什么索引
            7、事务的特性和隔离级别
        缓存
            1、Redis用过哪些数据数据，以及Redis底层怎么实现
            2、Redis缓存穿透，缓存雪崩
            3、如何使用Redis来实现分布式锁
            4、Redis的并发竞争问题如何解决
            5、Redis持久化的几种方式，优缺点是什么，怎么实现的
            6、Redis的缓存失效策略
            7、Redis集群，高可用，原理
            8、Redis缓存分片
            9、Redis的数据淘汰策略
        JVM
            1、详细jvm内存模型
            2、讲讲什么情况下回出现内存溢出，内存泄漏？
            3、说说Java线程栈
            4、JVM 年轻代到年老代的晋升过程的判断条件是什么呢？
            5、JVM 出现 fullGC 很频繁，怎么去线上排查问题？
            6、类加载为什么要使用双亲委派模式，有没有什么场景是打破了这个模式？
            7、类的实例化顺序
            8、JVM垃圾回收机制，何时触发MinorGC等操作
            9、JVM 中一次完整的 GC 流程（从 ygc 到 fgc）是怎样的
            10、各种回收器，各自优缺点，重点CMS、G1
            11、各种回收算法
            12、OOM错误，stackoverflow错误，permgen space错误
    
###8.3 常规面试题3
        Java常见面试题总结
        一、Java基础
            1、String类为什么是final的。
            2、HashMap的源码，实现原理，底层结构。
            3、说说你知道的几个Java集合类：list、set、queue、map实现类咯。。。
            4、描述一下ArrayList和LinkedList各自实现和区别
            5、Java中的队列都有哪些，有什么区别。
            6、反射中，Class.forName和classloader的区别
            7、Java7、Java8的新特性(baidu问的,好BT)
            8、Java数组和链表两种结构的操作效率，在哪些情况下(从开头开始，从结尾开始，从中间开始)，哪些操作(插入，查找，删除)的效率高
            9、Java内存泄露的问题调查定位：jmap，jstack的使用等等
            10、string、stringbuilder、stringbuffer区别
            11、hashtable和hashmap的区别
            13、异常的结构，运行时异常和非运行时异常，各举个例子
            14、String a= “abc” String b = "abc" String c = new String("abc") String d = "ab" + "c" .他们之间用 == 比较的结果
            15、String 类的常用方法
            16、Java 的引用类型有哪几种
            17、抽象类和接口的区别
            18、java的基础类型和字节大小。
            19、Hashtable,HashMap,ConcurrentHashMap 底层实现原理与线程安全问题（建议熟悉 jdk 源码，才能从容应答）
            20、如果不让你用Java Jdk提供的工具，你自己实现一个Map，你怎么做。说了好久，说了HashMap源代码，如果我做，就会借鉴HashMap的原理，说了一通HashMap实现
            21、 Hash冲突怎么办？哪些解决散列冲突的方法？
            22、HashMap冲突很厉害，最差性能，你会怎么解决?从O（n）提升到log（n）咯，用二叉排序树的思路说了一通
            23、rehash
            24、hashCode() 与 equals() 生成算法、方法怎么重写
         
        二、Java IO
            1、讲讲IO里面的常见类，字节流、字符流、接口、实现类、方法阻塞。
            2、讲讲NIO。
            3、String 编码UTF-8 和GBK的区别?
            4、什么时候使用字节流、什么时候使用字符流?
            5、递归读取文件夹下的文件，代码怎么实现
         
        三、Java Web
            1、session和cookie的区别和联系，session的生命周期，多个服务部署时session管理。
            2、servlet的一些相关问题
            3、webservice相关问题
            4、jdbc连接，forname方式的步骤，怎么声明使用一个事务。举例并具体代码
            5、无框架下配置web.xml的主要配置内容
            6、jsp和servlet的区别
         
        四、JVM
            1、Java的内存模型以及GC算法
            2、jvm性能调优都做了什么
            3、介绍JVM中7个区域，然后把每个区域可能造成内存的溢出的情况说明
            4、介绍GC 和GC Root不正常引用。
            5、自己从classload 加载方式，加载机制说开去，从程序运行时数据区，讲到内存分配，讲到String常量池，讲到JVM垃圾回收机制，算法，hotspot。反正就是各种扩展
            6、jvm 如何分配直接内存， new 对象如何不分配在堆而是栈上，常量池解析
            7、数组多大放在 JVM 老年代（不只是设置 PretenureSizeThreshold ，问通常多大，没做过一问便知）
            8、老年代中数组的访问方式
            9、GC 算法，永久代对象如何 GC ， GC 有环怎么处理
            10、谁会被 GC ，什么时候 GC
            11、如果想不被 GC 怎么办
            12、如果想在 GC 中生存 1 次怎么办
         
        五、开源框架
            1、hibernate和ibatis的区别
            2、讲讲mybatis的连接池。
            3、spring框架中需要引用哪些jar包，以及这些jar包的用途
            4. springMVC的原理
            5、springMVC注解的意思
            6、spring中beanFactory和ApplicationContext的联系和区别
            7、spring注入的几种方式（循环注入）
            8、spring如何实现事物管理的
            9、springIOC
            10、spring AOP的原理
            11、hibernate中的1级和2级缓存的使用方式以及区别原理（Lazy-Load的理解）
            12、Hibernate的原理体系架构，五大核心接口，Hibernate对象的三种状态转换，事务管理。
         
        六、多线程
            1、Java创建线程之后，直接调用start()方法和run()的区别
            2、常用的线程池模式以及不同线程池的使用场景
            3、newFixedThreadPool此种线程池如果线程数达到最大值后会怎么办，底层原理。
            4、多线程之间通信的同步问题，synchronized锁的是对象，衍伸出和synchronized相关很多的具体问题，例如同一个类不同方法都有synchronized锁，一个对象是否可以同时访问。或者一个类的static构造方法加上synchronized之后的锁的影响。
            5、了解可重入锁的含义，以及ReentrantLock 和synchronized的区别
            6、同步的数据结构，例如concurrentHashMap的源码理解以及内部实现原理，为什么他是同步的且效率高
            7、atomicinteger和volatile等线程安全操作的关键字的理解和使用
            8、线程间通信，wait和notify
            9、定时线程的使用
            10、场景：在一个主线程中，要求有大量(很多很多)子线程执行完之后，主线程才执行完成。多种方式，考虑效率。
            11、进程和线程的区别
            12、什么叫线程安全？举例说明
            13、线程的几种状态
            14、并发、同步的接口或方法
            15、HashMap 是否线程安全，为何不安全。 ConcurrentHashMap，线程安全，为何安全。底层实现是怎么样的。
            16、J.U.C下的常见类的使用。 ThreadPool的深入考察； BlockingQueue的使用。（take，poll的区别，put，offer的区别）；原子类的实现。
            17、简单介绍下多线程的情况，从建立一个线程开始。然后怎么控制同步过程，多线程常用的方法和结构
            18、volatile的理解
            19、实现多线程有几种方式，多线程同步怎么做，说说几个线程里常用的方法
         
        七、网络通信
            1、http是无状态通信，http的请求方式有哪些，可以自己定义新的请求方式么。
            2、socket通信，以及长连接，分包，连接异常断开的处理。
            3、socket通信模型的使用，AIO和NIO。
            4、socket框架netty的使用，以及NIO的实现原理，为什么是异步非阻塞。
            5、同步和异步，阻塞和非阻塞。
            6、OSI七层模型，包括TCP,IP的一些基本知识
            7、http中，get post的区别
            8、说说http,tcp,udp之间关系和区别。
            9、说说浏览器访问www.taobao.com，经历了怎样的过程。
            10、HTTP协议、  HTTPS协议，SSL协议及完整交互过程；
            11、tcp的拥塞，快回传，ip的报文丢弃
            12、https处理的一个过程，对称加密和非对称加密
            13、head各个特点和区别
            14、说说浏览器访问www.taobao.com，经历了怎样的过程。
         
        八、数据库MySql
            1、MySql的存储引擎的不同
            2、单个索引、联合索引、主键索引
            3、Mysql怎么分表，以及分表后如果想按条件分页查询怎么办(如果不是按分表字段来查询的话，几乎效率低下，无解)
            4、分表之后想让一个id多个表是自增的，效率实现
            5、MySql的主从实时备份同步的配置，以及原理(从库读主库的binlog)，读写分离
            6、写SQL语句。。。
            7、索引的数据结构，B+树
            8、事务的四个特性，以及各自的特点（原子、隔离）等等，项目怎么解决这些问题
            9、数据库的锁：行锁，表锁；乐观锁，悲观锁
            10、数据库事务的几种粒度；
            11、关系型和非关系型数据库区别
         
        九、设计模式
            1、单例模式：饱汉、饿汉。以及饿汉中的延迟加载,双重检查
            2、工厂模式、装饰者模式、观察者模式。
            3、工厂方法模式的优点（低耦合、高内聚，开放封闭原则）
         
        十、算法
            1、使用随机算法产生一个数，要求把1-1000W之间这些数全部生成。（考察高效率，解决产生冲突的问题）
            2、两个有序数组的合并排序
            3、一个数组的倒序
            4、计算一个正整数的正平方根
            5、说白了就是常见的那些查找、排序算法以及各自的时间复杂度
            6、二叉树的遍历算法
            7、DFS,BFS算法
            9、比较重要的数据结构，如链表，队列，栈的基本理解及大致实现。
            10、排序算法与时空复杂度（快排为什么不稳定，为什么你的项目还在用）
            11、逆波兰计算器
            12、Hoffman 编码
            13、查找树与红黑树
         
        十一、并发与性能调优
            1、有个每秒钟5k个请求，查询手机号所属地的笔试题(记得不完整，没列出)，如何设计算法?请求再多，比如5w，如何设计整个系统?
            2、高并发情况下，我们系统是如何支撑大量的请求的
            3、集群如何同步会话状态
            4、负载均衡的原理
            5、如果有一个特别大的访问量，到数据库上，怎么做优化（DB设计，DBIO，SQL优化，Java优化）
            6、如果出现大面积并发，在不增加服务器的基础上，如何解决服务器响应不及时问题“。
            7、假如你的项目出现性能瓶颈了，你觉得可能会是哪些方面，怎么解决问题。
            8、如何查找 造成 性能瓶颈出现的位置，是哪个位置照成性能瓶颈。
            9、你的项目中使用过缓存机制吗？有没用用户非本地缓存
         
        十二、其他
            1、常用的linux下的命令
            
###8.4 阿里最全面试120道题目
        https://blog.csdn.net/Z_Date/article/details/84307201
        https://www.cnblogs.com/my376908915/p/6825896.html  (带答案)
        
        1. junit用法，before,beforeClass,after, afterClass的执行顺序
        
        一个测试类单元测试的执行顺序为：
        
        @BeforeClass –> @Before –> @Test –> @After –> @AfterClass
        
        每一个测试方法的调用顺序为：
        
        @Before –> @Test –> @After
        
        参考： http://blog.csdn.net/wangpeng047/article/details/9631193
        
        
        2. 分布式锁
        
        一、zookeeper 瞬时有序节点：每个客户端对某个功能加锁时，在zookeeper上的与该功能对应的指定节点的目录下，生成一个唯一的瞬时有序节点
        
        二、memcached add函数：add会添加第一个到达的值，并返回true，后续的添加则都会返回false。（无法持久化）
        
        三、redis分布式锁
        
        redis分布式锁即可以结合zk分布式锁锁高度安全和memcached并发场景下效率很好的优点，可以利用jedis客户端实现。
        
        参考http://blog.csdn.net/java2000_wl/article/details/8740911
        
        http://surlymo.iteye.com/blog/2082684
        
        
        3. nginx的请求转发算法，如何配置根据权重转发
        
        当在一台主机上部署了多个不同的web服务器，并且需要能在80端口同时访问这些web服务器时，可以使用 nginx 的反向代理功能: 用 nginx 在80端口监听所有请求，并依据转发规则(比较常见的是以 URI 来转发)转发到对应的web服务器上。
        
        例如有 webmail , webcom 以及 webdefault 三个服务器分别运行在 portmail , portcom , portdefault 端口，要实现从80端口同时访问这三个web服务器，则可以在80端口运行 nginx， 然后将 /mail 下的请求转发到 webmail 服务器， 将 /com下的请求转发到 webcom 服务器， 将其他所有请求转发到 webdefault 服务器。
        
        http://blog.csdn.net/tobacco5648/article/details/51099426
        
        
        4. 用hashmap实现redis有什么问题
        
        （死锁，死循环，可用ConcurrentHashmap）
        
        
        5. 线程的状态
        
        五个状态之一：新建状态、就绪状态、运行状态、阻塞状态及死亡状态。
        
        
        5. 线程的阻塞的方式
        
        sleep() wait() join()
        
        
        6. sleep和wait的区别
        
        对于sleep()方法，是属于Thread类中的。而wait()方法，则是属于Object类中的。
        
        sleep()方法导致了程序暂停执行指定的时间，让出cpu该其他线程，线程不会释放对象锁。
        
        而当调用wait()方法的时候，线程会放弃对象锁，对象调用notify()方法后本线程才获取对象锁进入运行状态。
        
        
        7. hashmap的底层实现
        
        HashMap是基于哈希表的Map接口的非同步实现（Hashtable跟HashMap很像，唯一的区别是Hashtalbe中的方法是线程安全的，也就是同步的）。
        
        HashMap底层就是一个数组，数组中的每一项又是一个链表。当程序试图将一个key-value对放入HashMap中时，程序首先根据该 key 的 hashCode() 返回值决定该 Entry 的存储位置：如果两个 Entry 的 key 的 hashCode() 返回值相同，那它们的存储位置相同。如果这两个 Entry 的 key 通过 equals 比较返回 true，新添加 Entry 的 value 将覆盖集合中原有 Entry 的 value，但key不会覆盖。如果这两个 Entry 的 key 通过 equals 比较返回 false，新添加的 Entry 将与集合中原有 Entry 形成 Entry 链
        
        http://www.cnblogs.com/ITtangtang/p/3948406.html
        
        
        8. 一万个人抢100个红包，如何实现（不用队列），如何保证2个人不能抢到同一个红包？
        
        可用jedisLock—redis分布式锁实现：基本原理：用一个状态值表示锁，对锁的占用和释放通过状态值来标识。
        
        SETNX key value/expire KEY seconds/del KEY
        
        http://blog.csdn.net/u010359884/article/details/50310387
        
        http://www.cnblogs.com/0201zcr/p/5942748.html
        
        
        9. java内存模型，
        
        垃圾回收机制，不可达算法
        
        
        10. 两个Integer的引用对象传给一个swap方法在方法内部交换引用，返回后，两个引用的值是否会发现变化
        
        不会！Java里方法的参数传递方式只有一种：值传递。
        
        Integer a =1;Integer b=2; swap(Integer a1,Integer b1){c=b1;b1=a1;a1=c}
        
        
        
        11. aop的底层实现，动态代理是如何动态，假如有100个对象，如何动态的为这100个对象代理
        
        AOP的核心机制通过动态代理来实现(jdk动态代理和cglib动态代理)
        
        
        12. 是否用过maven install。 maven test。Git（make install是安装本地jar包）
        
        maven install 生成jar包
        
        maven test 运行 src/test/java下的测试用例
        
        mvn install -Dmaven.test.skip=true跳过测试
        
        http://www.cnblogs.com/phoebus0501/archive/2011/05/10/2042511.html
        
        
        13. tomcat的各种配置，如何配置docBase
        
        appBase这个目录下面的子目录将自动被部署为应用
        
        docBase只是指向了你某个应用的目录
        
        http://blog.csdn.net/liuxuejin/article/details/9104055
        
        
        14. spring的bean配置的几种方式
        
        基于XML的配置、基于注解的配置和基于Java类的配置。
        
        http://www.cnblogs.com/zhangwenjing/p/3546006.html
        
        
        15. web.xml的配置
        
        最终加载顺序：ServletContext -> listener -> filter -> servlet
        
        http://www.cnblogs.com/xxoome/p/5954633.html
        
        
        16. spring的监听器。
        
        http://blog.csdn.net/xrt95050/article/details/6132179
        
        
        17. zookeeper的实现机制，有缓存，如何存储注册服务的
        
        ZooKeeper是Hadoop Ecosystem中非常重要的组件，它的主要功能是为分布式系统提供一致性协调(Coordination)服务
        
        http://blog.csdn.net/xinguan1267/article/details/38422149
        
        
        18. IO会阻塞吗？readLine是不是阻塞的
        
        readLine()是一个阻塞函数，当没有数据读取时，就一直会阻塞在那，而不是返回null
        
        readLine()只有在数据流发生异常或者另一端被close()掉时，才会返回null值。
        
        http://blog.csdn.net/swingline/article/details/5357581
        
        
        19. 用过spring的线程池还是java的线程池？
        
        SpringFrameWork 的 ThreadPoolTaskExecutor 是辅助 JDK 的 ThreadPoolExecutor 的工具类，它将属性通过 JavaBeans 的命名规则提供出来，方便进行配置。
        
        http://www.cnblogs.com/chkk/p/5386356.html
        
        
        20. 字符串的格式化方法 （20，21这两个问题问的太低级了）
        
        String类的format()方法
        
        21. 时间的格式化方法
        
        SimpleDateFormat的format()方法
        22. 定时器用什么做的
        
        http://lengchaotian.iteye.com/blog/1887439
        
        
        23. 线程如何退出结束
        
        1. 使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。 while (!exit);
        2. 使用stop方法强行终止线程。
        3. 使用interrupt方法中断线程。
        
        
        24. java有哪些锁？
        
        乐观锁 悲观锁 synchronized 可重入锁 读写锁,
        
        用过reentrantlock吗？reentrantlock与synmchronized的区别
        
        1. 等待可中断 tryLock(long timeout, TimeUnit unit)。
        
        2.公平锁与非公平锁(synchronized的是非公平锁）
        
        3.绑定多个Condition
        
        http://www.cnblogs.com/fanguangdexiaoyuer/p/5313653.html
        
        
        25. ThreadLocal的使用场景
        
        ThreadLocal就是用于线程间的数据隔离的。最适合的是按线程多实例（每个线程对应一个实例）的对象的访问
        
        
        26. java的内存模型，垃圾回收机制
        
        
        27. 为什么线程执行要调用start而不是直接run
        
        （直接run，跟普通方法没什么区别，先调start，run才会作为一个线程方法运行）
        
        
        28. qmq消息的实现机制(qmq是去哪儿网自己封装的消息队列)
        29. 遍历hashmap的三种方式
        
        方式1：通过遍历keySet()遍历HashMap的value
        用时:61
        方式2：通过遍历values()遍历HashMap的value
        用时:7
        方式3：通过entrySet().iterator()遍历HashMap的key和映射的value
        用时:12
        
        http://blog.csdn.net/fly_zxy/article/details/43015193
        
         
        
        30. jvm的一些命令
        
        jps jstat jmap jhat jstack jinfo
        
         
        
        31. memcache和redis的区别
        
        1  Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。
        2  Redis支持数据的备份，即master-slave模式的数据备份。
        3  Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用
        
        http://blog.csdn.net/tonysz126/article/details/8280696/
        
        32. MySQL的行级锁加在哪个位置
        
        表级，直接锁定整张表，在你锁定期间，其它进程无法对该表进行写操作。如果你是写锁，则其它进程则读也不允许
        行级,，仅对指定的记录进行加锁，这样其它进程还是可以对同一个表中的其它记录进行操作。
        页级，表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。
        
        http://www.jb51.net/article/50047.htm
        
        
        33. ConcurrentHashmap的锁是如何加的？是不是分段越多越好
        
        http://www.cnblogs.com/my376908915/p/6759667.html
        
        
        34. myisam和innodb的区别
        
        （innodb是行级锁，myisam是表级锁）
        
        
        35. mysql其他的性能优化方式
        
        http://www.cnblogs.com/eric-gao/articles/5549801.html
        
         
        
        36. linux系统日志在哪里看
        
        /var/log/*     ，用Ls / cat查看
        
        http://mushme.iteye.com/blog/1001478
        
         
        
        37. 如何查看网络进程
        
        38. 统计一个整数的二进制表示中bit为1的个数
        
        while (n >0)
            {
                if((n &1) ==1) // 当前位是1
                    ++c ; // 计数器加1
                n >>=1 ; // 移位
            }
        http://www.cnblogs.com/graphics/archive/2010/06/21/1752421.html
        
         
        
        39. jvm内存模型，java内存模型
        
        http://www.cnblogs.com/my376908915/p/6753498.html
        
         
        
        40. 如何把java内存的数据全部dump出来
        jmap来获取内存镜像；MAT/ visualvm来进行内存镜像分析
        http://f.dataguru.cn/thread-714170-1-1.html
        
        41. 如何手动触发全量回收垃圾，如何立即触发垃圾回收
        
        手动调用gc函数
        
         
        
        42. hashmap如果只有一个写其他全读会出什么问题
        
        如果value为空(表示这个key还没有插入)，那么很可能同时几个线程get的value都是null
        
         
        
        43. git rebase git merge 区别
        
        在当前的分支下rebase一下master分支，这样我这个分支的几个commits相对于master还是处于最顶端的，也就是说rebase主要用来跟上游同步，同时把自己的修改顶到最上面。
        
        用rebase有时候会需要多次fix冲突；用merge确实只需要解决一遍冲突，比较简单粗暴。
        
         
        
        44. mongodb和hbase的区别
        
        Redis定位在"快"，HBase定位于"大",mongodb定位在"灵活"。
        
        mongodb可以当作简单场景下的但是性能高数倍的MySQL,定位是取代关系型数据库，想当一个主流数据库。因为他有非结构化、方便扩充字段、写性能优于mysql。万事万物有利有弊，mongodb的内存型缓存内容，让其速度飞快，带来内存率多，掉电数据问题等
        
        Redis基本只会用来做缓存，是一个小而美的数据库，主要用在key-value 的内存缓存，读写性能极佳，list，set，hash等几种简单结构使得使用也很简单。缓存与简单是其定位，分布式redis架构的出现，让redis更加广泛的使用，稳坐缓存第一把交椅。
        
        HBase用来做离线计算，定位非结构化大数据，可伸缩性好，并不是完全高可用，底层依靠hadoop提供的HDFS，使用时有一整套zookeeper，pig，hive的生态系统。
        
         
        
        45. 如何解决并发问题
        
        代码中的处理就是线程池，多线程，生产者消费者的应用了。
        
        一、 web外网加速相关技术
        1.	镜像站点:譬如一个美国网站的中国镜像可以使来自中国的用户直接从这个中国的镜像访问，从而加快了速度。这可以看作是一种全球范围的缓存。
        2.	DNS负载均衡:在DNS服务器中为同一个主机名配置多个IP地址，DNS服务器对每个查询将以DNS文件中主机记录的IP地址按顺序返回不同的解析结果，将客户端的访问引导到不同的机器上去，从而达到负载均衡的目的
        3.	CDN内容分发:尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和环节
        二、	内网加速技术
        
        0. HTML静态化 ：HTTP请求---Web服务器---Servlet--HTML--响应请求
        1．	负载均衡（软件负载均衡：LVS、Nginx，第七层（应用层）的应用。硬件负载均衡：F5，第四层(传输层)上的应用）
        2．	Web缓存服务器（数据库缓存：memcached/redis来做缓存集群）
        3．	Web/应用服务器分布式文件系统（图片服务器分离 )
        4．	分布式数据库。数据库主从分布： M-M-Slaves方式，2个主Mysql（写），多个Slaves（读-负载均衡，结合LVS）。数据库分割：从业务层面上进行分区，比如id，切分到不同的数据库集群去。）
        
        http://www.ablanxue.com/prone_1020_1.html
        
        
        46. volatile的用途
        
        一个最常见的volatile的应用场景是boolean的共享状态标志位，或者单例模式的双重检查锁
        
        http://www.cnblogs.com/my376908915/p/6757533.html
        
        
        47. java线程池
        
        http://www.cnblogs.com/my376908915/p/6761364.html
        
        
        48. mysql的binlog
        
        binlog日志用于记录所有更新了数据或者已经潜在更新了数据（例如，没有匹配任何行的一个DELETE）的所有语句。语句以“事件”的形式保存，它描述数据更改。
        
        作用:因为有了数据更新的binlog，所以可以用于实时备份，与master/slave复制。
        
        http://blog.csdn.net/wyzxg/article/details/7412777
        
        
        49. 代理模式
        50. mysql是如何实现事务的
        MySQL的事务支持不是绑定在MySQL服务器本身，而是与存储引擎相关
        
        1.MyISAM：不支持事务，用于只读程序提高性能
        
        2.InnoDB：支持ACID事务、行级锁、并发
        
        3.Berkeley DB：支持事务
        
        在MySQL中，事务开始使用COMMIT或ROLLBACK语句开始工作和结束。
        
        http://www.cnblogs.com/ymy124/p/3718439.html
        
         
        
        51. 读写分离何时强制要读主库，读哪个从库是通过什么方式决定的，从库的同步mysql用的什么方式
        
        开启了读写分离，在写数据的时候写入了主库，写完都需要刷新redis缓存，强制要读主库。在写操作的同步延迟窗口之内读，则读取主库，其他情况下读从库。
        
        读写一致：http://www.it610.com/article/4872058.htm
        
         
        
        主从架构本来就是一种高可用性解决方案，主从架构下的强一致性（银行业）：只需要在主机写入时，确认更新已经同步到备机之后，再返回写操作成功即可。主流数据库均支持这种完全的同步模式。（MySQL的Semi-sync功能）
        
        目前互联网企业对于“高并发的写操作”问题比较典型的解决方案是分表分库+写缓存，增加针对写操作的缓存层，把写操作放到队列里，排队到数据库结点上异步执行。（在数据库层之上架构一个redis这样的分布式缓存）
        
         
        
        
        52. mysql的存储引擎
        
        MySQL5.5以后默认使用InnoDB存储引擎，其中InnoDB和BDB提供事务安全表，其它存储引擎都是非事务安全表。若要修改默认引擎，可以修改配置文件中的default-storage-engine。
        
        http://www.cnblogs.com/gbyukg/archive/2011/11/09/2242271.html
        
        
        53. mysql的默认隔离级别，其他隔离级别
        
        Read Uncommitted（读取未提交内容）,脏读
        
        Read Committed（读取提交内容）：大多数数据库系统的默认隔离级别，不可重复读
        
        Repeatable Read（可重读）：MySQL的默认事务隔离级别，幻读
        
        Serializable（可串行化）：最高的隔离级别
        
        http://www.jb51.net/article/96179.htm
        
        
        54. 将一个链表反转（用三个指针，但是每次只发转一个）
        
        思路：从原链表的头部一个一个取节点并插入到新链表的头部
        
        思路：每次都将原第一个结点之后的那个结点放在新的表头后面。
        
        http://blog.csdn.net/hyqsong/article/details/49429859
        
        
        55. spring Aop的实现原理，具体说说
        
        IOC（反转控制）：对成员变量的赋值的控制权从代码中反转到配置文件中。
        AOP：Aspect（切面） Oriented（面向） Programming（编程），面向切面编程。
        
        动态代理和发射技术，已经基本实现了AOP的功能: http://www.jb51.net/article/81788.htm
        
        
        56. 何时会内存泄漏，内存泄漏会抛哪些异常
        
        首先，这些对象是可达的，即在有向图中，存在通路可以与其相连；
        其次，这些对象是无用的，即不被程序使用，然而它却占用内存。
        
        一个生存周期远大于另一个生存周期，而且生存周期大的对象有指向生存周期小的对象的引用，
        
        而且生存周期小的对象不再有指向其他对象的引用，那好，既然大的有指向小的引用，那垃圾回收器对小的也无可奈何。
        
        memory leak：最终会导致out of memory！
        
        
        57. 是否用过Autowire注解
        
        Spring 2.5 引入了 @Autowired 注释，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。 通过 @Autowired的使用来消除 set ，get方法。
        
        Spring 通过一个 BeanPostProcessor 对 @Autowired 进行解析，所以要让 @Autowired 起作用必须事先在 Spring 容器中声明 AutowiredAnnotationBeanPostProcessor Bean。
        
        Spring 将直接采用 Java 反射机制对 Boss 中的 car 和 office 这两个私有成员变量进行自动注入。
        
        
        58. spring的注入bean的方式
        
        Spring中依赖注入有三种注入方式：
        
        一、构造器注入；
        
        二、设值注入（setter方式注入）；
        
        三、Feild方式注入（注解方式注入）。
        
        http://glzaction.iteye.com/blog/1299441
        
        
        59. sql语句各种条件的执行顺序，如select， where， order by， group by
        
        from--where--group by--having--select--order by---limit
        
        http://www.cnblogs.com/huminxxl/p/3149097.html
        
        
        60. select  xx from xx where xx and xx order by xx limit xx； 如何优化这个（看explain）
        加limit，和不加走的索引不一样。 select  xx from （select  xx from xx where xx and xx order by xx ） yy limit xx；
        在order by  limit 一起时 执行顺序不是按照：where ------order by ------ limit
        
        而是：order by ----- limit -------where的顺序去执行，这样就会有一个问题，按照我们管用的思路，上面的查询肯定是会丢失数据的。
        
        http://blog.csdn.net/wulantian/article/details/42679167
        
         
        
        MySQL 对于千万级的大表要怎么优化？
        
        第一优化你的sql和索引；MySQL性能优化的最佳20+条经验：http://coolshell.cn/articles/1846.html
        
        第二加缓存，memcached,redis；
        
        第三以上都做了后，还是慢，就做主从复制或主主复制，读写分离
        
        第四如果以上都做了还是慢，不要想着去做切分，mysql自带分区表：http://www.cnblogs.com/zemliu/archive/2013/07/21/3203511.html
        
        第五如果以上都做了，那就先做垂直拆分，其实就是根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；列与列之间关联性不大的时候，垂直切分。
        
        第六才是水平切分，针对数据量大的表，行很多的时候水平切分表，表名取模：http://www.cnblogs.com/sns007/p/5790838.html
        
         
        
        61. 四则运算写代码
        
        http://www.jb51.net/article/71487.htm
        
         
        
        62. 统计100G的ip文件中出现ip次数最多的100个ip
        
        1，分割IP：读原始文件，去掉IP中的点转化为一个long型变量，取模为0,1,2...99的 IP都分到一个（写）文件了。（内存不够，分而治之http://blog.csdn.net/yan5105105/article/details/50783262）
        
        2，哈希表map<(ip, count>，将每个IP作为关键字映射为出现次数，这个哈希表建好之后也得先写入硬盘
        
        3，建小顶堆，每次有数据输入的时候可以先与根节点比较。若小于或等于根节点，则舍弃；否则用新数值替换根节点数值。并进行最小堆的调整。http://blog.csdn.net/ephuizi/article/details/11790957
        
        基于堆实现的优先级队列：PriorityQueue 解决 Top K 问题：https://my.oschina.net/leejun2005/blog/135085
        
        
        63. zookeeper的事务，结点，服务提供方挂了如何告知消费方
        64. 5台服务器如何选出leader
        
        分布式-选举算法（bully算法）：
        
        当任何一个进程发现协调者不响应请求时，他发起一次选举，选举过程如下：
        
        a， P进程向所有编号比他大的进程发送一个election消息；
        
        b， 如果无人响应，则P获胜，成为协调者
        
        c，如果编号比他大的进程响应，则由响应者接管选举工作，P的工作完成。
        
        http://blog.csdn.net/huangwei19892008/article/details/9004970
        
         
        
        65. 适配器和代理模式的区别
        
        代理模式（Proxy）：为其他对象提供一种代理以控制对这个对象的访问。用同一接口的子类的方法去 实现接口的方法
        
        适配器模式（Adapter）：将一个类的接口转换成客户希望的另外一个接口，使得原本接口不兼容而不能一起工作的那些类可以一起工作。用不同接口的子类的方法去 实现接口的方法
        
        
        66. 读写锁
        
        对于读多写少的场景，一个读操作无须阻塞其它读操作，只需要保证读和写  或者 写与写  不同时发生即可。
        
        读写锁的锁定规则如下：
        获得读锁后，其它线程可获得读锁而不能获取写锁
        获得写锁后，其它线程既不能获得读锁也不能获得写锁
        
        http://www.cnblogs.com/my376908915/p/6758681.html
        
        
        67. static加锁
        
        synchronized是对类的当前实例进行加锁，防止其他线程同时访问该类的该实例的所有synchronized块，注意这里是“ 类的当前实例 ”，类的两个不同实例就没有这种约束了。
        
        那么static synchronized恰好就是要控制类的所有实例的访问了，static synchronized是限制线程同时访问jvm中该类的所有实例同时访问对应的代码块。
        
        http://blog.csdn.net/zbuger/article/details/50827762
        
        
        68. 事务隔离级别
        
        MySQL数据库为我们提供的四种隔离级别：
        
        　　① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。
        
        　　② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。
        
        　　③ Read committed (读已提交)：可避免脏读的发生。
        
        　　④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。
        
        http://www.cnblogs.com/fjdingsd/p/5273008.html
        
        
        69. 门面模式，类图(外观模式)
        
        为子系统中的一组接口提供一个一致的界面，此模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。
        
        http://www.cnblogs.com/wangjq/archive/2012/07/10/2583672.html
        
        
        70. mybatis如何映射表结构
        在SqlMapConfig.xml中 
        <typeAliases>
        <typeAlias alias="game" type="实体类路径"/>
        </typeAliases>
        这就是把你的实体类写了个别名
        
        最后：在写查询添加的时候：
        <select id="gameDao" resultType="game">返回实体类对象
        select * from tb_game
        </select>
        这样查出的结果就对应上数据了。
        
        71. 二叉树遍历
        
        前序、中序以及后序三种遍历方法。
        
        递归实现：
        
        void preOrder1(BinTree *root)     //递归前序遍历，按照“根结点-左孩子-右孩子”的顺序进行访问。
        {
            if(root!=NULL)
            {
                cout<<root->data<<" ";
                preOrder1(root->lchild);
                preOrder1(root->rchild);
            }
        }
        
        非递归实现：
        1)访问结点P，并将结点P入栈;
        2)判断结点P的左孩子是否为空，若为空，则取栈顶结点并进行出栈操作，并将栈顶结点的右孩子置为当前的结点P，循环至1);若不为空，则将P的左孩子置为当前的结点P;
        3)直到P为NULL并且栈为空，则遍历结束。
        void preOrder2(BinTree *root)     //非递归前序遍历 
        {
            stack<BinTree*> s;
            BinTree *p=root;
            while(p!=NULL||!s.empty())
            {
                while(p!=NULL)
                {
                    cout<<p->data<<" ";
                    s.push(p);
                    p=p->lchild;
                }
                if(!s.empty())
                {
                    p=s.top();
                    s.pop();
                    p=p->rchild;
                }
            }
        }
        72. 主从复制
        
        整体上来说，复制有3个步骤：   
        
               (1)    master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）；
               (2)    slave将master的binary log events拷贝到它的中继日志(relay log)；
        
               (3)    slave重做中继日志中的事件，将改变反映它自己的数据。
        
        在每个事务更新数据完成之前，master在二日志记录这些改变。MySQL将事务串行的写入二进制日志，即使事务中的语句都是交叉执行的。在事件写入二进制日志完成后，master通知存储引擎提交事务。
        下一步就是slave将master的binary log拷贝到它自己的中继日志。首先，slave开始一个工作线程——I/O线程。I/O线程在master上打开一个普通的连接，然后开始binlog dump process。Binlog dump process从master的二进制日志中读取事件，如果已经跟上master，它会睡眠并等待master产生新的事件。I/O线程将这些事件写入中继日志。
        SQL slave thread（SQL从线程）处理该过程的最后一步。SQL线程从中继日志读取事件，并重放其中的事件而更新slave的数据，使其与master中的数据一致。只要该线程与I/O线程保持一致，中继日志通常会位于OS的缓存中，所以中继日志的开销很小。
        
        http://blog.csdn.net/hguisu/article/details/7325124
        
        
        73. mysql引擎区别
        
        MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持。
        
        MyISAM类型的表强调的是性能，其执行数度比InnoDB类型更快，但是不提供事务支持，而InnoDB提供事务支持已经外部键等高级数据库功能。
        
        一般来说，MyISAM适合：
        (1)做很多count 的计算；
        (2)插入不频繁，查询非常频繁；
        (3)没有事务。
        
        InnoDB适合：
        (1)可靠性要求比较高，或者要求事务；
        (2)表更新和查询都相当的频繁，并且表锁定的机会比较大的情况指定数据引擎的创建
        
        http://www.jb51.net/article/38004.htm
        
        
        74. 静态内部类加载到了哪个区？
        
        方法区
        
        静态内部类，又叫类级内部类。
        
        延迟加载单例模式：类装载的时候不去初始化对象，延迟初始化到getInstance方法时初始化。
        
        http://www.javaweb1024.com/java/Javajichu/2015/03/25/454.html
        
         
        
        75. class文件编译后加载到了哪
        
        方法区
        
        java编译期会加载.class文件：加载的类是你需要编译类所依赖的类，如你使用了System这个类，因为在jdk里的lib已经存在了，所以你不用显示的如加载，已经在classpath下面了。
        
        如果你自己写的一个Class1，把它编译后，再写了个Class2。再编译Class2的时候就需要把Class1的编译文件加载到classpath中。
        
         
        
        76. web的http请求如果整体响应时间变长导致处理的请求数变少，该如何处理？
        
        瓶颈在哪里？
        
        用队列，当处理不了那么多http请求时将请求放到队列中慢慢处理
        
         
        
        77. 线程安全的单例模式
        
        静态内部类实现单例模式：
        
        public class MySingleton {  
            //内部类  
            private static class MySingletonHandler{  
                private static MySingleton instance = new MySingleton();  //静态内部类在类加载是被实例化
            }   
              
            private MySingleton(){}  
            public static MySingleton getInstance() {   
                return MySingletonHandler.instance;  
            }  
        } 
        枚举数据类型实现单例模式：EnumFactory.singletonFactory.getInstance()
        
        public enum EnumFactory{    
            singletonFactory;  
            private MySingleton instance;  
            private EnumFactory(){//枚举类的构造方法在类加载是被实例化  （在使用枚举时，构造方法会被自动调用）
                instance = new MySingleton();  
            }  
               
            public MySingleton getInstance(){  
                return instance;  
            }  
        } 
        http://blog.csdn.net/cselmu9/article/details/51366946
        
        78. 快速排序性能考虑
        
        该方法的基本思想（分治法）是：
        
        1．先从数列中取出一个数作为基准数。
        
        2．分区过程，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边。
        
        3．再对左右区间重复第二步，直到各区间只有一个数。
        
        http://blog.csdn.net/morewindows/article/details/6684558
        
        http://www.cnblogs.com/luchen927/archive/2012/02/29/2368070.html
        
         
        
        79. volatile关键字用法
        
        volatile的应用场景是boolean的共享状态标志位，或者单例模式的双重检查锁
        
        http://www.cnblogs.com/my376908915/p/6757533.html
        
         
        
        80. 求表的size，或做数据统计可用什么存储引擎
        
        MyISAM
        
        一般来说，MyISAM适合：
        (1)做很多count 的计算；
        (2)插入不频繁，查询非常频繁；
        (3)没有事务。
        
        InnoDB适合：
        (1)可靠性要求比较高，或者要求事务；
        (2)表更新和查询都相当的频繁，并且表锁定的机会比较大的情况指定数据引擎的创建
        
        http://www.jb51.net/article/38004.htm
        
         
        
        81. 读多写少可用什么引擎
        
        MyISAM
        
         
        
        82. 假如要统计多个表应该用什么引擎
        
        MyISAM
        
         
        
        83. concurrenhashmap求size是如何加锁的，如果刚求完一段后这段发生了变化该如何处理
        
        Put等操作都是在单个Segment中进行的，但是ConcurrentHashMap有一些操作是在多个Segment中进行，比如size操作，ConcurrentHashMap的size操作也采用了一种比较巧的方式，来尽量避免对所有的Segment都加锁。 　　
        
        前面我们提到了一个Segment中的有一个modCount变量，代表的是对Segment中元素的数量造成影响的操作的次数，这个值只增不减，
        
        size操作就是遍历了两次Segment，每次记录Segment的modCount值，然后将两次的modCount进行比较，如果相同，则表示期间没有发生过写入操作，就将原先遍历的结果返回，
        
        如果不相同，则把这个过程再重复做一次，
        
        如果再不相同，则就需要将所有的Segment都锁住，然后一个一个遍历了。
        
         
        
        ConcurrentHashMap加锁的时候根据散列值锁住了散列值锁对应的那段，因此提高了并发性能。
        
        ConcurrentHashMap也增加了对常用复合操作的支持，比如"若没有则添加"：putIfAbsent()，替换：replace()。这2个操作都是原子操作。
        
        http://www.cnblogs.com/my376908915/p/6759667.html
        
         
        
        84. 1000个苹果，请你将它放进10个箱子，如何放，使得顾客不管要多少个苹果，你总可以从10箱子里拿出若干个箱子，其苹果之和就是顾客要的苹果数？
        
        箱子中依次放1，2，4，8，16，32，64，128，256，489个苹果
        
        算法：其实就是数的二进制表示而已，你将所有的数转换成2进制后，就会发现，所有的数都是由不同位上的1组成
        二进制表：1，10，100，1000，10000，100000。。。。。。。
        
         
        
        85. 可重入的读写锁，可重入是如何实现的？
        
        可重入锁又叫做递归锁。
        
        reentrant 锁意味着什么呢？简单来说，它有一个与锁相关的获取计数器，如果拥有锁的某个线程再次得到锁，那么获取计数器就加1，然后锁需要被释放两次才能获得真正释放。
        
        这相当于是模仿了synchronized中又可以嵌套一个synchronized这样的场景
        
        http://blog.csdn.net/johnking123/article/details/50043961
        
         
        
        86. 是否用过NIO
        
        Buffer和Channel是标准NIO中的核心对象（网络NIO中还有个Selector核心对象），几乎每一个IO操作中都会用到它们。
        
        http://www.cnblogs.com/my376908915/p/6767922.html
        
         
        
        87. java的concurrent包用过没
        
        java.util.concurrent包，
        
        AtomicI原子化，基于原子操作的循环CAS算法。
        
        Collections容器，ConcurrentLinkedQueue（非阻塞队列---基于原子引用的循环CAS），ConcurrentHashMap
        
        Locks锁，基于非阻塞队列的循环CAS + JNI的unsafe.park(false, 0L)阻塞线程
        
        Executor线程池。
        
        http://www.cnblogs.com/my376908915/p/6758278.html
        
         
        
        88. sting s=new string("abc")分别在堆栈上新建了哪些对象
        
        栈：sting s
        堆：new string("abc")
        字符串池（方法区）："abc"
        
        JVM中存在着一个字符串池，使用引号 创建文本的方式的String对象都会放入字符串池。可以提高效率。
        String a="abc"; String b="abc";//这两句在字符串池 只创建一个实例对象。 
        String a="ab"+"cd";//这一句在字符串池 创建三个实例对象。
        new方式新建String对象则不会放入字符串池，放入堆。
        参考： http://blog.csdn.net/lubiaopan/article/details/4776000/
        89. java虚拟机的区域分配，各区分别存什么
        
        http://www.cnblogs.com/my376908915/p/6753498.html
        
         
        
        90. 分布式事务（JTA）
        普通的jdbc事务只能针对单个connection，要实现多个数据库事务的操作，jta可以满足要求。
        
        jta应用程序要调用 javax.transaction.UserTransaction 接口中的方法。
        
        “用 JTA 界定事务，那么就需要有一个实现 javax.sql.XADataSource 、 javax.sql.XAConnection 和 javax.sql.XAResource 接口的 JDBC 驱动程序。
        一个实现了这些接口的驱动程序将可以参与 JTA 事务。一个 XADataSource 对象就是一个 XAConnection 对象的工厂。 XAConnection s 是参与 JTA 事务的 JDBC 连接。” 
        要使用JTA事务，必须使用XADataSource来产生数据库连接，产生的连接为一个XA连接。 
        XA连接（javax.sql.XAConnection）和非XA（java.sql.Connection）连接的区别在于：XA可以参与JTA的事务，而且不支持自动提交。
        Innodb存储引擎支持XA事务，通过XA事务可以支持分布式事务的实现。
        http://blog.csdn.net/mchdba/article/details/13076803
         
        
        91. threadlocal使用时注意的问题
        
        （ThreadLocal和Synchonized都用于解决多线程并发访问。但是ThreadLocal与synchronized有本质的区别。
        
        synchronized是利用锁的机制，使变量或代码块在某一时该只能被一个线程访问。
        
        而ThreadLocal为每一个线程都提供了变量的副本，使得每个线程在某一时间访问到的并不是同一个对象，这样就隔离了多个线程对数据的数据共享。
        
        而Synchronized却正好相反，它用于在多个线程间通信时能够获得数据共享）
        
        http://www.cnblogs.com/my376908915/p/6763210.html
        
         
        
        92. java有哪些容器
        
        (各种集合，tomcat也是一种容器)
        
         
        
        93. 二分查找算法
        
        用二分查找在已排序的数组中查看该数组是否含有一个特定的值。速度是非常快速的。
        
        迭代方式：
        
        public int BinarySearchIteration(int[] array, int key)  
        {  
            int begin = 0;  
            int end = array.Length - 1;  
            while (begin <= end)  
            {  
                int mid = begin + (end - begin) / 2;  
                if (array[mid] > key)  
                {  
                    end = mid - 1;  
                }  
                else if (array[mid] < key)  
                {  
                    begin = mid + 1;  
                }  
                else  
                {  
                    return mid;  
                }  
            }  
            return -1;  
        } 
        http://blog.csdn.net/beiyeqingteng/article/details/5736004
        
        
        94. myisam的优点，和innodb的区别
        
        MyISAM
        
        一般来说，MyISAM适合：
        (1)做很多count 的计算；
        (2)插入不频繁，查询非常频繁；
        (3)没有事务。
        
        InnoDB适合：
        (1)可靠性要求比较高，或者要求事务；
        (2)表更新和查询都相当的频繁，并且表锁定的机会比较大的情况指定数据引擎的创建
        
        http://www.jb51.net/article/38004.htm
        
        
        95. redis能存哪些类型
        
        redis常用五种数据类型:string,hash,list,set,zset(sorted set).
        
        http://blog.csdn.net/qq_19943157/article/details/50495925
        
        
        96. http协议格式，get和post的区别
        
        http协议格式: http://www.cnblogs.com/li0803/archive/2008/11/03/1324746.html
        
        get和post的区别:http://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html
        
        
        97. 可重入锁中对应的wait和notify
        
        条件锁，http://www.cnblogs.com/my376908915/p/6758681.html
        
        Conditon中的await()对应Object的wait()；
        Condition中的signal()对应Object的notify()；
        Condition中的signalAll()对应Object的notifyAll()。
        
        
        98. redis能把内存空间交换进磁盘中吗
        
        Redis利用swap文件将数据从内存转移到磁盘。
        
        http://blog.csdn.net/nvnh7553/article/details/50107971
        
        如果你打开虚拟内存功能，当内存用尽时, Redis就会把那些不经常使用的数据存储到磁盘。
        如果Redis里的虚拟内存被禁了，他就会用上操作系统的虚拟内存(交换内存)，同时性能急剧下降。
        你可以配置maxmemory参数，来避免Redis默认再分配更多的内存。
        
        http://www.dewen.net.cn/q/242
        
        
        99. java线程池中基于缓存和基于定长的两种线程池，当请求太多时分别是如何处理的？
        
        Java自带的几种线程池：
        
        1、newCachedThreadPool 创建一个可缓存的线程池。
        
        这种类型的线程池特点是：
        
        a).工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger. MAX_VALUE), 这样可灵活的往线程池中添加线程。
        
        b).如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。
        
        2、newFixedThreadPool 创建一个指定工作线程数量的线程池。
        
        每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。
        
        3、newSingleThreadExecutor 创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，如果这个线程异常结束，会有另一个取代它，保证顺序执行(我觉得这点是它的特色)。
        
        单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的 。
        
        4、newScheduleThreadPool 创建一个定长的线程池，而且支持定时的以及周期性的任务执行，类似于Timer。
        
        
        100. synchronized加在方法上用的什么锁
        方法对应的实例对象。
        
         
        
        101. 可重入锁中的lock和trylock的区别
        
        ReentrantLock获取锁定与三种方式：
            a)  lock(), 如果获取了锁立即返回，如果别的线程持有锁，当前线程则一直处于休眠状态，直到获取锁
        
            b) tryLock(), 如果获取了锁立即返回true，如果别的线程正持有锁，立即返回false；
        
            c) tryLock(long timeout,TimeUnit unit)，   如果获取了锁定立即返回true，如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false；
        
            d) lockInterruptibly:如果获取了锁定立即返回，如果没有获取锁定，当前线程处于休眠状态，直到获得锁定，或者当前线程被别的线程中断
        
        http://www.cnblogs.com/my376908915/p/6758681.html
        
        
        102. innodb对一行数据的读会加锁吗？
        
        对于insert、update、delete，InnoDB会自动给涉及的数据加排他锁（X）；
        
        对于一般的Select语句，InnoDB不会加任何锁，事务可以通过以下语句给显示加共享锁或排他锁。
        
        共享锁：select * from table_name where .....lock in share mode
        
        排他锁：select * from table_name where .....for update
        
        http://www.2cto.com/database/201508/429967.html
        
        
        103. redis做缓存是分布式存的？不同的服务器上存的数据是否重复？guava cache呢？是否重复？不同的机器存的数据不同
        
        由于redis是单点，项目中需要使用，必须自己实现分布式。
        
        分布式实现：通过key做一致性哈希，实现key对应redis结点的分布。
        
        http://www.open-open.com/lib/view/open1384603154712.html
        
        Mysql是适合海量数据存储的，然后通过Memcached将一些常用的数据进行缓存，加快访问速度。
        
        当数据量不断的增大的时候，进行切表，拆表的，Memcached也需要不断的跟着扩容，Memcached和Mysql的数据一致性的问题，Memcached数据命中率低或者Down机，大量的访问就会穿透到数据库，这时候Mysql可能会无法支撑。
        
                  Redis使用最佳方式是全部数据in-memory。
        
                  Redis更多场景是作为Memcached的替代者来使用。
        
                 当需要除key/value之外的更多数据类型支持时，使用Redis更合适。
        
                 当存储的数据不能被剔除时，使用Redis更合适。
        
        http://blog.csdn.net/bemavery/article/details/47061663
        
        
        104. 用awk统计一个ip文件中top10
        
        cat File.log | awk -F ',' '{print $8}' | sort | uniq -c | sort -k1nr | head -10
        
         
        
        Linux awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。http://www.cnblogs.com/ggjucheng/archive/2013/01/13/2858470.html
        
        简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。http://man.linuxde.net/awk
        
        https://yq.aliyun.com/ziliao/73055
        
        
        105. 对表做统计时可直接看schema info信息，即查看表的系统信息
        
        MySQL中有一个名为 information_schema 的数据库，在该库中有一个 TABLES 表，这个表主要字段分别是：
        
        TABLE_SCHEMA : 数据库名
        
        TABLE_NAME：表名
        
        ENGINE：所使用的存储引擎
        
        TABLES_ROWS：记录数
        
        DATA_LENGTH：数据大小
        
        INDEX_LENGTH：索引大小
        
         
        
        use information_schema;
        select table_name,table_rows from tables
        where TABLE_SCHEMA = '数据库名'
        order by table_rows desc;
        查询出来的是每张表的行数
        
        http://help.wopus.org/mysql-manage/607.html
        
        
        106. mysql目前用的版本
        
        据说5.0.x的版本比较稳定，兼容比较好，我现在安装的5.1.63
        
        Mysql查看版本号的五种方式介绍：http://www.jb51.net/article/36370.htm
        
        
        107. 公司经验丰富的人给了什么帮助？(一般boss面会问这些)
        
        具体技术的点拨，思考问题的方式，解决问题的方式。
        
        
        108. 自己相对于一样的应届生有什么优势
        
        经验丰富
        
        
        109. 自己的好的总结习惯，给自己今后的工作带了什么帮助，举例为证
        
        技术总结分享，文档归档。
        
         
        
        110. 原子类，线程安全的对象，异常的处理方式
        JUC出现之后，这些原子操作 基于JNI提供了新的实现，
        
        比如AtomicInteger,AtomicLong,AtomicBoolean,AtomicReference,AtomicIntegerArray/AtomicLongArray/AtomicReferenceArray；
        
        这些操作中提供一些原子化操作，比如incrementAndGet（相当于i++），compareAndSet（安全赋值）等，直接读源代码也很容易懂。
        
        www.cnblogs.com/my376908915/p/6758415.html
        
         
        
        111. 4亿个int数，如何找出重复的数
        
        大数据处理算法一：Bitmap算法
        
        核心思想即通过将一个数作为下标（index）来索引一个bit表示一个数是否存在，排序时的时间复杂度为O(N)，需要的额外空间的复杂度O(N/8)，支持整个int范围（正负数都支持）的算法
        
        http://www.open-open.com/lib/view/open1430902831226.html
        
        
        112. 4亿个url，找出其中重复的（考虑内存不够，通过hash算法，将url分配到1000个文件中，不同的文件间肯定就不会重复了，再分别找出重复的）
        
        hash算法：   H（url）出一个整数后，取1000的余数，分割到1000个文件中，余数即为文件名。
        
        读入每个小文件，进内存HashMap（url，Counts）。
        
        遍历EntrySets，即Counts 〉1的url
        
        hash算法原理详解 ：http://blog.csdn.net/tanggao1314/article/details/51457585
        
        
        有1万个数组，每个数组有1000个整数，每个数组都是降序的，从中找出最大的10个数。
        
        每个数组取出前10个，堆排序，或优先队列。
        
         
        
        113. LinkedHashmap的底层实现
        
        LinkedHashMap实现与HashMap的不同之处在于，LinkedHashMap维护着一个运行于所有条目的双向链接列表。此链接列表定义了迭代顺序，该迭代顺序可以是插入顺序或者是访问顺序。
        
        对于LinkedHashMap而言，它继承与HashMap、底层使用哈希表与双向链表来保存所有元素。其基本操作与父类HashMap相似，它通过重写父类相关的方法，来实现自己的链接列表特性。
        
        Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而在哈希表的基础上又构成了双向链接列表。
        
        http://zhangshixi.iteye.com/blog/673789
        
        
        114. 类序列化时类的版本号的用途，如果没有指定一个版本号，系统是怎么处理的？如果加了字段会怎么样？
        
        如果没有显式声明序列号，那么在程序编译时会自己生成这个版本序列号。
        
        如果加了字段，更改了实体类的时候又会重新生成一个序列号。
        
        运行程序，就会报错：InvalidClassException
        
        http://blog.sina.com.cn/s/blog_7f73e06d0100u52c.html
        
        
        115. Override和Overload的区别，分别用在什么场景
        
        方法重载（overload），参数不同。
        
        方法覆盖（override），方法内容不同。用在子类
        
        http://blog.csdn.net/zhouhong1026/article/details/8232350
        
        
        116. java的反射是如何实现的
        
        反射机制其实就是指程序在运行的时候能够获取自身的信息。
        
        如果知道一个类的名称/或者它的一个实例对象， 就能把这个类的所有方法和变量的信息(方法名，变量名，方法，修饰符，类型，方法参数等等所有信息)找出来。
        
        如果明确知道这个类里的某个方法名+参数个数 类型，还能通过传递参数来运行那个类里的那个方法，这就是反射。
        
        尽管Java不是一种动态语言，但它却有一个非常突出的动态机制：Reflection。它使我们可以于运行时加载、探知、使用编译期间完全未知的 classes。
        
        换句话说，Java程序可以加载一个运行时才得知名称的class，获悉其完整构造（但不包括methods定义），并生成其对象实体、 或对其fields设值、或唤起其methods。既一种“看透class”的能力。
        
        http://www.tuicool.com/articles/zuIN7r
        
        http://www.cnblogs.com/my376908915/p/6752707.html
        
        http://blog.csdn.net/liujiahan629629/article/details/18013523
        
         
        
        117.HashMap  HashTable的区别
        
        http://www.importnew.com/7010.html
        
        118.Map集合的四种遍历方式
        
        http://www.cnblogs.com/blest-future/p/4628871.html
        
        119.HashMap如何实现
        
        Hash算法-〉数组，
        
        + 处理冲突-〉链表法
        
        120.多线程循环删除List数组容器里面元素，会ConcurrentModificationException
        
        把List数组转换成Iterator进行迭代删除，一点问题都没有：listA.iterator().remove(); Iterator进行循环操作，然后删除，是很安全的。
        
        121.线程间共享数据的方式
        
        http://www.cnblogs.com/my376908915/p/6756895.html
        
        122.Spring MVC页面渲染的几种方式
        
        http://blog.csdn.net/suifeng3051/article/details/51648360
        
        123.Ioc和AOP的理解和源码
        
        AOP基于什么设计模式实现的？具体说下cglib代理和jdk代理的区别，他们是怎么实现动态代理的，核心类和核心方法是什么
        
        http://www.cnblogs.com/my376908915/p/6782604.html
        
        124.spring bean的几种状态
        
        用的最多的还是singleton单态，prototype原型多态。
        
        125.spring的缓存优化是怎么做的？如何清缓存，缓存哪里用到了，用他做什么？
        
        http://blog.csdn.net/a494303877/article/details/53780597
        
        http://blog.csdn.net/dlf123321/article/details/51382666
        
        126.使用spring初始化需要加载的东西，
        
        bean.dispatcherServlet,加载Html,spring的配置文件
        
        127.如果redis缓存宕掉了怎么办
        
        https://baijiahao.baidu.com/po/feed/share?wfr=spider&for=pc&context=%7B%22sourceFrom%22%3A%22bjh%22%2C%22nid%22%3A%22news_3562062748706618586%22%7D
        
        128.java中异常机制
        
        Throwable是Error和Exception的父类，
        Error一般是指JVM抛出的错误，不需要捕获，Exception是程序错误，需要捕获处理；
        129.有10亿条文本，找出前一万条重复率高的
        
        先Hash算法分割到1000个文件中去；HashMap（文本，counts）；堆排序。
        
        BitMap算法：使用hash计算并存储次数，然后遍历一次找出top10；
        
        
        
        
        
        130.对一千万条数据排序，你认为最好的方式是什么
        分块查找，堆排序。
        BitMap算法。是否有重复。申请长度为一千万位的位向量bit[10000000]，所有位设置为0，顺序读取待排序文件，每读入一个数i，便将bit[i]置为1。当所有数据读入完成，便对bit做从头到尾的遍历，如果bit[i]=1，则输出i到文件，当遍历完成，重复的数据被输出。
        131.10w行数据，每行一个单词，统计出现次数出现最多的前100个。
        
        （1）可以使用小根堆；
        
        （2）在linux中实现：cat words.txt | sort | uniq -c | sort -k1,1nr | head -10
        
        uniq -c: 
         显示唯一的行，并在每行 行首 加上本行在文件中出现的次数
        
        sort -k1,1nr:  按照第一个字段，数值排序，且为逆序
        132.一个文本文件，给你一个单词，判断单词是否出现。
        
        grep -wq "fail" 123.txt && echo "no"||echo "yes"
        -w 精确匹配
        http://www.2cto.com/os/201411/348541.html
        

            redis能把内存空间交换进磁盘中吗(这个应该是可以的，但是那个面试官非跟我说不可以)
            java线程池中基于缓存和基于定长的两种线程池，当请求太多时分别是如何处理的？定长的事用的队列，如果队列也满了呢？交换进磁盘？基于缓存的线程池解决方法呢？
            synchronized加在方法上用的什么锁
            可重入锁中的lock和trylock的区别
            innodb对一行数据的读会枷锁吗？不枷锁，读实际读的是副本
            redis做缓存是分布式存的？不同的服务器上存的数据是否重复？guava cache呢？是否重复？不同的机器存的数据不同
            用awk统计一个ip文件中top10
            对表做统计时可直接看schema info信息，即查看表的系统信息
            mysql目前用的版本
            公司经验丰富的人给了什么帮助？(一般boss面会问这些)
            自己相对于一样的应届生有什么优势
            自己的好的总结习惯给自己今后的工作带了什么帮助，举例为证
            原子类，线程安全的对象，异常的处理方式
            4亿个int数，如何找出重复的数（用hash方法，建一个2的32次方个bit的hash数组，每取一个int数，可hash下2的32次方找到它在hash数组中的位置，然后将bit置1表示已存在）
            4亿个url，找出其中重复的（考虑内存不够，通过hash算法，将url分配到1000个文件中，不同的文件间肯定就不会重复了，再分别找出重复的）
            有1万个数组，每个数组有1000个整数，每个数组都是降序的，从中找出最大的N个数，N<1000
            
            LinkedHashmap的底层实现
            类序列化时类的版本号的用途，如果没有指定一个版本号，系统是怎么处理的？如果加了字段会怎么样？
            Override和Overload的区别，分别用在什么场景
            java的反射是如何实现的
            架构之路学习思路，学习方向！面试阿里必备技能
            1.开源框架解析
            2.架构师巩基
            3.高性能架构
            4.微服务架构
            5.团队协作开发
            6.B2C商城项目
            7.设计模式
    
###8.5 mysql面试题
        https://blog.csdn.net/w372426096/article/details/83511008   (比较全面)
        
        https://blog.csdn.net/w372426096/article/details/83061395
        https://blog.csdn.net/w372426096/article/details/82621532
        https://blog.csdn.net/w372426096/article/details/81281755
        https://blog.csdn.net/w372426096/article/details/80968191
        https://blog.csdn.net/w372426096/article/details/80968221
        
        mysql目前用的版本？
            5.7.21；目前最高5.7.*
        
        left join，right join，inner join？
            left join(左连接) 返回包括左表中的所有记录和右表中连接字段相等的记录 
            right join(右连接) 返回包括右表中的所有记录和左表中连接字段相等的记录
            左连接和右连接很像只是顺序问题。
            inner join(等值连接、内连接) 只返回两个表中连接字段相等的行
        
        MySQL 分页查询语句？
            select * from table limit (start-1)*limit,limit; 其中start是页码，limit是每页显示的条数
        
        脏读、幻读、不可重复读?
            https://blog.csdn.net/w372426096/article/details/80798062
            
        数据库的几大范式?
            第一范式----数据库中的表(所有字段值)都是不可分割的原子数据项。
            第二范式----数据库表中的每一列都和主键相关，而不能只和主键的某一部分相关。也就是说 一个表中只能只能包含一个，不能把多种数据
                保存在同一个表中。
            第三范式----数据库表中每一列数据都和主键直接相关，不能间接相关。
        数据库常见的命令?
            https://www.cnblogs.com/zhouzetian/p/6429181.html
        
        注意truncat,drop（DDL）,delete(操作语句)区别
        
        DDL、DML、DCL分别指什么？
            一、DDL :数据定义语言，用于定义和管理 SQL 数据库中的所有对象的语言
                1.CREATE - to create objects in the database 创建
                2.ALTER - alters the structure of the database 修改
                3.DROP - delete objects from the database 删除
                4.TRUNCATE - remove all records from a table, including all spaces allocated for the records are removed
                　　Truncate table 表名 速度快,而且效率高,因为:
                　　TRUNCATE TABLE 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。
                　　DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。
                　　TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 DROP TABLE 语句。
                　　对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。
                　　TRUNCATE TABLE 不能用于参与了索引视图的表。
                5.COMMENT - add comments to the data dictionary 注释
                6.GRANT - gives user's access privileges to database 授权
                7.REVOKE - withdraw access privileges given with the GRANT command 收回已经授予的权限
            
            二、DML:数据操作语言，SQL中处理数据等操作统称为数据操纵语言
                1.SELECT - retrieve data from the a database 查询
                2.INSERT - insert data into a table 添加
                3.UPDATE - updates existing data within a table 更新
                4.DELETE - deletes all records from a table, the space for the records remain 删除
                5.CALL - call a PL/SQL or Java subprogram
                6.EXPLAIN PLAN - explain access path to data
                7.LOCK TABLE - control concurrency 锁，用于控制并发
            
            三、DCL:数据控制语言，用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等
                1.COMMIT - save work done 提交
                2.SAVEPOINT - identify a point in a transaction to which you can later roll back 保存点
                3.ROLLBACK - restore database to original since the last COMMIT 回滚
                4.SET TRANSACTION - Change transaction options like what rollback segment to use 设置当前事务的特性，它对后面的事务没有影响．
            
        SQL的整个解析、执行过程原理；
        
        SQL行转列
        
        sql语句各种条件的执行顺序，如select， where， order by， group by
            写的顺序：select ... from... where.... group by... having... order by..
            执行顺序：from... where...group by... having.... select ... order by...
        
        查看执行计划
        
    索引
        
        索引类别（B+树索引、全文索引、哈希索引）、索引的原理？为什么要用 B+tree作为MySQL索引的数据结构？
            1.索引的作用：提高数据查询效率
            2.常见索引模型：哈希表、有序数组、搜索树
            3.哈希表：键 - 值(key - value)。
            4.哈希思路：把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置
            5.哈希冲突的处理办法：链表
            6.哈希表适用场景：只有等值查询的场景（NOSQL）
            7.有序数组：按顺序存储。查询用二分法就可以快速查询，时间复杂度是：O(log(N))
            8.有序数组查询效率高，更新效率低
            9.有序数组的适用场景：静态存储引擎。
            10.二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子
            11.二叉搜索树：查询时间复杂度O(log(N))，更新时间复杂度O(log(N))
            12.数据库存储大多不适用二叉树，因为树高过高(会查磁盘比查内存慢)，会适用N叉树
            13.InnoDB中的索引模型：B+Tree
            14.索引类型：主键索引、非主键索引
            主键索引的叶子节点存的是整行的数据(聚簇索引)，非主键索引的叶子节点内容是主键的值(二级索引)
            15.主键索引和普通索引的区别：主键索引只要搜索ID这个B+Tree即可拿到数据。普通索引先搜索索引拿到主键值，再到主键索引树搜索一次(回表)
            16.一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。
            17.从性能和存储空间方面考量，自增主键往往是更合理的选择。
        
        $$聚集索引与非聚集索引的区别？
            根本区别
                聚集索引和非聚集索引的根本区别是:表记录的排列顺序和与索引的排列顺序是否一致。
                     聚集索引
                        聚集索引表记录的排列顺序和索引的排列顺序一致，所以查询效率快，只要找到第一个索引值记录，其余就连续性的记录在物理也一样连续存放。
                        聚集索引对应的缺点就是修改慢，因为为了保证表中记录的物理和索引顺序一致，在记录插入的时候，会对数据页重新排序。
                非聚集索引
                    非聚集索引制定了表中记录的逻辑顺序，但是记录的物理和索引不一定一致，两种索引都采用B+树结构，非聚集索引的叶子层并不和实际数据页相重叠，
                    而采用叶子层包含一个指向表中的记录在数据页中的指针方式。非聚集索引层次多，不会造成数据重排。
        
        SQL 索引的顺序，字段的顺序？最左原则?
            回表：回到主键索引树搜索的过程，称为回表
            覆盖索引：某索引已经覆盖了查询需求，称为覆盖索引，例如：select ID from T where k between 3 and 5
                在引擎内部使用覆盖索引在索引K上其实读了三个记录，R3~R5(对应的索引k上的记录项)，但对于MySQL的Server层来说，它就是找引擎拿到了两条记录，因此MySQL认为扫描行数是2
            最左前缀原则：B+Tree这种索引结构，可以利用索引的"最左前缀"来定位记录
                只要满足最左前缀，就可以利用索引来加速检索。
            最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符
            第一原则是：如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。
            索引下推：在MySQL5.6之前，只能从根据最左前缀查询到ID开始一个个回表。到主键索引上找出数据行，再对比字段值。
            MySQL5.6引入的索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。
        
    order by 原理
        1.MySQL会为每个线程分配一个内存（sort_buffer）用于排序该内存大小为sort_buffer_size
            1>如果排序的数据量小于sort_buffer_size，排序将会在内存中完成
            2>如果排序数据量很大，内存中无法存下这么多数据，则会使用磁盘临时文件来辅助排序，也称外部排序
            3>在使用外部排序时，MySQL会分成好几份单独的临时文件用来存放排序后的数据，然后在将这些文件合并成一个大文件
        2.mysql会通过遍历索引将满足条件的数据读取到sort_buffer，并且按照排序字段进行快速排序
            1>如果查询的字段不包含在辅助索引中，需要按照辅助索引记录的主键返回聚集索引取出所需字段
            2>该方式会造成随机IO，在MySQL5.6提供了MRR的机制，会将辅助索引匹配记录的主键取出来在内存中进行排序，然后在回表
            3>按照情况建立联合索引来避免排序所带来的性能损耗，允许的情况下也可以建立覆盖索引来避免回表
        
        全字段排序
            1.通过索引将所需的字段全部读取到sort_buffer中
            2.按照排序字段进行排序
            3.将结果集返回给客户端
            缺点：
            1.造成sort_buffer中存放不下很多数据，因为除了排序字段还存放其他字段，对sort_buffer的利用效率不高
            2.当所需排序数据量很大时，会有很多的临时文件，排序性能也会很差
            优点：MySQL认为内存足够大时会优先选择全字段排序，因为这种方式比rowid 排序避免了一次回表操作
            
        rowid排序
            1.通过控制排序的行数据的长度来让sort_buffer中尽可能多的存放数据，max_length_for_sort_data
            2.只将需要排序的字段和主键读取到sort_buffer中，并按照排序字段进行排序
            3.按照排序后的顺序，取id进行回表取出想要获取的数据
            4.将结果集返回给客户端
            优点：更好的利用内存的sort_buffer进行排序操作，尽量减少对磁盘的访问
            缺点：回表的操作是随机IO，会造成大量的随机读，不一定就比全字段排序减少对磁盘的访问
            3.按照排序的结果返回客户所取行数
        
        MySQL 索引使用的注意事项
        遇到过索引失效的情况没，什么时候可能会出现，如何解决？
        索引失效的场景
        
    优化：
        常见的数据库优化方案？
        第一阶段 优化sql和索引
            这一步成本最低啊，不需要加什么中间件。你没经过索引优化和SQL优化。
            步骤大概
            用慢查询日志定位执行效率低的 SQL语句
            用 explain分析 SQL的执行计划
            确定问题，采取相应的优化措施，建立索引啊，等
            
        第二阶段 搭建缓存
            在优化sql无法解决问题的情况下，才考虑搭建缓存。毕竟你使用缓存的目的，就是将复杂的、耗时的、不常变的执行结果缓存起来，降低数据库的资源消耗。
            这里需要注意的是:搭建缓存后，系统的复杂性增加了。你需要考虑很多问题，比如:
            缓存和数据库一致性问题？(比如是更缓存，还是删缓存),这点可以看我的一篇文章《数据库和缓存双写一致性方案解析》。
            缓存击穿、缓存穿透、缓存雪崩问题如何解决？是否有做缓存预热的必要。不过我猜，大部分中小公司应该都没考虑。这点可以看我的另一篇《分布式之redis复习精讲》
        
        第三阶段 读写分离
            缓存也搞不定的情况下，搞主从复制，上读写分离。在应用层，区分读写请求。或者利用现成的中间件 mycat或者 altas等做读写分离。
            需要注意的是,只要你敢说你用了主从架构，有三个问题，你要准备:
            1.主从的好处？
                回答:实现数据库备份，实现数据库负载均衡，提高数据库可用性
            2.主从的原理?
                回答:如图所示（图片不是自己画的，偷懒了）
                主库有一个 logdump线程，将 binlog传给从库
                从库有两个线程，一个I/O线程，一个SQL线程，I/O线程读取主库传过来的 binlog内容并写入到 relay log,SQL线程从 relay log里面读取内容，
                写入从库的数据库。
            3.如何解决主从一致性?
            
                回答:这个问题，我不建议在数据库层面解决该问题。根据 CAP 定理，主从架构本来就是一种高可用架构，是无法满足一致性的。
                 哪怕你采用同步复制模式或者半同步复制模式，都是弱一致性，并不是强一致性。所以，推荐还是利用缓存，来解决该问题。
                
                步骤如下:
                
                自己通过测试，计算主从延迟时间，建议mysql版本为5.7以后，因为mysql自5.7开始，多线程复制功能比较完善，一般能保证延迟在1s内。
                不过话说回来，mysql现在都出到8.x了，还有人用5.x的版本么。
                
                数据库的写操作，先写数据库，再写cache，但是有效期很短，就比主从延时的时间稍微长一点。
                读请求的时候，先读缓存，缓存存在则直接返回。如果缓存不存在(这时主从同步已经完成)，再读数据库。
            
        第四阶段 利用分区表
        
            说句实在话，你们面试的时候，其实可以略过这个阶段。因为很多互联网公司都不建议用分区表，我自己也不太建议用分区表，采用这个分区表，坑太多。
            这里引用一下其他文章的回答:
            
            什么是mysql的分区表？
            
            回答：所有数据还在一个表中，但物理存储根据一定的规则放在不同的文件中。这个是mysql支持的功能，业务代码不需要改动，
            但是sql语句需要改动，sql条件需要带上分区的列。
            
            缺点
            分区键设计不太灵活，如果不走分区键，很容易出现全表锁
            在分区表使用 ALTER TABLE … ORDER BY，只能在每个分区内进行 orderby。
            分区表的分区键创建索引，那么这个索引也将被分区。分区键没有全局索引一说。
            自己分库分表，自己掌控业务场景与访问模式，可控。分区表，研发写了一个sql，都不确定该去哪个分区查，不太可控。 …不列举了，不推荐
        
        第五阶段 垂直拆分
            上面四个阶段都没搞定，就来垂直拆分了。垂直拆分的复杂度还是比水平拆分小的。将你的表，按模块拆分为不同的小表。
            大家应该都看过《大型网站架构演变之路》，这种类型的文章或者书籍，基本都有提到这一阶段。
            
            如果你有幸能够在什么运营商、银行等公司上班，你会发现他们一个表，几百个字段都是很常见的事情。所以，应该要进行拆分，拆分原则一般是如下三点:
                把不常用的字段单独放在一张表。
                把常用的字段单独放一张表
                经常组合查询的列放在一张表中（联合索引）。
        
        第六阶段 水平拆分
            OK,水平拆分是最麻烦的一个阶段，拆分后会有很多的问题，我再强调一次，水平拆分一定是最最最最后的选择。从某种意义上，
            我觉得还不如垂直拆分。因为你用垂直拆分，分成不同模块后，发现单模块的压力过大，你完全可以给该模块单独做优化，
            例如提高该模块的机器配置等。如果是水平拆分，拆成两张表，代码需要变动，然后发现两张表还不行，再变代码，再拆成三张表的？
            水平拆分后，各模块间耦合性太强，成本太大，慎重。
            
        在你的项目中数据库如何进行优化的？
        `慢sql优化，缓存，读写分离，设计表时候垂直拆分，终极水平拆分
        
        SQL 优化的常见方法有哪些？
        
        SQL逻辑相同,性能差异较大的,大概有以下几类:
            一.字段发生了转换,导致本该使用索引而没有用到索引
                1.条件字段函数操作
                2.隐式类型转换
                3.隐式字符编码转换
                (如果驱动表的字符集比被驱动表得字符集小，关联列就能用到索引,如果更大,需要发生隐式编码转换,则不能用到索引,latin<gbk<utf8<utf8mb4)
        
            二.嵌套循环,驱动表与被驱动表选择错误
                1.连接列上没有索引,导致大表驱动小表,或者小表驱动大表(但是大表走的是全表扫描) --连接列上建立索引
                2.连接列上虽然有索引,但是驱动表任然选择错误。--通过straight_join强制选择关联表顺序
                3.子查询导致先执行外表在执行子查询,也是驱动表与被驱动表选择错误。
                --可以考虑把子查询改写为内连接,或者改写内联视图(子查询放在from后组成一个临时表,在于其他表进行关联)
                4.只需要内连接的语句,但是写成了左连接或者右连接。比如select * from t left join b on t.id=b.id where b.name='abc'驱动表被固定,大概率会扫描更多的行,导致效率降低.
                --根据业务情况或sql情况,把左连接或者右连接改写为内连接
        
            三.索引选择不同,造成性能差异较大
                1.select * from t where aid= and create_name>'' order by id limit 1;
                选择走id索引或者选择走(aid,create_time)索引,性能差异较大.结果集都有可能不一致
                --这个可以通过where条件过滤的值多少来大概判断,该走哪个索引
            
            四.其它一些因素
                1.比如之前学习到的是否有MDL X锁
                2.innodb_buffer_pool设置得太小,innodb_io_capacity设置得太小,刷脏速度跟不上
                3.是否是对表做了DML语句之后,马上做select,导致change buffer收益不高
                4.是否有数据空洞
                5.select选取的数据是否在buffer_pool中
                6.硬件原因,资源抢占
                原因多种多样,还需要慢慢补充。
        
        explain命令？
            https://blog.csdn.net/w372426096/article/details/82421378
        
        对于SQL慢查询的优化？（主要是从查询语句和数据库表设计两个方面来考虑，查询语句方面可以增加索引，增加查询筛选的限制条件；
        数据库表设计的时候可以拆分表，设计得更细粒度。但是后来才发现面试官想要的就是查询大量数据的慢查询问题的优化。）
        
        MySQL遇到的死锁问题、如何排查与解决？
        日志
        
        limit 20000 加载很慢怎么解决？
         https://uule.iteye.com/blog/2422189
        
        select xx from xx where xx and xx order by xx limit xx； 如何优化这个（看explain）
        事务：
        
        mysql是如何实现事务的
        MySQL 事务特性及特性和隔离级别
            1、事务的特性：原子性、一致性、隔离性、持久性
            2、多事务同时执行的时候，可能会出现的问题：脏读、不可重复读、幻读
            3、事务隔离级别：读未提交、读提交（Oracle默认）、可重复读（Mysql默认）、串行化
            4、不同事务隔离级别的区别：
                读未提交：一个事务还未提交，它所做的变更就可以被别的事务看到
                读提交：一个事务提交之后，它所做的变更才可以被别的事务看到
                可重复读：一个事务执行过程中看到的数据是一致的。未提交的更改对其他事务是不可见的
                串行化：对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行
            5、配置方法：启动参数transaction-isolation
            6、事务隔离的实现：每条记录在更新的时候都会同时记录一条回滚操作。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。
        
        如何避免长事务对业务的影响？
        
        首先，从应用开发端来看：
            确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，
            通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。
            
            确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好
            几个 select 语句放到了事务中。这种只读事务可以去掉。
            
            业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外
            执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）
        
        其次，从数据库端来看：(事务隔离相关)
            1.innodb支持RC和RR隔离级别实现是用的一致性视图(consistent read view)
            2.事务在启动时会拍一个快照,这个快照是基于整个库的.
                基于整个库的意思就是说一个事务内,整个库的修改对于该事务都是不可见的(对于快照读的情况)
                如果在事务内select t表,另外的事务执行了DDL t表,根据发生时间,要嘛锁住要嘛报错(参考第六章)
            
            3.事务是如何实现的MVCC呢?
                (1)每个事务都有一个事务ID,叫做transaction id(严格递增)
                (2)事务在启动时,找到已提交的最大事务ID记为up_limit_id。
                (3)事务在更新一条语句时,比如id=1改为了id=2.会把id=1和该行之前的row trx_id写到undo log里,
                    并且在数据页上把id的值改为2,并且把修改这条语句的transaction id记在该行行头
                (4)再定一个规矩,一个事务要查看一条数据时,必须先用该事务的up_limit_id与该行的transaction id做比对,
                如果up_limit_id>=transaction id,那么可以看.如果up_limit_id<transaction id,则只能去undo log里去取。去undo log
                查找数据的时候,也需要做比对,必须up_limit_id>transaction id,才返回数据
            
            4.什么是当前读,由于当前读都是先读后写,只能读当前的值,所以为当前读.会更新事务内的up_limit_id为该事务的transaction id
            
            5.为什么rr（可重复读）能实现可重复读而rc(读提交)不能,分两种情况
                (1)快照读的情况下,rr不能更新事务内的up_limit_id,
                    而rc每次会把up_limit_id更新为快照读之前最新已提交事务的transaction id,则rc不能可重复读
                (2)当前读的情况下,rr是利用record lock+gap lock来实现的,而rc没有gap,所以rc不能可重复读 
        
        
        选择普通索引还是唯一索引？
        对于查询过程来说：
        a、普通索引，查到满足条件的第一个记录后，继续查找下一个记录，知道第一个不满足条件的记录
        b、唯一索引，由于索引唯一性，查到第一个满足条件的记录后，停止检索
        但是，两者的性能差距微乎其微。因为InnoDB根据数据页来读写的。
        对于更新过程来说：
        概念：change buffer
        当需要更新一个数据页，如果数据页在内存中就直接更新，如果不在内存中，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中。下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中的与这个页有关的操作。
        change buffer是可以持久化的数据。在内存中有拷贝，也会被写入到磁盘上
        purge:将change buffer中的操作应用到原数据页上，得到最新结果的过程，成为purge
        访问这个数据页会触发purge，系统有后台线程定期purge，在数据库正常关闭的过程中，也会执行purge
        唯一索引的更新不能使用change buffer
        change buffer用的是buffer pool里的内存，change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。
        将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。
        change buffer 因为减少了随机磁盘访问，所以对更新性能的提升很明显。
        change buffer使用场景
        在一个数据页做purge之前，change buffer记录的变更越多，收益就越大。
        对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。
        反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer,但之后由于马上要访问这个数据页，会立即触发purge过程。
        这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用。
        索引的选择和实践：
        尽可能使用普通索引。
        redo log主要节省的是随机写磁盘的IO消耗(转成顺序写)，而change buffer主要节省的则是随机读磁盘的IO消耗。 
        
        监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
        Percona 的 pt-kill 这个工具不错，推荐使用；
        在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
        如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。
        如果某次写入使用了 change buffer 机制，之后主机异常重启，是否会丢失 change buffer 和数据。
        
        不会丢失。虽然是只更新内存，但是在事务提交的时候，把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。
        
        merge 的过程是否会把数据直接写回磁盘。
        
        merge 的执行流程是这样的：
        
        从磁盘读入数据页到内存（老版本的数据页）；
        
        从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页；
        
        写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。
        
        到这里 merge 过程就结束了。这时候，数据页和内存中 change buffer 对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。
        
        数据一致性问题目前来说主要分为三类
        1.主从不一致
        解决办法:半同步复制after_commit,after_sync,MGR(after_prepare)。但是都不能完成满足完全实时一致,由于等待的ack点不同,相对来说一致性的强度是递增.
        2.数据库与缓存的不一致
        解决办法:读操作直接读缓存,写操作先更新到数据库,淘汰缓存(程序需要保证两个操作的原子性).由于该key的缓存已经清理掉,那么下次读的时候需要先读数据库,在重建缓存.
        由于redis是单线程,保证了一个操作的原子性.可以通过设置appendfsync always来保证每次操作都把该操作记录并落盘到aof文件里(不过一般redis该值为everysec),毕竟使用redis的目的不是为了保证acid.还是要根据业务来选择
        3.一个事务跨多个节点或者多种数据库(分库分表和银行转账这种例子)
        目前好像都是通过2pc,3pc来保证的。
        count(字段值):如果该字段上有null值.每行的行头有一个标记位,标记该行是否为null.所以多了一层判断。相对更耗时
        count(主键id):即便是选择的有null值的二级索引,但是也可以挺快的正确计数。因为null的话字段值虽然为null,但是该行上主键id以及指向聚簇索引该id的指针还是存在的,所以不影响计数,也不用做判断,直接遍历该二级索引,取出id值,按行累加就行。
        count(1)和count(*):看官方文档上说是5.7.18版本之前是扫描聚簇索引,之后是二级索引。虽然不取值,只计数。但是二级索引比聚簇索引需要扫描的页数相对来说更少,这应该也是一种优化,不过我做测试percona版本的5.6都是选择了二级索引
        这期干货挺多的,学会了如果某表上有count比较多的操作,最好是用count(1)或者count(*),然后选择一列占用字节数最少的建立索引(比如tinyint类型)
        
        分布式事务的理解，常见的解决方案有哪些，什么是两阶段提交、三阶段提交；
        
        两阶段提交
        
        
        
        分布式事务的原理2阶段提交，同步\异步\阻塞\非阻塞；
        数据库事务隔离级别，MySQL默认的隔离级别、Spring如何实现事务、JDBC如何实现事务、嵌套事务实现、分布式事务实现；
        
        MySQL记录binlog的方式主要包括三种模式？每种模式的优缺点是什么？
        mysql的binlog
        
        redolog是物理的，binlog是逻辑的；现在由于redo是属于InnoDB引擎，所以必须要有binlog，因为你可以使用别的引擎
        保证数据库的一致性，必须要保证2份日志一致，使用的2阶段式提交；其实感觉像事务，不是成功就是失败，不能让中间环节出现，也就是一个成功，一个失败
        如果有一天mysql只有InnoDB引擎了，有redo来实现复制，那么感觉oracle的DG就诞生了，物理的速度也将远超逻辑的，毕竟只记录了改动向量
        binlog几大模式，一般采用row，因为遇到时间，从库可能会出现不一致的情况，但是row更新前后都有，会导致日志变大
        最后2个参数，保证事务成功，日志必须落盘，这样，数据库crash后，就不会丢失某个事务的数据了
        备份时间问题：
        备份时间周期的长短，感觉有2个方便
        首先，是恢复数据丢失的时间，既然需要恢复，肯定是数据丢失了。如果一天一备份的话，只要找到这天的全备，加入这天某段时间的binlog来恢复，如果一周一备份，假设是周一，而你要恢复的数据是周日某个时间点，那就，需要全备+周一到周日某个时间点的全部binlog用来恢复，时间相比前者需要增加很多；看业务能忍受的程度
        其次，是数据库丢失，如果一周一备份的话，需要确保整个一周的binlog都完好无损，否则将无法恢复；而一天一备，只要保证这天的binlog都完好无损；当然这个可以通过校验，或者冗余等技术来实现，相比之下，上面那点更重要
        binlog主从复制
        引擎
        存储引擎的 InnoDB与MyISAM区别，优缺点，使用场景
        mysql的存储引擎,区别
        
        innodb对一行数据的读会枷锁吗？不枷锁，读实际读的是副本
        
        求表的size，或做数据统计可用什么存储引擎
        
        读多写少可用什么引擎
        假如要统计多个表应该用什么引擎
        
        分表分库
        说说分库与分表设计
        分库与分表带来的分布式困境与应对之策（如何解决分布式下的分库分表，全局表？）
        
        锁
        
        MySQL锁，悲观锁、乐观锁、排它锁、共享锁、表级锁、行级锁；
        
        全局锁，表级锁
        
        一、全局锁：
        对整个数据库实例加锁。
        MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL)
        这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。
        使用场景：全库逻辑备份。
        风险：
        1.如果在主库备份，在备份期间不能更新，业务停摆
        2.如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟
        官方自带的逻辑备份工具mysqldump，当mysqldump使用参数--single-transaction的时候，会启动一个事务，确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。
        一致性读是好，但是前提是引擎要支持这个隔离级别。
        如果要全库只读，为什么不使用set global readonly=true的方式？
        1.在有些系统中，readonly的值会被用来做其他逻辑，比如判断主备库。所以修改global变量的方式影响太大。
        2.在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。
        二、表级锁
        MySQL里面表级锁有两种，一种是表锁，一种是元数据所(meta data lock,MDL)
        表锁的语法是:lock tables ... read/write
        可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
        对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。
        MDL：不需要显式使用，在访问一个表的时候会被自动加上。
        MDL的作用：保证读写的正确性。
        在对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。
        读锁之间不互斥。读写锁之间，写锁之间是互斥的，用来保证变更表结构操作的安全性。
        MDL 会直到事务提交才会释放，在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新。
        
        行级锁
        
        两阶段锁：在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放， 而是要等到事务结束时才释放。
        建议：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。
        死锁：当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态。
        解决方案：
        1、通过参数 innodb_lock_wait_timeout 根据实际业务场景来设置超时时间，InnoDB引擎默认值是50s。
        2、发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑（默认是开启状态）。
        如何解决热点行更新导致的性能问题？
        1、如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关闭掉。一般不建议采用
        2、控制并发度，对应相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。
        3、将热更新的行数据拆分成逻辑上的多行来减少锁冲突，但是业务复杂度可能会大大提高。
        
        innodb行级锁是通过锁索引记录实现的，如果更新的列没建索引是会锁住整个表的。
        
        要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：
        
        第一种，直接执行 delete from T limit 10000;
        
        第二种，在一个连接中循环执行 20 次delete from T limit 500;
        
        第三种，在20 个连接中同时执行 delete from T limit500.
        
        第二种好，第一种单个语句占用时间长，锁的时间长，大事务导致主从延迟；第三种会认为造成锁冲突。
        
        1.delete操作会生成插入相同记录的记录复用和page复用
        2.delete会产生page空洞，随机insert也会产生page空洞(页分裂)，索引update分解为delete和insert也会产生空洞
        3.重建表可以使数据在page上更紧凑
        4.alter table tb_name engine=innodb 在非online ddl 时server层生成临时表且mdl写锁，阻塞其他会话dml操作，锁阻塞时间久。在online ddl时，时innodb引擎操作步骤：
        a.扫描表页，获取表的dml读锁;
        b.将表页复制到一个临时文件，以b+树格式存储;
        c.在扫描和复制page的过程生成row.log日志记录ddl复制过程的dml操作;
        d.将row.log操作日志应用到临时文件;
        e.获取dml的写锁，临时文件与表文件替换
        
        5.ddl online时，innodb表加全文索引时，会阻塞dml操作，其实效果与非online ddl一样的。ddl online一定是inplace;inplace 的ddl不一定是online,例如:添加fulltext索引和spatial索引
        6.optimize table 重建表及索引，收集统计信息
        alter table tb engine=innodb 重建表其实也收集统计信息
        analyze table tb 重新收集统计信息
        
        乐观锁的业务场景及实现方式；
        mysql的行级锁加在哪个位置
        如何选择合适的分布式主键方案
        选择合适的数据存储方案
        
        分布式
        读写分离何时强制要读主库，读哪个从库是通过什么方式决定的，从库的同步mysql用的什么方式
        主从复制
        对表做统计时可直接看schema info信息，即查看表的系统信息
        统计100G的ip文件中出现ip次数最多的100个ip
        不可重复读会出现在什么场景？
        项目 MySQL 的数据量和并发量有多大？
        mvcc,Next-Key Lock
        
        select * from table where a=1 and b=1; select * from table where b=1; select * from table where a=1; 你会如何创建索引？
        user表和blog表连表查询规定时间范围内发帖最多的前10个用户；
        MySQL索引的创建原理和准则给你道题让你创建；
        从数据中查出某个时间段访问最多前10个ip，sql或者shell都行；
        
        5万条数据怎么实现删除
        性能优化
    
### 8.6 综合常规面试题
    https://www.zhihu.com/collection/72725882   (优先级高)
    
### 8.7 mybatis常见面试题
    https://segmentfault.com/a/1190000013678579
    
### 8.8 综合常规面试题
    https://github.com/leelovejava/doc/blob/master/interview/interview.md
### 8.9 常见综合面试题
    https://blog.csdn.net/sinat_35512245/article/details/59056120   (优先级低)
### 8.10 常见综合面试题
    https://juejin.im/post/5c788d986fb9a049f154e479 (优先级高)
### 8.11 kafka面试题
    https://juejin.im/post/5c7bd09b6fb9a049ba424c15
### 8.12 计算机网络面试题
    https://www.cnblogs.com/qiaozhoulin/p/5075372.html
### 8.13 常见面试题包括dubbo,netty
    https://juejin.im/post/5c86fb99e51d4557a74b98bc
### 8.14 面试题
    (阿里)
       1）Java的数据结构相关的类实现原理，比如LinkedList，ArrayList，HashMap，TreeMap这一类的。以下简单模拟一个数据结构的连环炮。
    　　比如，面试官先问你HashMap是不是有序的？
    　　你肯定回答说，不是有序的。那面试官就会继续问你，有没有有顺序的Map实现类？
    　　你如果这个时候说不知道的话，那这个问题就到此结束了。如果你说有TreeMap和LinkedHashMap。
    　　那么面试官接下来就可能会问你，TreeMap和LinkedHashMap是如何保证它的顺序的？
    　　如果你回答不上来，那么到此为止。如果你依然回答上来了，那么面试官还会继续问你，你觉得它们两个哪个的有序实现比较好？
    　　如果你依然可以回答的话，那么面试官会继续问你，你觉得还有没有比它更好或者更高效的实现方式？
    　　如果你还能说出来的话，那么就你所说的实现方式肯定依然可以问你很多问题。
    　　以上就是一个面试官一步一步提问的例子。所以，如果你了解的不多，千万不要敷衍，因为可能下一个问题你就暴露了，还不如直接说不会，把这个问题结束掉，赶紧切换到你熟悉的领域。
    
    　　2）Java并发包当中的类，它们都有哪些作用，以及它们的实现原理，这些类就是java.concurrent包下面的。与上面一样，咱们也简单的模拟一个并发包的连环炮。
    　　比如面试官可能会先问你，如果想实现所有的线程一起等待某个事件的发生，当某个事件发生时，所有线程一起开始往下执行的话，有什么好的办法吗？
    　　这个时候你可能会说可以用栅栏（Java的并发包中的CyclicBarrier），那么面试官就会继续问你，你知道它的实现原理吗？
    　　如果你继续回答的话，面试官可能会继续问你，你还知道其它的实现方式吗？
    　　如果你还能说出很多种实现方式的话，那么继续问你，你觉得这些方式里哪个方式更好？
    　　如果你说出来某一个方式比较好的话，面试官依然可以继续问你，那如果让你来写的话，你觉得还有比它更好的实现方式吗？
    　　如果你这个时候依然可以说出来你自己更好的实现方式，那么面试官肯定还会揪着这个继续问你。
    　　为什么说面试的时候要引导面试官，原因就在这了。因为面试官的提问很多时候都是有迹可循的，你如果抓住了他的轨迹，能够猜到他下面很可能会问什么，那你在回答的时候就可以往你想要谈的方向去说。这样面试时就会显得更加从容，更加的游刃有余。
    　　
        3）IO包和NIO包中的内容。这部分里面NIO会是重点，IO包大部分都会比较熟悉，因此可能会直接略过，直接问你NIO的内容。
    　　IO包和NIO包的内容相对来说不是很多，首先NIO模型要熟悉，特别是其中的selector一定要非常清楚它的职责和实现原理。其实NIO的核心是IO线程池，一定要记住这个关键点。有的时候，面试官可能也会问你IO包的设计模式（装饰器模式），为什么要这样设计？
    　　有的面试官还会问你有没有更好的设计，这个时候如果你不知道请果断说自己现在的水平有限，想不出来更好的设计，千万不要信口开河，随意YY。
    　　
        4）Java的虚拟机的内容。这部分主要包括三部分，GC、类加载机制，以及内存。
    　　一个GC部分简单的连环炮。
        面试官可以先问你什么时候一个对象会被GC？
    　　接着继续问你为什么要在这种时候对象才会被GC？
    　　接着继续问你GC策略都有哪些分类？
    　　你如果说出来了，继续问你这些策略分别都有什么优劣势？都适用于什么场景？
    　　你继续说出来了以后，给你举个实际的场景，让你选择一个GC策略？
    　　你如果选出来了，继续问你，为什么要选择这个策略？
    　　
        下面是关于类加载机制的简单连环炮。
            首先肯定是先问你Java的类加载器都有哪些？
        　　回答了这些以后，可能会问你每个类加载器都加载哪些类？
        　　说完以后，可能会问你这些类加载之间的父子关系是怎样的？
        　　你在回答的时候可能会提到双亲委派模型，那么可以继续问你什么是双亲委派模型？
        　　你解释完了以后，可能会继续问你，为什么Java的类加载器要使用双亲委派模型？
        　　你回答完以后，可能会继续问你如何自定义自己的类加载器，自己的类加载器和Java自带的类加载器关系如何处理？
    　　
        再来一个关于内存的连环炮。
            首先肯定就是问你内存分为哪几部分，这些部分分别都存储哪些数据？
        　　然后继续问你一个对象从创建到销毁都是怎么在这些部分里存活和转移的？
        　　接着可能会问你，内存的哪些部分会参与GC的回收？
        　　完事以后，可能还会问你Java的内存模型是怎么设计的？
        　　你回答了以后，还会继续问你为什么要这么设计？
        　　问完以后，还可能会让你结合内存模型的设计谈谈volatile关键字的作用？
        　　你在谈的时候，肯定会提到可见性，那么接着可见性这三个字，还可以继续问你并发的内容。
        　　基本上Java语言本身以及语言稍微高级点的内容就是以上部分，如果你能把以上四部分了解的非常透彻，那基本上Java这部分就没啥问题了，因为光以上的内容就够你跟面试官聊很久了。你聊这些聊得久了，自然问你其它问题的时间就会短点。
        　　你从LZ写这些问题的过程也应该能感受出来，很多时候，面试官都是顺着一条线一路问下去的，如果你觉得这条线你不熟悉的话，就要及时拐弯，引导面试官去问其它方面的问题。千万不要一直往下深入，直到自己跳不出来为止，那就尴了个尬了。
    
    　　2、讲述自己的项目，并在中间穿插着问题
            这一部分是面试过程中必问，也是聊得最久的一个阶段。除非你前面的语言部分非常扎实，扎实到面试官问了一两个小时，依旧没有探出你对语言本身的了解到底有多深。否则的话，你一定逃不过自己的项目这一关，而且一般情况下聊得时间不会太短。
        　　这一部分内容，一般的模式就是你自己去讲你做过的项目，然后面试官会冷不丁的让你去解释其中某一部分，比如让你解释当时为什么要这么做，或者问你现在觉得有没有更好的办法。而这些穿插的问题，大部分与你的项目所用到的技术有关。而你需要做的，就是充分、再充分的去总结自己做过的项目（尤其是最近的一两个项目），挖掘出一个甚至N个亮点，以备于到时候可以让面试官产生眼前一亮的感觉。如果你能达到这种效果的话，基本上离你成功就不远了。
        　　这部分内容由于和每个人自己的经历息息相关，因此这里也没法列举可能问到的问题。这篇文章《程序员面经：面试前到底该不该刷题以及面试前该如何准备》是LZ之前写的，里面大概讨论了下如何在面试前总结，有兴趣的可以去了解一下。
    
    　　3、额外的加分项
        　　上面两个阶段基本上是必问的，还有一些加分项。这些加分项中，有些内容面试官也会问你（比如TCP/IP协议、算法），但更多的是会先问你了解不了解，你了解的话再继续聊，不了解的话就直接略过了，不至于因为这种问题而直接把你打入地狱。
        　　下面LZ列举一下这些加分项，如果可以的话，这些加分项还是要争取一下的。
        　　1、计算机系统原理。
        　　2、网络通信协议（TCP/IP，HTTP等）。
        　　3、数据结构与算法。
        　　4、著名开源项目的源码。
        　　5、你自己有很棒的开源项目。
        　　6、你的个人博客。
        　　7、待评论区补充。
    　　    这几项当中，对于前1-3项，如果你之前就比较了解，只是由于时间问题忘记了的话，还是可以临时抱佛脚一下的。至于后面4-6项，就需要你日常的积累了，不是一时半会儿能做到的。如果你平日里没有积累，那么后面这三个加分项只能抛弃了。
    
    　　4、与你职位相关的内容
        　　其实这最后一项是对前面三项的补充，你应该尽量去主攻和你面试的职位相关的内容。比如你面试一个实时计算的职位，那么你的算法最好要厉害，对于著名的实时计算开源项目要熟悉，最好阅读过源码，而且还要对分布式系统有一定的见解。
        　　因此，这个第4部分没有具体的内容，只是提醒你，如果你很明确自己的面试职位，最好在面试前准备的时候，尽量朝职位的需求方向靠拢，这样成功的可能性更大。
            对于Java程序猿学习的建议 　　这一部分其实也算是今天的重点，这一部分用来回答很多群里的朋友所问过的问题，那就是LZ你是如何学习Java的，能不能给点建议？
        　　今天LZ是打算来点干货，因此咱们就不说一些学习方法和技巧了，直接来谈每个阶段要学习的内容甚至是一些书籍。这一部分的内容，同样适用于一些希望转行到Java的同学。
    
###8.15 脉脉收集面试题
    1.binlog同步延迟怎么解决
    2.rocketmq如何保证消息一定被消费
    3.数据的乐观锁和悲观锁使用场景是什么,分别有啥优缺点
    4.什么是mysql的覆盖索引
    5.线上定位问题的步骤
    6.有个系统怎么去优化
    7.怎么设计一个秒杀系统
    8.怎么解决超卖问题
    9.二分查找的时间复杂度是多少
    10.java层怎么限流
    11.怎么设计一个类似elk的系统
    12.docker有什么优点和缺点,有什么注意的,部署java服务有什么问题没
    13.怎么设计一个orm框架
    
### 8.16 阿里实习生
    
    数据结构的线性表两个常见的实现。
    （为什么不能直接说数组＋链表？我听成了渐进表？卧槽这么高级？）
    你怎么兼顾数组和链表来实现一个？
    我说可以弄hashmap或者双向链表。
    Arraylist扩容的1.5这个数值可以变吗？
    Treemap了解吗？
    说了一段什么非黑即红不拉不拉，原本准备下午就看红黑树的呀！！！！你就打过来了！
    二叉树的平衡算法？
    Java里面的模块你比较熟悉的是哪个？
    我回答了多线程的包。
    多线程的自带线程池有哪些？
    说了四个，说第五个的时候被打断说差不多了。
    ThreadLocal 怎么实现的？
    我说的是内部维护一个ThreadLocalMap 弱引用做key，然后会GC时候被回收等等
    四种引用类型？
    为什么要设计不可控的虚引用？
    ThreadLocalMap关联哪个实体？
    JDK的fork join框架了解吗？（我知道Thread类的join）
    Java里面为什么需要加锁？
    我提到了不是原子操作，问我为什么会不是原子操作？然后我就说了JMM内存模型
    （哎哟卧槽！！！！在写这个的时候突然想到，尼玛难道不是因为指令重排序吗！！！
    真的是太紧张了 这都忘了。。。）
    怎么保证内存可见性？
    Java里面的各种锁,说一下？
    什么叫自旋锁？什么是可重入锁？
    哪种是不可重入锁？
    TCP三次握手四次挥手 详细说一下？
    为什么要四次挥手？
    Hibernate的一级缓存知道吗？答是session，一级缓存
    问内部怎么实现？存在哪里？
    啊？存在。。。内存？
    介绍一下servlet？
    线程安全吗？
    数据库的隔离级别？
    可重复读是什么？
    Mysql 的间隙锁了解吗？
    
    
    
## 9.网络IO/j2ee等基础

###目录
    1.一个Http请求 
        DNS域名解析 –> 发起TCP的三次握手 –> 建立TCP连接后发起http请求 –> 服务器响应http请求，浏览器得到html代码 
        –> 浏览器解析html代码，并请求html代码中的资源（如javascript、css、图片等） –> 浏览器对页面进行渲染呈现给用户
    2.TCP/UDP区别,以及TCP如何保证传输可靠性
                         TCP                  UDP 
            是否连接     面向连接           面向非连接 
            传输可靠性    可靠               不可靠 
            应用场合    传输大量数据          少量数据 
            速度          慢                   快
        可靠性
            在TCP的连接中，数据流必须以正确的顺序送达对方。TCP的可靠性是通过顺序编号和确认（ACK）来实现的。TCP在开始传送一个段时，
            为准备重传而首先将该段插入到发送队列之中，同时启动时钟。其后，如果收到了接受端对该段的ACK信息，就将该段从队列中删去。
            如果在时钟规定的时间内，ACK未返回，那么就从发送队列中再次送出这个段。TCP在协议中就对数据可靠传输做了保障，握手与断开
            都需要通讯双方确认，数据传输也需要双方确认成功，在协议中还规定了：分包、重组、重传等规则；而UDP主要是面向不可靠连接的，
            不能保证数据正确到达目的地。 
            
    3.OSI七层模型?TCP/IP五层模型的协议
    4.http状态码?  (301永久重定向,302临时重定向)
    5.https解决的问题?
        HTTPS=HTTP+加密+认证+完整性保护
        以前是HTTP直接和TCP通信。现在是HTTP先和SSL进行通信，然后SSL和TCP通信。
        先用非对称加密加密对称加密的秘钥,后期直接用对称加密对称加密的性能好.
        
        (共享密钥：加密和解密用同一个密钥。问题：怎样安全的发送密钥？任何人只要拿到密钥，就能破解报文。
        公开密钥和私有密钥：公开密钥可以随意发布，任何人都可以获得；私有密钥不能让任何人知道；使用公开密钥进行加密，使用私有密钥进行解密。
        HTTPS的加密机制，就是上述两种的混合。先使用公开加密的方式传递共享密钥；之后的通信阶段，则采用共享密钥加密方式。之所以这样是因为
        公开加密的处理速度要比共享密钥加密慢。)
        
    6.字符与字节编码?
        字节要按指定的编码解码
            String(byte[] bytes, Charset charset) ,通过使用指定的 charset解码指定的 byte数组，构造一个新的 String。
        字符可以指定编码为字节
            byte[] getBytes(Charset charset) 
                把JVM内存中unicode形式的String按encoding制定的编码，转成字节流 
                使用给定的 charset 将此 String 编码到 byte 序列，并将结果存储到新的 byte 数组。  
        
    7.三次握手,为什么不能两次
        https://www.jianshu.com/p/4d39863e62ab
        SYN：同步序号
        ACK ：确认序号
        seq： (sequence number) 序列号。它是发送端数据包的初始序号。seq=x 表示发送端数据包的初始序号为x
        
        如果是两次握手,client连接请求超时到达server,client已经关闭,server还一直以为连接已经建立,并等待client发送请求,
        浪费服务器资源.
        
        已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，
        以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client
        再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。
        由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送ack包。（校注：此时因为client没有发起建立连接请求，
        所以client处于CLOSED状态，接受到任何包都会丢弃，谢希仁举的例子就是这种场景。但是如果服务器发送对这个延误的旧连接报文的确认的同时，
        客户端调用connect函数发起了连接，就会使客户端进入SYN_SEND状态，当服务器那个对延误旧连接报文的确认传到客户端时，因为客户端已经
        处于SYN_SEND状态，所以就会使客户端进入ESTABLISHED状态，此时服务器端反而丢弃了这个重复的通过connect函数发送的SYN包，见第三个图。
        而连接建立之后，发送包由于SEQ是以被丢弃的SYN包的序号为准，而服务器接收序号是以那个延误旧连接SYN报文序号为准，导致服务器丢弃后续
        发送的数据包）但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”
        的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。
        
    8.四次挥手
        服务端收到客户端关闭命令后,先发送ack,数据传输完成哦,再发送FIN.
    9.为什么连接的时候是三次握手，关闭的时候却是四次握手？
        握手时是服务端同时发送了SYN和ACK,挥手是可能还在传输数据,先发送ACK,再发送FIN
        
        (因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。
        但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，"你发的FIN
        报文我收到了"。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。)
       
    10.为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？
        我们必须假象网络是不可靠的，有可以最后一个ACK丢失。
        所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，
        将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK.
        
    11.如果已经建立了连接，但是客户端突然出现故障了怎么办？
       TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会
       重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75分钟
       发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。
    
    12.HTTP 2.0与HTTP 1.1区别
        HTTP/2采用二进制格式而非文本格式
        HTTP/2是完全多路复用的，而非有序并阻塞的——只需一个连接即可实现并行
        使用报头压缩，HTTP/2降低了开销
        HTTP/2让服务器可以将响应主动“推送”到客户端缓存中
        
    13.HTTP协议结构
        请求报文和响应报文都是由以下4部分组成
        1.请求行
        2.请求头
        3.空行
        4.消息主体
    14.TCP滑动窗口控制流量的原理
        首先是第一次发送数据这个时候的窗口大小是根据链路带宽的大小来决定的。我们假设这个时候窗口的大小是3。这个时候接受方收到数据以后会
        对数据进行确认告诉发送方我下次希望手到的是数据是多少。这里我们看到接收方发送的ACK=3(这是发送方发送序列2的回答确认，下一次接收方
        期望接收到的是3序列信号)。这个时候发送方收到这个数据以后就知道我第一次发送的3个数据对方只收到了2个。就知道第3个数据对方没有收到。
        下次在发送的时候就从第3个数据开始发。这个时候窗口大小就变成了2 。 
        
        看到接收方发送的ACK是5就表示他下一次希望收到的数据是5，发送方就知道我刚才发送的2个数据对方收了这个时候开始发送第5个数据。 
        这就是滑动窗口的工作机制，当链路变好了或者变差了这个窗口还会发生变话，并不是第一次协商好了以后就永远不变了。
        
        滑动窗口协议 
            滑动窗口协议，是TCP使用的一种流量控制方法。该协议允许发送方在停止并等待确认前可以连续发送多个分组。由于发送方不必每发一个分组就停下来等待确认，因此该协议可以加速数据的传输。 
            只有在接收窗口向前滑动时（与此同时也发送了确认），发送窗口才有可能向前滑动。    
            收发两端的窗口按照以上规律不断地向前滑动，因此这种协议又称为滑动窗口协议。    
            当发送窗口和接收窗口的大小都等于1时，就是停止等待协议。
    15.网络拥塞
         拥塞：即对资源的需求超过了可用的资源。若网络中许多资源同时供应不足，网络的性能就要明显变坏，整个网络的吞吐量随之负荷的增大而下降。
        
            拥塞控制：防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。
            拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。
        
            流量控制：指点对点通信量的控制，是端到端正的问题。流量控制所要做的就是抑制发送端发送数据的速率，以便使接收端来得及接收。
        
            拥塞控制代价：需要获得网络内部流量分布的信息。在实施拥塞控制之前，还需要在结点之间交换信息和各种命令，以便选择控制的策略
            和实施控制。这样就产生了额外的开销。拥塞控制还需要将一些资源分配给各个用户单独使用，使得网络资源不能更好地实现共享。
        
        2. 几种拥塞控制方法
            慢开始( slow-start )、拥塞避免( congestion avoidance )、快重传( fast retransmit )和快恢复( fast recovery )。
    16.粘包
        TCP粘包是指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。
        如何处理粘包现象
        　　（1）发送方
        　　对于发送方造成的粘包现象，我们可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭Nagle算法。
        　　（2）接收方
        　　遗憾的是TCP并没有处理接收方粘包现象的机制，我们只能在应用层进行处理。
        　　（3）应用层处理
        　　应用层的处理简单易行！并且不仅可以解决接收方造成的粘包问题，还能解决发送方造成的粘包问题。
        　　解决方法就是循环处理：应用程序在处理从缓存读来的分组时，读完一条数据时，就应该循环读下一条数据，直到所有的数据都被处理；但是如何判断每条数据的长度呢？
        　　两种途径：
        　　　　1）格式化数据：每条数据有固定的格式（开始符、结束符），这种方法简单易行，但选择开始符和结束符的时候一定要注意每条数据的内部一定不能出现开始符或结束符；
        　　　　2）发送长度：发送每条数据的时候，将数据的长度一并发送，比如可以选择每条数据的前4位是数据的长度，应用层处理时可以根据长度来判断每条数据的开始和结束。
        
###结束
    
    位 bit 
    字节 byte 
    字 word 
    
    1字=2字节(1 word = 2 byte) 
    1字节=8位(1 byte = 8bit) 
     
    一个字的字长为16 
    一个字节的字长是8
    
    bps 是 bits per second 的简称。一般数据机及网络通讯的传输速率都是以「bps」为单位。如56Kbps、100.0Mbps 等等。 
    Bps即是Byte per second 的简称。而电脑一般都以Bps 显示速度，如1Mbps 大约等同 128 KBps。 
    bit 电脑记忆体中最小的单位，在二进位电脑系统中，每一bit 可以代表0 或 1 的数位讯号。 
    Byte一个Byte由8 bits 所组成，可代表一个字元(A~Z)、数字(0~9)、或符号(,.?!%&+-*/)，是记忆体储存资料的基本单位，
    至於每个中文字则须要两Bytes。当记忆体容量过大时，位元组这个单位就不够用，因此就有千位元组的单位KB出现，
    以下乃个记忆体计算单位之间的相关性：

    一个Http请求 
        DNS域名解析 –> 发起TCP的三次握手 –> 建立TCP连接后发起http请求 –> 服务器响应http请求，浏览器得到html代码 
        –> 浏览器解析html代码，并请求html代码中的资源（如javascript、css、图片等） –> 浏览器对页面进行渲染呈现给用户
        
        设计存储海量数据的存储系统：设计一个叫“中间层”的一个逻辑层，在这个层，将数据库的海量数据抓出来，做成缓存，运行在服务器的内存中，
        同理，当有新的数据到来，也先做成缓存，再想办法，持久化到数据库中，这是一个简单的思路。主要的步骤是负载均衡，
        将不同用户的请求分发到不同的处理节点上，然后先存入缓存，定时向主数据库更新数据。读写的过程采用类似乐观锁的机制，可以一直读
        （在写数据的时候也可以），但是每次读的时候会有个版本的标记，如果本次读的版本低于缓存的版本，会重新读数据，这样的情况并不多，可以忍受。
    
    TCP/UDP区别以及TCP如何保证传输可靠性
        TCP（Transmission Control Protocol，传输控制协议）是基于连接的协议，也就是说，在正式收发数据前，必须和对方建立可靠的连接。
        一个TCP连接必须要经过三次“对话”才能建立起来，其中的过程非常复杂，我们这里只做简单、形象的介绍，你只要做到能够理解这个过程即可。
        我们来看看这三次对话的简单过程：主机A向主机B发出连接请求数据包：“我想给你发数据，可以吗？”，这是第一次对话；主机B向主机A发送同意
        连接和要求同步（同步就是两台主机一个在发送，一个在接收，协调工作）的数据包：“可以，你什么时候发？”，这是第二次对话；主机A再发出
        一个数据包确认主机B的要求同步：“我现在就发，你接着吧！”，这是第三次对话。三次“对话”的目的是使数据包的发送和接收同步，经过三次
        “对话”之后，主机A才向主机B正式发送数据。 
    
        UDP（User Data Protocol，用户数据报协议）是与TCP相对应的协议。它是面向非连接的协议，它不与对方建立连接，而是直接就把数据包发送过去！ 
         UDP适用于一次只传送少量数据、对可靠性要求不高的应用环境。比如，我们经常使用“ping”命令来测试两台主机之间TCP/IP通信是否正常，
         其实“ping”命令的原理就是向对方主机发送UDP数据包，然后对方主机确认收到数据包，如果数据包是否到达的消息及时反馈回来，那么网络就是通的。
         例如，在默认状态下，一次“ping”操作发送4个数据包。大家可以看到，发送的数据包数量是4包，收到的也是4包（因为对方主机收到后会发回
         一个确认收到的数据包）。这充分说明了UDP协议是面向非连接的协议，没有建立连接的过程。正因为UDP协议没有连接的过程，所以它的通信效果高；
         但也正因为如此，它的可靠性不如TCP协议高。QQ就使用UDP发消息，因此有时会出现收不到消息的情况。 
    
        tcp协议和udp协议的差别 
                         TCP                  UDP 
            是否连接     面向连接           面向非连接 
            传输可靠性    可靠               不可靠 
            应用场合    传输大量数据          少量数据 
            速度          慢                   快
    
    TCP的可靠性如何保证：
        在TCP的连接中，数据流必须以正确的顺序送达对方。TCP的可靠性是通过顺序编号和确认（ACK）来实现的。TCP在开始传送一个段时，
        为准备重传而首先将该段插入到发送队列之中，同时启动时钟。其后，如果收到了接受端对该段的ACK信息，就将该段从队列中删去。
        如果在时钟规定的时间内，ACK未返回，那么就从发送队列中再次送出这个段。TCP在协议中就对数据可靠传输做了保障，握手与断开
        都需要通讯双方确认，数据传输也需要双方确认成功，在协议中还规定了：分包、重组、重传等规则；而UDP主要是面向不可靠连接的，
        不能保证数据正确到达目的地。     
    
    OSI七层模型
    OSI 中的层                         功能                                                                                 TCP/IP协议族 
    应 用层                 文件传输，电子邮件，文件服务，虚拟终 端         TFTP，HTTP，SNMP，FTP，SMTP，DNS，Telnet 
    表示层                 数据格式化，代码转换，数据加密                                    没有协议 
    会话 层                 解除或建立与别的接点的联系                                          没有协议 
    传输层                 提供端对端的接口                                                        TCP，UDP 
    网 络层                 为数据包选择路由                                                        IP，ICMP，RIP，OSPF，BGP，IGMP 
    数据链路层           传输有地址的帧以及错误检测功能                            SLIP，CSLIP，PPP，ARP，RARP，MTU
    物 理层                 以二进制数据形式在物理媒体上传输数据                             ISO2110，IEEE802，IEEE802.2
    
    ***************************************************************************************************************
    
    TCP/IP五层模型的协议
        应用层 
        传输层：四层交换机、也有工作在四层的路由器
        网络层：路由器、三层交换机
        数据链路层：网桥（现已很少使用）、以太网交换机（二层交换机）、网卡（其实网卡是一半工作在物理层、一半工作在数据链路层）
        物理层：中继器、集线器、还有我们通常说的双绞线也工作在物理层
    
    TCP/IP 协议族按层次分别4层
        应用层、 传输层、 网络层、链路层。层与层之间传输数据时，每经过一层会加上或者删除该层所属的首部信息。
        http协议属于应用层；IP协议属于网络层，负责把各种数据送给对方。TCP协议属于传输层，将大块数据分割成报文段进行传输。
        域名解析的DNS服务，属于应用层。
        
    HTTP协议的特点
        无状态：不会对请求和响应之间的通信状态进行保存。
        如果想要保存状态：引入cookie技术
        
        关于持久连接和管线化：
        
        每进行一次HTTP请求，就会有一次TCP连接，通信完成后，就会断开。持久连接就是一次连接，保持连接状态，减少TCP建立和断开的开销。
        connect:keep-alive
        以前，发送下一个请求，必须要等待上一个请求收到响应后，才能发送。现在，有管线化技术，可以同时并行发送多个请求。
    
    状态码
        204 ： no content 响应中没有实体内容返回。
        206 ： partical Content 进行了范围请求，客户端用了Range字段，服务端返回了Content-Range字段
        301 : 永久重定向
        302 : 临时重定向
        服务器同时会返回Location字段，客户端会重新向Location的值发送一次请求。要知道哪些情况下出现301，比如资源URI更改；出现302，比如URL劫持。
        304 : not modified 与重定向没有关系，一般表示客户端资源使用协商缓存。
        401 : 请求需要认证信息，服务端会返回一个字段，来质询用户信息。
        403 : 禁止访问，服务端会做处理，禁止未授权的发送源IP地址请求访问。
        404 : 请求资源不存在。
        503 : 服务器忙。
        
    HTTPS解决的问题
        1 首先，TCP/IP是可能被窃听的网络，报文信息会被窃听，就算是加密，加密内容本身也是能被窃听的。其次，通信者的身份可能被伪装，
            无法确定目标服务器，目标客户端，通信双方的权限。最后，报文完整性无法保证，可能被篡改。
        2 为了解决上述HTTP的缺点，出现了HTTPS。HTTP+加密+认证+完整性保护=HTTPS。
            以前是HTTP直接和TCP通信。现在是HTTP先和SSL进行通信，然后SSL和TCP通信。
            共享密钥：加密和解密用同一个密钥。问题：怎样安全的发送密钥？任何人只要拿到密钥，就能破解报文。
            公开密钥和私有密钥：公开密钥可以随意发布，任何人都可以获得；私有密钥不能让任何人知道；使用公开密钥进行加密，使用私有密钥进行解密。
            HTTPS的加密机制，就是上述两种的混合。先使用公开加密的方式传递共享密钥；之后的通信阶段，则采用共享密钥加密方式。之所以这样是因为
            公开加密的处理速度要比共享密钥加密慢。
    
    TCP和UDP的区别和优缺点
    https://blog.csdn.net/xiaobangkuaipao/article/details/76793702
    
    TCP三次握手与四次挥手过程
    https://blog.csdn.net/qq_35216516/article/details/80554575
    
    TCP 有哪些状态
    https://www.cnblogs.com/qingergege/p/6603488.html
    
    HTTP与HTTPS的区别
    https://www.cnblogs.com/wqhwe/p/5407468.html
    
    nginx
    https://github.com/dunwu/nginx-tutorial
    
    分布式Session框架 
        1. 配置服务器，Zookeeper集群管理服务器可以统一管理所有服务器的配置文件 
        2. 共享这些Session存储在一个分布式缓存中，可以随时写入和读取，而且性能要很好，如Memcache，Tair。 
        3. 封装一个类继承自HttpSession，将Session存入到这个类中然后再存入分布式缓存中 
        4. 由于Cookie不能跨域访问，要实现Session同步，要同步SessionID写到不同域名下。

    String编码和byte 解码总结——字节流和字符流
    https://www.cnblogs.com/JMLiu/p/6478833.html
        1、InputStreamReader 是字节流通向字符流的桥梁：它使用指定的 charset 读取字节并将其解码为字符 
        InputStreamReader(InputStream in, String charsetName) 
        2、OutputStreamWriter 是字符流通向字节流的桥梁：可使用指定的 charset 将要写入流中的字符编码成字节。它使用的字符集可以由名称指定或显式给定，否则将接受平台默认的字符集。 
        OutputStreamWriter(OutputStream out, String charsetName) 
        3、String(byte[] bytes, Charset charset) 
          通过使用指定的 charset解码指定的 byte数组，构造一个新的 String。 
        4、byte[] getBytes(Charset charset) 
          把JVM内存中unicode形式的String按encoding制定的编码，转成字节流 
          使用给定的 charset 将此 String 编码到 byte 序列，并将结果存储到新的 byte 数组。 
        
    【问题1】为什么连接的时候是三次握手，关闭的时候却是四次握手？
        答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。
        但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，"你发的FIN
        报文我收到了"。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。
    
    【问题2】为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？
        答：虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。
        所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，
        将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。
        Client会设置一个计时器，等待2MSL的时间。如果在该时间内再次收到FIN，那么Client会重发ACK并再次等待2MSL。所谓的2MSL是两倍的MSL
        (Maximum Segment Lifetime)。MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，
        Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。
    
    【问题3】如果已经建立了连接，但是客户端突然出现故障了怎么办？
        TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会
        重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75分钟
        发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。
    
    http/https 协议
    
        http
            1.0 协议缺陷:
                无法复用链接，完成即断开，重新慢启动和 TCP 3次握手
                head of line blocking: 线头阻塞，导致请求之间互相影响
        
            1.1 改进:
                长连接(默认 keep-alive)，复用
                host 字段指定对应的虚拟站点
                新增功能:
                    断点续传
                    身份认证
                    状态管理
                    cache 缓存
                        Cache-Control
                        Expires
                        Last-Modified
                        Etag
                        
            2.0:
                多路复用
                二进制分帧层: 应用层和传输层之间
                首部压缩
                服务端推送
            
        https: 较为安全的网络传输协议
            证书(公钥)
            SSL 加密
            端口 443
        
        TCP:
            三次握手
            四次挥手
            滑动窗口: 流量控制
            拥塞处理
                慢开始
                拥塞避免
                快速重传
                快速恢复
        
        缓存策略: 可分为 强缓存 和 协商缓存
        
            Cache-Control/Expires: 浏览器判断缓存是否过期，未过期时，直接使用强缓存，Cache-Control的 max-age 优先级高于 Expires
            
            当缓存已经过期时，使用协商缓存
                唯一标识方案: Etag(response 携带) & If-None-Match(request携带，上一次返回的 Etag): 服务器判断资源是否被修改，
                最后一次修改时间: Last-Modified(response) & If-Modified-Since (request，上一次返回的Last-Modified)
                
                如果一致，则直接返回 304 通知浏览器使用缓存
                如不一致，则服务端返回新的资源
            
            Last-Modified 缺点：
                周期性修改，但内容未变时，会导致缓存失效
                最小粒度只到 s， s 以内的改动无法检测到
            
            Etag 的优先级高于 Last-Modified

    
## 11.java IO NIO

###目录
    1.NIO和IO的主要区别
        1、面向流与面向缓冲
        2、阻塞与非阻塞IO
        3、选择器（Selectors）
        
    2.直接缓冲区与非直接缓冲器有什么区别
        非直接缓冲区：通过allocate()分配缓冲区，将缓冲区建立在JVM的内存中,便于回收
        直接缓冲区：  内核地址空间和用户地址空间之间形成了一个物理内存映射文件，减少了之间的copy过程。
                    存在风险：用户不易控制、GC
        对于NIO来说，缓存可以使用DirectByteBuffer和HeapByteBuffer。如果使用了DirectByteBuffer，一般来说可以减少一次系统空间到
        用户空间的拷贝。但Buffer创建和销毁的成本更高，更不宜维护，一般用来读取大文件时使用。
        
    3.NIO三个核心对象
          通道(Channel)、缓冲区(Buffer)和选择器(Selector)
          Selector轮询是阻塞的，而真正的I/O是异步非阻塞的。
        
    4.Reactor多线程模型
        （1）有专门一个NIO线程-Acceptor线程用于监听服务端，接收客户端的TCP连接请求
        （2）网络IO操作-读、写等由一个NIO线程池负责，线程池可以采用标准的JDK线程池实现，它包含一个任务队列和N个可用的线程，
            由这些NIO线程负责消息的读取、解码、编码和发送。
        （3）一个NIO线程可以同时处理N条链路，但是一个链路只对应一个NIO线程，防止发生并发操作问题。
            由于单独一个Acceptor线程可能会存在性能不中的问题，所以需要主从Reactor模型。
            服务端用于接收客户端的不再是一个单独的NIO线程，而是一个独立的NIO线程池。
             
    5.NIO中， 如果不显式的调用System.gc()那会出现什么问题？
        如果DirectByteBuffer的空间够用，那么System.gc()是不会触发FullGC的。也就是说在空间不够用时，显示调用才能进行回收，
            如果不显式调用，那只能是抛出内存异常了。
        在垃圾收集时，虽然虚拟机会对DirectMemory进行回收，但是DirectMemory却不像新生代和老年代那样，发现空间不足了就通知
            收集器进行垃圾回收，它只能等待老年代满了后FullGC，然后“顺便地”帮它清理掉内存中废弃的对象。否则，只能等到抛出内存
            溢出异常时，在catch块里调用System.gc()。
            
            DirectByteBuffer通过unsafe.allocateMemory申请堆外内存，并在ByteBuffer的address变量中维护指向该内存的地址。
            unsafe.setMemory(base, size, (byte) 0)方法把新申请的内存数据清零。
            
    6.selector一定要非常清楚它的职责和实现原理。
    
    7.其实NIO的核心是IO线程池，一定要记住这个关键点。有的时候，面试官可能也会问你IO包的设计模式（装饰器模式），为什么要这样设计？
       
    8.epoll     
    https://www.cnblogs.com/lojunren/p/3856290.html
        新建epoll描述符==epoll_create()
        epoll_ctrl(epoll描述符，添加或者删除所有待监控的连接)
        返回的活跃连接 ==epoll_wait（ epoll描述符 ）
        
    9.BIO、NIO和AIO的区别
        同步阻塞IO（BIO）
            我们熟知的Socket编程就是BIO，一个socket连接一个处理线程（这个线程负责这个Socket连接的一系列数据传输操作）。
            阻塞的原因在于：操作系统允许的线程数量是有限的，多个socket申请与服务端建立连接时，服务端不能提供相应数量的处理线程，
            没有分配到处理线程的连接就会阻塞等待或被拒绝。
            
        同步非阻塞IO（NIO）
             New IO是对BIO的改进，基于Reactor模型。我们知道，一个socket连接只有在特点时候才会发生数据传输IO操作，大部分时间这个
             “数据通道”是空闲的，但还是占用着线程。NIO作出的改进就是“一个请求一个线程”，在连接到服务端的众多socket中，只有需要进行IO
             操作的才能获取服务端的处理线程进行IO。这样就不会因为线程不够用而限制了socket的接入。客户端的socket连接到服务端时，就会
             在事件分离器注册一个 IO请求事件 和 IO 事件处理器。在该连接发生IO请求时，IO事件处理器就会启动一个线程来处理这个IO请求，
             不断尝试获取系统的IO的使用权限，一旦成功（即：可以进行IO），则通知这个socket进行IO数据传输。
             
        异步阻塞IO（AIO）
           NIO是同步的IO，是因为程序需要IO操作时，必须获得了IO权限后亲自进行IO操作才能进行下一步操作。AIO是对NIO的改进
           （所以AIO又叫NIO.2），它是基于Proactor模型的。每个socket连接在事件分离器注册 IO完成事件 和 IO完成事件处理器。
           程序需要进行IO时，向分离器发出IO请求并把所用的Buffer区域告知分离器，分离器通知操作系统进行IO操作，操作系统自己不断
           尝试获取IO权限并进行IO操作（数据保存在Buffer区），操作完成后通知分离器；分离器检测到 IO完成事件，则激活 IO完成事件
           处理器，处理器会通知程序说“IO已完成”，程序知道后就直接从Buffer区进行数据的读写。
 
           也就是说：AIO是发出IO请求后，由操作系统自己去获取IO权限并进行IO操作；NIO则是发出IO请求后，由线程不断尝试获取IO权限，
           获取到后通知应用程序自己进行IO操作。
           
    10.FileInputStream 在使用完以后，不关闭流，想二次使用可以怎么操作
        反射获取native方法 open0
                int len;
                byte [] by=new byte[8192];
                while ((len=inputStream.read(by))!=-1){
                    outputStream.write(by,0,len);
                }
                if(inputStream.read()==-1){
                    Class in=inputStream.getClass();
                    Method openo= in.getDeclaredMethod("open0", String.class);
                    openo.setAccessible(true);
                    openo.invoke(inputStream,"E:/test/te.txt");
                }
                while ((len=inputStream.read(by))!=-1){
                    outputStreams.write(by,0,len);
                }


###结束

    https://mp.weixin.qq.com/s?__biz=MzU4NDQ4MzU5OA==&mid=2247483956&idx=1&sn=57692bc5b7c2c6dfb812489baadc29c9&chksm=fd985455caefdd4331d828d8e89b22f19b304aa87d6da73c5d8c66fcef16e4c0b448b1a6f791&scene=21#wechat_redirect
    
    比较完整
    https://github.com/Snailclimb/JavaGuide/blob/master/Java%E7%9B%B8%E5%85%B3/Java%20IO%E4%B8%8ENIO.md#%E4%B8%80-java-io%EF%BC%8C%E7%A1%AC%E9%AA%A8%E5%A4%B4%E4%B9%9F%E8%83%BD%E5%8F%98%E8%BD%AF
    代码demo
    https://github.com/wanwanpp/netty-demo
    
    https://www.cnblogs.com/aspirant/p/8630283.html (推荐)
    *NIO和IO的主要区别
        1、面向流与面向缓冲
             Java IO和NIO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，
             直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将
             它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这
             就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，
             不要覆盖缓冲区里尚未处理的数据。
             
        2、阻塞与非阻塞IO
             Java IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。
             该线程在此期间不能再干任何事情了。Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，
             如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。
              非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非
              阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。
        
        3、选择器（Selectors）
             Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”
             通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。
          
    NIO和IO如何影响应用程序的设计
        1、API调用
            当然，使用NIO的API调用时看起来与使用IO时有所不同，但这并不意外，因为并不是仅从一个InputStream逐字节读取，
            而是数据必须先读入缓冲区再处理。   
        2、数据处理
            使用纯粹的NIO设计相较IO设计，数据处理也受到影响。
            
            BIO,在IO设计中，我们从InputStream或 Reader逐字节读取数据。假设你正在处理一基于行的文本数据流，例如：
                Name: Anna 
                Age: 25
                Email: anna@mailserver.com 
                Phone: 1234567890 
                
                InputStream input = ... ; // get the InputStream from the client socket   
                BufferedReader reader = new BufferedReader(new InputStreamReader(input));   
                String nameLine   = reader.readLine(); 
                String ageLine    = reader.readLine(); 
                String emailLine  = reader.readLine(); 
                String phoneLine  = reader.readLine(); 
                 正如你可以看到，该处理程序仅在有新数据读入时运行，并知道每步的数据是什么。一旦正在运行的线程已处理过读入的某些数据，
                 该线程不会再回退数据（大多如此
                
            NIO,而一个NIO的实现会有所不同，下面是一个简单的例子：
                ByteBuffer buffer = ByteBuffer.allocate(48); 
                int bytesRead = inChannel.read(buffer); 
                while(! bufferFull(bytesRead) ) {   
                       bytesRead = inChannel.read(buffer);   
                } 
    
    总结
        NIO可让您只使用一个（或几个）单线程管理多个通道（网络连接或文件），但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂。

    直接缓冲区与非直接缓冲器有什么区别 
        物理磁盘->内核地址空间->用户地址空间->应用程序 
        OS -> JVM
        
        直接缓冲区：
            内核地址空间和用户地址空间之间形成了一个物理内存映射文件，减少了之间的copy过程。
            存在风险：用户不易控制、GC
        
        非直接缓冲区：通过allocate()分配缓冲区，将缓冲区建立在JVM的内存中 
            
    https://www.cnblogs.com/mazhimazhi/p/9632545.html   (必读,里面有demo)
    异步写非阻塞的时候,数据没写完,咋个办
    
    NIO三个核心对象
        通道(Channel)、缓冲区(Buffer)和选择器(Selector)
        具体说就是Selector会不断轮询注册在其上的Channel，如果某个Channel上有新的TCP连接，读或者写事件，这个Channel就处于就绪状态，
        会被Selector轮询出来，然后通过SelectorKey可以获取就绪Channel的集合，进行后续I/O操作。
    
    *Java Reactor模式 异步非阻塞IO Reactor多线程模型
        （1）有专门一个NIO线程-Acceptor线程用于监听服务端，接收客户端的TCP连接请求
        （2）网络IO操作-读、写等由一个NIO线程池负责，线程池可以采用标准的JDK线程池实现，它包含一个任务队列和N个可用的线程，
            由这些NIO线程负责消息的读取、解码、编码和发送。
        （3）一个NIO线程可以同时处理N条链路，但是一个链路只对应一个NIO线程，防止发生并发操作问题。
            由于单独一个Acceptor线程可能会存在性能不中的问题，所以需要主从Reactor模型。
            服务端用于接收客户端的不再是一个单独的NIO线程，而是一个独立的NIO线程池。 
    
    NIO中， 如果不显式的调用System.gc()那会出现什么问题？
        DirectBuffer是分配在操作系统的内存中的，所以省去了应用程序到内核空间的拷贝，而HeapBuffer是分配到堆上的，所以便于垃圾回收。
        DirectBuffer的GC规则与堆对象的回收规则是一样的，只有垃圾对象才会被回收，而判定是否为垃圾对象依然是根据引用树中的存活节点来判定。
        如果DirectByteBuffer的空间够用，那么System.gc()是不会触发FullGC的。也就是说在空间不够用时，显示调用才能进行回收，
            如果不显式调用，那只能是抛出内存异常了。
        在垃圾收集时，虽然虚拟机会对DirectMemory进行回收，但是DirectMemory却不像新生代和老年代那样，发现空间不足了就通知收集器
            进行垃圾回收，它只能等待老年代满了后FullGC，然后“顺便地”帮它清理掉内存中废弃的对象。否则，只能等到抛出内存溢出异常时，
            在catch块里调用System.gc()。
        
    channel
        NIO把它支持的I/O对象抽象为Channel，Channel又称“通道”，类似于原I/O中的流（Stream），但有所区别：
        1、流是单向的，通道是双向的，可读可写。
        2、流读写是阻塞的，通道可以异步读写。
        3、流中的数据可以选择性的先读到缓存中，通道的数据总是要先读到一个缓存中，或从缓存中写入
        
    Selector
        如果一个 Channel 要注册到 Selector 中, 那么这个 Channel 必须是非阻塞的, 即channel.configureBlocking(false);
        因为 Channel 必须要是非阻塞的, 因此 FileChannel 是不能够使用选择器的, 因为 FileChannel 都是阻塞的.
        
        在使用 Channel.register()方法时, 第二个参数指定了我们对 Channel 的什么类型的事件感兴趣
            Connect, 即连接事件(TCP 连接), 对应于SelectionKey.OP_CONNECT
            Accept, 即确认事件, 对应于SelectionKey.OP_ACCEPT
            Read, 即读事件, 对应于SelectionKey.OP_READ, 表示 buffer 可读.
            Write, 即写事件, 对应于SelectionKey.OP_WRITE, 表示 buffer 可写.
        
        Selector 的基本使用流程
            1.通过 Selector.open() 打开一个 Selector.
            2.将 Channel 注册到 Selector 中, 并设置需要监听的事件(interest set)
            3.不断重复:
                调用 select() 方法
                调用 selector.selectedKeys() 获取 selected keys
                迭代每个 selected key:
                    *从 selected key 中获取 对应的 Channel 和附加信息(如果有的话)
                    *判断是哪些 IO 事件已经就绪了, 然后处理它们. 如果是 OP_ACCEPT 事件, 则调用 "SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept()" 获取 SocketChannel, 并将它设置为 非阻塞的, 然后将这个 Channel 注册到 Selector 中.
                    *根据需要更改 selected key 的监听事件.
                    *将已经处理过的 key 从 selected keys 集合中删除.
                    
        
    
## 12.spring

###目录
    1.bean生命周期
        ApplicationContext容器中，Bean的生命周期流程如上图所示，流程大致如下：
            1.首先容器启动后，会对scope为singleton且非懒加载的bean进行实例化，
            2.按照Bean定义信息配置信息，注入所有的属性，
            3.如果Bean实现了BeanNameAware接口，会回调该接口的setBeanName()方法，传入该Bean的id，此时该Bean就获得了自己在配置文件中的id，
            4.如果Bean实现了BeanFactoryAware接口,会回调该接口的setBeanFactory()方法，传入该Bean的BeanFactory，这样该Bean就获得了自己所在的BeanFactory，
            5.如果Bean实现了ApplicationContextAware接口,会回调该接口的setApplicationContext()方法，传入该Bean的ApplicationContext，这样该Bean就获得了自己所在的ApplicationContext，
            6.如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessBeforeInitialzation()方法，
            7.如果Bean实现了InitializingBean接口，则会回调该接口的afterPropertiesSet()方法，
            8.如果Bean配置了init-method方法，则会执行init-method配置的方法，
            9.如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessAfterInitialization()方法，
            10.经过流程9之后，就可以正式使用该Bean了,对于scope为singleton的Bean,Spring的ioc容器中会缓存一份该bean的实例，而对于scope为prototype的Bean,每次被调用都会new一个新的对象，期生命周期就交给调用方管理了，不再是Spring容器进行管理了
            11.容器关闭后，如果Bean实现了DisposableBean接口，则会回调该接口的destroy()方法，
            12.如果Bean配置了destroy-method方法，则会执行destroy-method配置的方法，至此，整个Bean的生命周期结束
    
    2.Spring的BeanFactory和ApplicationContext的区别?
            ApplicationContext是BeanFactory的子类
            1.由于ApplicationContext扩展了MessageResource接口，因而具有消息处理的能力(i18N)，具体spring如何使用国际化
            2.即当ApplicationContext中发布一个事件的时，所有扩展了ApplicationListener的Bean都将会接受到这个事件，并进行相应的处理。  
            3.ApplicationContext扩展了ResourceLoader(资源加载器)接口，从而可以用来加载多个Resource，而BeanFactory是没有扩展ResourceLoader  
            4.ApplicationContext能以声明的方式创建，如使用ContextLoader。当然你也可以使用ApplicationContext的实现之一来以编程的方式创建ApplicationContext实例 。  
            5.其它区别  
              1).BeanFactroy采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化，这样，我们就不能发现一些存在的Spring的配置问题。而ApplicationContext则相反，它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误。  
              2).BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册
  
    3.spring事件
        1.ContextRefreshedEvent
        2.ContextStartedEvent
        3.ContextStoppedEvent
        4.ContextClosedEvent
        
    4.Bean 的作用域
          singleton	该作用域将 bean 的定义的限制在每一个 Spring IoC 容器中的一个单一实例(默认)。
          prototype	该作用域将单一 bean 的定义限制在任意数量的对象实例。
          request	该作用域将 bean 的定义限制为 HTTP 请求。只在 web-aware Spring ApplicationContext 的上下文中有效。
          session	该作用域将 bean 的定义限制为 HTTP 会话。 只在web-aware Spring ApplicationContext的上下文中有效。
          global-session	该作用域将 bean 的定义限制为全局 HTTP 会话。只在 web-aware Spring ApplicationContext 的上下文中有效。
        
    5.动态代理
        定义：给目标对象提供一个代理对象，并由代理对象控制对目标对象的引用
        为什么要用代理
            最最最主要的原因就是，在不改变目标对象方法的情况下对方法进行增强，比如，我们希望对方法的调用增加日志记录，鉴权
            或者对方法的调用进行拦截，等等...
            
        既然有了静态代理，为什么需要动态代理呢
            接口与代理类是1对1的，有多个接口需要代理，就需要新建多个代理类，繁琐，类爆炸。
            
        关于Jdk代理和Cglib代理，其优缺点主要在于：
            Jdk代理生成的代理类只有一个，因而其编译速度是非常快的；而由于被代理的目标类是动态传入代理类中的，Jdk代理的执行效率相对
            来说低一点，这也是Jdk代理被称为动态代理的原因；
            Cglib代理需要为每个目标类生成相应的子类，因而在实际运行过程中，其可能会生成非常多的子类，过多的子类始终不是太好的，因为
            这影响了虚拟机编译类的效率；但由于在调用过程中，代理类的方法是已经静态编译生成了的，因而Cglib代理的执行效率相对来说高一些。
        jdk动态代理
            1.加强逻辑实现InvocationHandler接口,逻辑写在
                public Object invoke(Object proxy, Method method, Object[] args)里面,持有被代理的对象
                调用被代理对象方法method.invoke(target,args);
            2.创建代理对象,传入classLoader,被代理的接口class,invocationHandler
                DoSomething proxy = (DoSomething)Proxy.newProxyInstance(myDoSomething.getClass().getClassLoader(),
                				new Class[]{DoSomething.class,Test2.class},
                				invocationHandler);
                		proxy.doSomething1();
                		
        cglib
            1.加强逻辑实现MethodInterceptor接口,逻辑写在
                public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy)里面
                还可以实现CallbackFilter接口,过滤加强的方法
                methodProxy.invokeSuper(o, objects);
            2.创建Enhancer,设置
                    enhancer.setSuperclass(this.target.getClass());
                    // 回调方法
                    enhancer.setCallback(this);
                    // 创建代理
                    return enhancer.create();
                    
        *为什么jdk动态代理必须实现接口?
            可以猜测到接口创建的新类proxyClassFile 不管采用什么接口，都是以下结构
            public class $Proxy1 extends Proxy implements 传入的接口{
            }
            
            生成新类的看不到源代码，不过猜测它的执行原理很有可能是如果类是Proxy的子类，则调用InvocationHandler进行方法的Invoke
            到现在大家都应该明白了吧，JDK动态代理的原理是根据定义好的规则，用传入的接口创建一个新类，这就是为什么采用动态代理时为什么
            只能用接口引用指向代理，而不能用传入的类引用执行动态类。
            即代理类要实现被代理类实现的接口
            
            cglib采用的是用创建一个继承实现类的子类，用asm库动态修改子类的代码来实现的，所以可以用传入的类引用执行代理类
    
    6.spring包括哪些模块
        1.核心容器（Core Container）
            核心容器包括spring-core，spring-beans，spring-context，spring-context-support和spring-expression（SpEL，Spring表达式语言，Spring Expression Language）等模块。
            spring-core和spring-beans模块是Spring框架的基础，包括控制反转和依赖注入等功能。BeanFactory是工厂模式的微妙实现，它移除了编码式单例的需要，并且可以把配置和依赖从实际编码逻辑中解耦。
            Context（spring-context）模块是在Core和Bean模块的基础上建立起来的，它以一种类似于JNDI注册的方式访问对象。Context模块继承自Bean模块，并且添加了国际化（比如，使用资源束）、事件传播、资源加载和透明地创建上下文（比如，通过Servelet容器）等功能。Context模块也支持Java EE的功能，比如EJB、JMX和远程调用等。ApplicationContext接口是Context模块的焦点。spring-context-support提供了对第三方库集成到Spring上下文的支持，比如缓存（EhCache, Guava, JCache）、邮件（JavaMail）、调度（CommonJ, Quartz）、模板引擎（FreeMarker, JasperReports, Velocity）等。
            spring-expression模块提供了强大的表达式语言用于在运行时查询和操作对象图。它是JSP2.1规范中定义的统一表达式语言的扩展，支持set和get属性值、属性赋值、方法调用、访问数组集合及索引的内容、逻辑算术运算、命名变量、通过名字从Spring IoC容器检索对象，还支持列表的投影、选择以及聚合等。
        2.AOP和检测（Instrumentation）
            spring-aop模块提供了面向切面编程（AOP）的实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。使用源码级的元数据，可以用类似于.Net属性的方式合并行为信息到代码中。
            spring-aspects模块提供了对AspectJ的集成。
            spring-instrument模块提供了对检测类的支持和用于特定的应用服务器的类加载器的实现。spring-instrument-tomcat模块包含了用于tomcat的Spring检测代理。
        3.消息处理（messaging）
            Spring 4 包含的spring-messaging模块是从Spring集成项目的关键抽象中提取出来的，这些项目包括Message、MessageChannel、MessageHandler和其它服务于消息处理的项目。这个模块也包含一系列的注解用于映射消息到方法，这类似于Spring MVC基于编码模型的注解。
        4.数据访问与集成
            数据访问与集成层包含JDBC、ORM、OXM、JMS和事务模块。 
            （译者注：JDBC=Java Data Base Connectivity，ORM=Object Relational Mapping，OXM=Object XML Mapping，JMS=Java Message Service）
            spring-jdbc模块提供了JDBC抽象层，它消除了冗长的JDBC编码和对数据库供应商特定错误代码的解析。
            spring-tx模块支持编程式事务和声明式事务，可用于实现了特定接口的类和所有的POJO对象。 
            （译者注：编程式事务需要自己写beginTransaction()、commit()、rollback()等事务管理方法，声明式事务是通过注解或配置由spring自动处理，编程式事务粒度更细）
            spring-orm模块提供了对流行的对象关系映射API的集成，包括JPA、JDO和Hibernate等。通过此模块可以让这些ORM框架和spring的其它功能整合，比如前面提及的事务管理。
            spring-oxm模块提供了对OXM实现的支持，比如JAXB、Castor、XML Beans、JiBX、XStream等。
            spring-jms模块包含生产（produce）和消费（consume）消息的功能。从Spring 4.1开始，集成了spring-messaging模块。
        5.Web
            Web层包括spring-web、spring-webmvc、spring-websocket、spring-webmvc-portlet等模块。
            spring-web模块提供面向web的基本功能和面向web的应用上下文，比如多部分（multipart）文件上传功能、使用Servlet监听器初始化IoC容器等。它还包括HTTP客户端以及Spring远程调用中与web相关的部分。
            spring-webmvc模块（即Web-Servlet模块）为web应用提供了模型视图控制（MVC）和REST Web服务的实现。Spring的MVC框架可以使领域模型代码和web表单完全地分离，且可以与Spring框架的其它所有功能进行集成。
            spring-webmvc-portlet模块（即Web-Portlet模块）提供了用于Portlet环境的MVC实现，并反映了spring-webmvc模块的功能。
        6.Test
            spring-test模块通过JUnit和TestNG组件支持单元测试和集成测试。它提供了一致性地加载和缓存Spring上下文，也提供了用于单独测试代码的模拟对象（mock object）。                    
    
    7.springMVC流程
        流程说明（重要）：
        （1）客户端（浏览器）发送请求，直接请求到 DispatcherServlet。
        （2）DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。
        （3）解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。
        （4）HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑。
        （5）处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。
        （6）ViewResolver 会根据逻辑 View 查找实际的 View。
        （7）DispaterServlet 把返回的 Model 传给 View（视图渲染）。
        （8）把 View 返回给请求者（浏览器）
        
    8.Spirng IoC容器的初始化过程
          https://www.cnblogs.com/chenjunjie12321/p/6124649.html
          分为三步：
          
          1.Resource定位（Bean的定义文件定位）
          2.将Resource定位好的资源载入到BeanDefinition
          3.将BeanDefiniton注册到容器中
          
          管理bean的生命周期
          
          总结：IoC容器的初始化过程就是将xml配置资源的信息抽象到BeanDefinition信息对象中，再将BeanDefinition设置到基本容器的map中，
          BeanDefinition中的信息是容器建立依赖反转的基础，IoC容器的作用就是对这些信息进行处理和维护。
          
          refresh()方法主要为IoC容器Bean的生命周期管理提供条件，Spring IoC容器载入Bean定义资源文件从其子类容器的refreshBeanFactory()
          方法启动，所以整个refresh()中“ConfigurableListableBeanFactory beanFactory =obtainFreshBeanFactory();”这句以后代码
          的都是注册容器的信息源和生命周期事件，载入过程就是从这句代码启动。
          
           refresh()方法的作用是：在创建IoC容器前，如果已经有容器存在，则需要把已有的容器销毁和关闭，以保证在refresh之后使用的是新建立
          起来的IoC容器。refresh的作用类似于对IoC容器的重启，在新建立好的容器中对容器进行初始化，对Bean定义资源进行载入
    9.@autowiring实现过程
        a.    对Bean的属性迭代调用getBean方法，完成依赖Bean的初始化和依赖注入。
        b.    将依赖Bean的属性引用设置到被依赖的Bean属性上。
        c.     将依赖Bean的名称和被依赖Bean的名称存储在IoC容器的集合中。
        
    10.说说Spring事务什么情况下才会回滚
        当所拦截的方法有指定异常抛出，事务才会自动进行回滚。默认情况下是捕获到方法的RuntimeException异常，也就是说抛出只要属于运行时
        的异常（即RuntimeException及其子类）都能回滚；但当抛出一个不属于运行时异常时，事务是不会回滚的。如果是其他异常想要实现回滚，
        可以进行配置。
    
    11.说说Spring事务的传播属性
        *同一个类种的事务,要么都声明要事务，要么分开成两个类，要么直接在方法里使用编程式事务，那么一切OK。被同一个类种的方法调用的方法上加事务会失效
        
       事务的传播性一般在事务嵌套时候使用，比如在事务A里面调用了另外一个使用事务的方法，那么这俩个事务是各自作为独立的事务执行提交，
       还是内层的事务合并到外层的事务一块提交那，这就是事务传播性要确定的问题。spring支持7种事务传播行为：
       
       *(这个是默认)PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。
       PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。
       PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。
       
       *(这个防止死锁)PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。新建的事务将和被挂起的事务没有任何关系，
       是两个独立的事务，外层事务失败回滚之后，不能回滚内层事务执行的结果，内层事务失败抛出异常，外层事务捕获，也可以不处理回滚操作
       
       PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。
       PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。
       PROPAGATION_NESTED – 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。
       备注：常用的两个事务传播属性是1和4，即PROPAGATION_REQUIRED，PROPAGATION_REQUIRES_NEW
       
    *12.说说Spring事务的隔离性
       事务的隔离性是指多个事务并发执行的时候相互之间不受到彼此的干扰。
       
       事务的隔离级别也分为四种，由低到高依次分别为：read uncommited（读未提交）、read commited（读提交）、read repeatable（读重复）、
       serializable（序列化），这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。
       
       1.脏读
            A事务读取B事务尚未提交的数据，此时如果B事务发生错误并执行回滚操作，那么A事务读取到的数据就是脏数据。
       2.不可重复读（前后多次读取，数据内容不一致）
            事务A在执行读取操作，由整个事务A比较大，前后读取同一条数据需要经历很长的时间 。而在事务A第一次读取数据，比如此时读取了
            小明的年龄为20岁，事务B执行更改操作，将小明的年龄更改为30岁，此时事务A第二次读取到小明的年龄时，发现其年龄是30岁，和之前
            的数据不一样了，也就是数据不重复了，系统不可以读取到重复的数据，成为不可重复读。
            通过加读写锁解决
       3.幻读（前后多次读取，数据总量不一致）
            事务A在执行读取操作，需要两次统计数据的总量，前一次查询数据总量后，此时事务B执行了新增数据的操作并提交后，这个时候事务A
            读取的数据总量和之前统计的不一样，就像产生了幻觉一样，平白无故的多了几条数据，成为幻读。
       
       1.read uncommited：是最低的事务隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。(脏读问题,读取到未提交的数据)
       2.read commited：保证一个事物提交后才能被另外一个事务读取。另外一个事务不能读取该事物未提交的数据。()
       3.repeatable read：这种事务隔离级别可以防止脏读，不可重复读。但是可能会出现幻象读。它除了保证一个事务不能被另外一个事务读取未
        提交的数据之外还避免了以下情况产生（不可重复读）。
       4.serializable：这是花费最高代价但最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读之外，还避免了幻读。
       5.DEFAULT 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别.(mysql默认 repeatalbe read)

    13.说说Spring事务的特性
       事务特性分为四个：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持续性（Durability）简称ACID。
       
       原子性（Atomicity）:事务是数据库逻辑工作单元，事务中包含的操作要么都执行成功，要么都执行失败。
       一致性（Consistency）：事务执行的结果必须是使数据库数据从一个一致性状态变到另外一种一致性状态。当事务执行成功后就说数据库处于一致性状态。如果在执行过程中发生错误，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这是数据库就处于不一致状态。
       隔离性（Isolation）：一个事务的执行过程中不能影响到其他事务的执行，即一个事务内部的操作及使用的数据对其他事务是隔离的，并发执行各个事务之间无不干扰。
       持久性（Durability）：即一个事务执一旦提交，它对数据库数据的改变是永久性的。之后的其它操作不应该对其执行结果有任何影响。
              
    14.IOC（依赖注入）方式
       构造器依赖注入：构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖。
       Setter方法注入：Setter方法注入是容器通过调用无参构造器或无参static工厂方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。
       用构造器参数实现强制依赖，setter方法实现可选依赖。
       
    15.Springboot启动过程
    https://blog.csdn.net/u010811939/article/details/80592461
       构造SpringApplication的实例
       调用SpringApplication.run()方法
           构造SpringApplicationRunListeners 实例
           发布ApplicationStartedEvent事件
           SpringApplicationRunListeners 实例准备环境信息
           创建ApplicationContext对象
           ApplicationContext实例准备环境信息
           刷新的上下文
                ⑴.准备刷新的上下文环境 
                ⑵.初始化BeanFactory 
                ⑶.对BeanFactory进行各种功能填充 
                ⑷.子类覆盖方法做额外的处理 
                ⑸.激活各种BeanFactory处理器 
                ⑹.注册拦截Bean创建的Bean处理，这里只是注册，真正调用是再拿去Bean的时候 
                ⑺.为上下文初始化Message源，即不同语言的消息体，国际化处理 
                ⑻.初始化应用消息广播器，并放到applicationEventMulticaster bean中 
                ⑼.留给子类来初始化其他bean 
                ⑽.在所有注册的bean中查找Listener bean，注册到消息广播中 
                ⑾.初始化剩下的单实例（非惰性） 
                ⑿.完成刷新过程，通知生命周期处理器lifecycleProcessor刷新过程，同时发出ContextRefreshEvent通知别人

    16.springboot常用注解
        @SpringBootApplication = (默认属性)@Configuration + @EnableAutoConfiguration + @ComponentScan。
        @Configuration：提到@Configuration就要提到他的搭档@Bean。使用这两个注解就可以创建一个简单的spring配置类，可以用来替代相应的xml配置文件。
        @EnableAutoConfiguration：能够自动配置spring的上下文，试图猜测和配置你想要的bean类，通常会自动根据你的类路径和你的bean定义自动配置
        @AutoConfigureAfter：在指定的配置类初始化后再加载 
        @AutoConfigureBefore：在指定的配置类初始化前加载 
        @AutoConfigureOrder：数越小越先初始化
        @ComponentScan：会自动扫描指定包下的全部标有@Component的类，并注册成bean，当然包括@Component下的子注解@Service,@Repository,@Controller。
        @Entity 对实体注释。任何Hibernate映射对象都要有这个注释
        @Table声明此对象映射到数据库的数据表，通过它可以为实体指定表(table),目录(Catalog)和schema的名字。该注释不是必须的，如果没有则系统使用默认值(实体的短类名)。
        @EnableScheduling开启计划任务
            1@Scheduled:声明该方法是一个计划任务，fixedRate属性设置每隔固定时间执行
            2cron属性：按照指定时间执行(11时28)cron事unix和类unix(linux)系统下的定时时间
        @WebServlet，在 SpringBootApplication 上使用@ServletComponentScan 注解后，Servlet、Filter、Listener 可以直接通过 @WebServlet、@WebFilter、@WebListener 注解自动注册，无需其他代码。
        @Transactional
            解决Transactional注解不回滚:
            1. 检查你方法是不是public的。
            2. 你的异常类型是不是unchecked异常。@Transactional(rollbackFor=Exception.class)
            类似的还有norollbackFor，自定义不回滚的异常。
            3. 数据库引擎要支持事务，如果是mysql，注意表要使用支持事务的引擎，比如innodb，如果是myisam，事务是不起作用的。
            4. 是否开启了对注解的解析
            <tx:annotation-driven transaction-manager="transactionManager" proxy-target class="true"/>
            5. spring是否扫描到你这个包，如下是扫描到org.test下面的包<context:component-scan base-package="org.test" ></context:component-scan>
        @Controller 用来响应页面，@Controller必须配合模版来使用。
        @RestController Spring4之后新加入的注解，原来返回json需要@ResponseBody和@Controller配合。
        @RequestBody 将接收的json数据装载到实体类中。
        @RequestMapping 配置url映射
        @PathVaribale 获取url中的数据
        @RequestParam 获取请求参数的值
        @GetMapping 组合注解
        @PostConstruct与@PreDestroy详解及实例
            @PostConstruct注释的方法，会在构造方法之后，init方法之前进行调用。
            只有一个方法可以使用此注释进行注解；
            被注解方法不得有任何参数；
            被注解方法返回值为void；
            被注解方法不得抛出已检查异常；
            被注解方法需是非静态方法；
            此方法只会被执行一次；
            PreDestroy 注释作为回调通知用于各方法，以表示该实例正处于被容器移除的过程中。用 PreDestroy 注释的方法通常用于释放它已持有的资源。
        @PropertySource 注解可以从properties文件中，获取对应的key-value值，将其赋予变量；
        @ConfigurationProperties的大致作用就是通过它可以把properties或者yml配置直接转成对象。
        @RunWith：这个是指定使用的单元测试执行类，这里就指定的是SpringJUnit4ClassRunner.class；
        @webservice:
            类上加上@WebService注解
            @WebMethod注解声明了这个方法是要发布出去的方法
        @Resource 默认是按照名称来装配注入的，只有当找不到与名称匹配的bean才会按照类型来注入。它有两个属性是比较重要的:
            ①. name: Spring 将 name 的属性值解析为 bean 的名称， 使用 byName 的自动注入策略
            ②. type: Spring 将 type的属性值解析为 bean 的类型，使用 byType 的自动注入策略
            注: 如果既不指定 name 属性又不指定 type 属性，Spring这时通过反射机制使用 byName 自动注入策略
            @Resource 的装配顺序
            　　1. 如果同时指定了 name 属性和 type 属性，那么 Spring 将从容器中找唯一匹配的 bean 进行装配，找不到则抛出异常
                2. 如果指定了 name 属性值，则从容器中查找名称匹配的 bean 进行装配，找不到则抛出异常
                3. 如果指定了 type 属性值，则从容器中查找类型匹配的唯一的 bean 进行装配，找不到或者找到多个都会抛出异常
                4. 如果都不指定，则会自动按照 byName 方式进行装配， 如果没有匹配，则回退一个原始类型进行匹配，如果匹配则自动装配
            @Autowried 默认是按照类型进行装配注入，默认情况下，它要求依赖对象必须存在，如果允许 null 值，可以设置它 required 为false。
            　　如果我们想要按名称进行装配的话，可以添加一个 @Qualifier 注解解决。
            
            demo
            @Autowried
            @Qualifier("adminDAO")
            private AdminDAO adminDAO;
        @Async和@EnableAsync为异步注解，放到方法上，表示调用该方法的线程与此方法异步执行，需要配合注解使用。@EnableAsync放在启动类上，Async放在方法上。
        @JsonInclude(Include.NON_NULL) 是springmvc中的标注，是为了控制返回的json字符串显示哪些字段。这里的设置是为null的字段不显示
        @ConfigurationProperties(prefix = "sso") 可以在实体类中将properties文件的值注入到属性中。
    
      
        
###结束

    *bean生命周期
    https://www.jianshu.com/p/3944792a5fff
        ApplicationContext容器中，Bean的生命周期流程如上图所示，流程大致如下：
        1.首先容器启动后，会对scope为singleton且非懒加载的bean进行实例化，
        2.按照Bean定义信息配置信息，注入所有的属性，
        3.如果Bean实现了BeanNameAware接口，会回调该接口的setBeanName()方法，传入该Bean的id，此时该Bean就获得了自己在配置文件中的id，
        4.如果Bean实现了BeanFactoryAware接口,会回调该接口的setBeanFactory()方法，传入该Bean的BeanFactory，这样该Bean就获得了自己所在的BeanFactory，
        5.如果Bean实现了ApplicationContextAware接口,会回调该接口的setApplicationContext()方法，传入该Bean的ApplicationContext，这样该Bean就获得了自己所在的ApplicationContext，
        6.如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessBeforeInitialzation()方法，
        7.如果Bean实现了InitializingBean接口，则会回调该接口的afterPropertiesSet()方法，
        8.如果Bean配置了init-method方法，则会执行init-method配置的方法，
        9.如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessAfterInitialization()方法，
        10.经过流程9之后，就可以正式使用该Bean了,对于scope为singleton的Bean,Spring的ioc容器中会缓存一份该bean的实例，而对于scope为prototype的Bean,每次被调用都会new一个新的对象，期生命周期就交给调用方管理了，不再是Spring容器进行管理了
        11.容器关闭后，如果Bean实现了DisposableBean接口，则会回调该接口的destroy()方法，
        12.如果Bean配置了destroy-method方法，则会执行destroy-method配置的方法，至此，整个Bean的生命周期结束
        
    Spring的BeanFactory和ApplicationContext的区别?
        https://my.oschina.net/yao00jun/blog/215642
        https://blog.csdn.net/qq_36748278/article/details/78264764
        
    Spring IOC 怎么注入类，怎么实例化对象 实例化
        Spring IoC容器则需要根据Bean定义里的配置元数据使用反射机制来创建Bean
        使用构造器实例化Bean 有参/无参;使用静态工厂实例化Bean;使用实例工厂实例化Bean.
        使用@Autowire注解注入的时机则是容器刚启动的时候就开始注入；注入之前要先初始化bean；ApplicationContext 的初始化和BeanFactory 有一个重大的区别：BeanFactory在初始化容器时，并未实例化Bean，直到第一次访问某个Bean 时才实例目标Bean；而ApplicationContext 则在初始化应用上下文时就实例化所有单实例的Bean。
        注入
        
        接口、setter、构造器
    
            
    对于作用域为 prototype 的 bean ，其destroy方法并没有被调用。如果 bean 的 scope 设为prototype时，当容器关闭时，
    destroy 方法不会被调用。对于 prototype 作用域的 bean，有一点非常重要，
    那就是 Spring不能对一个 prototype bean 的整个生命周期负责：容器在初始化、配置、装饰或者是装配完一个prototype实例后，
    将它交给客户端，随后就对该prototype实例不闻不问了

    spring 官方文档中文版
    https://blog.csdn.net/tangtong1/article/details/51326887

    依赖注入（DI）和控制反转（IoC）
        你可以把不相干组件组合在一起，从而组成一个完整的可以使用的应用。Spring根据设计模式编码出了非常优秀的代码，
        所以可以直接集成到自己的应用中。因此，大量的组织机构都使用Spring来保证应用程序的健壮性和可维护性。
    
    Spring支持三种依赖注入方式，
        分别是属性（Setter方法）注入，构造注入和接口注入。
    
    在Spring中，那些组成应用的主体及由Spring IOC容器所管理的对象被称之为Bean。
    
    Spring的IOC容器通过反射的机制实例化Bean并建立Bean之间的依赖关系。 
    简单地讲，Bean就是由Spring IOC容器初始化、装配及被管理的对象。 
    获取Bean对象的过程，首先通过Resource加载配置文件并启动IOC容器，然后通过getBean方法获取bean对象，就可以调用他的方法。 
    Spring Bean的作用域： 
    Singleton：Spring IOC容器中只有一个共享的Bean实例，一般都是Singleton作用域。 
    Prototype：每一个请求，会产生一个新的Bean实例。 
    Request：每一次http请求会产生一个新的Bean实例。



    模块
    
        核心容器（Core Container）
        
        核心容器包括spring-core，spring-beans，spring-context，spring-context-support和spring-expression（SpEL，Spring表达式语言，Spring Expression Language）等模块。
        spring-core和spring-beans模块是Spring框架的基础，包括控制反转和依赖注入等功能。BeanFactory是工厂模式的微妙实现，它移除了编码式单例的需要，并且可以把配置和依赖从实际编码逻辑中解耦。
        Context（spring-context）模块是在Core和Bean模块的基础上建立起来的，它以一种类似于JNDI注册的方式访问对象。Context模块继承自Bean模块，并且添加了国际化（比如，使用资源束）、事件传播、资源加载和透明地创建上下文（比如，通过Servelet容器）等功能。Context模块也支持Java EE的功能，比如EJB、JMX和远程调用等。ApplicationContext接口是Context模块的焦点。spring-context-support提供了对第三方库集成到Spring上下文的支持，比如缓存（EhCache, Guava, JCache）、邮件（JavaMail）、调度（CommonJ, Quartz）、模板引擎（FreeMarker, JasperReports, Velocity）等。
        spring-expression模块提供了强大的表达式语言用于在运行时查询和操作对象图。它是JSP2.1规范中定义的统一表达式语言的扩展，支持set和get属性值、属性赋值、方法调用、访问数组集合及索引的内容、逻辑算术运算、命名变量、通过名字从Spring IoC容器检索对象，还支持列表的投影、选择以及聚合等。
        

        AOP和检测（Instrumentation）

        spring-aop模块提供了面向切面编程（AOP）的实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。使用源码级的元数据，可以用类似于.Net属性的方式合并行为信息到代码中。
        spring-aspects模块提供了对AspectJ的集成。
        spring-instrument模块提供了对检测类的支持和用于特定的应用服务器的类加载器的实现。spring-instrument-tomcat模块包含了用于tomcat的Spring检测代理。
     
     
        消息处理（messaging）
        
        Spring 4 包含的spring-messaging模块是从Spring集成项目的关键抽象中提取出来的，这些项目包括Message、MessageChannel、MessageHandler和其它服务于消息处理的项目。这个模块也包含一系列的注解用于映射消息到方法，这类似于Spring MVC基于编码模型的注解。
        
        
        数据访问与集成
        数据访问与集成层包含JDBC、ORM、OXM、JMS和事务模块。 
        （译者注：JDBC=Java Data Base Connectivity，ORM=Object Relational Mapping，OXM=Object XML Mapping，JMS=Java Message Service）
        spring-jdbc模块提供了JDBC抽象层，它消除了冗长的JDBC编码和对数据库供应商特定错误代码的解析。
        spring-tx模块支持编程式事务和声明式事务，可用于实现了特定接口的类和所有的POJO对象。 
        （译者注：编程式事务需要自己写beginTransaction()、commit()、rollback()等事务管理方法，声明式事务是通过注解或配置由spring自动处理，编程式事务粒度更细）
        spring-orm模块提供了对流行的对象关系映射API的集成，包括JPA、JDO和Hibernate等。通过此模块可以让这些ORM框架和spring的其它功能整合，比如前面提及的事务管理。
        spring-oxm模块提供了对OXM实现的支持，比如JAXB、Castor、XML Beans、JiBX、XStream等。
        spring-jms模块包含生产（produce）和消费（consume）消息的功能。从Spring 4.1开始，集成了spring-messaging模块。
        
        
        Web
        Web层包括spring-web、spring-webmvc、spring-websocket、spring-webmvc-portlet等模块。
        spring-web模块提供面向web的基本功能和面向web的应用上下文，比如多部分（multipart）文件上传功能、使用Servlet监听器初始化IoC容器等。它还包括HTTP客户端以及Spring远程调用中与web相关的部分。
        spring-webmvc模块（即Web-Servlet模块）为web应用提供了模型视图控制（MVC）和REST Web服务的实现。Spring的MVC框架可以使领域模型代码和web表单完全地分离，且可以与Spring框架的其它所有功能进行集成。
        spring-webmvc-portlet模块（即Web-Portlet模块）提供了用于Portlet环境的MVC实现，并反映了spring-webmvc模块的功能。
        
        
        Test
        spring-test模块通过JUnit和TestNG组件支持单元测试和集成测试。它提供了一致性地加载和缓存Spring上下文，也提供了用于单独测试代码的模拟对象（mock object）。
        
    
    
    http://wiki.jikexueyuan.com/project/spring/bean-life-cycle.html
    
    Bean 的作用域
        singleton	该作用域将 bean 的定义的限制在每一个 Spring IoC 容器中的一个单一实例(默认)。
        prototype	该作用域将单一 bean 的定义限制在任意数量的对象实例。
        request	该作用域将 bean 的定义限制为 HTTP 请求。只在 web-aware Spring ApplicationContext 的上下文中有效。
        session	该作用域将 bean 的定义限制为 HTTP 会话。 只在web-aware Spring ApplicationContext的上下文中有效。
        global-session	该作用域将 bean 的定义限制为全局 HTTP 会话。只在 web-aware Spring ApplicationContext 的上下文中有效。
        
    Spring——Bean 后置处理器
        在初始化 bean 的之前和之后实现更复杂的逻辑，因为你有两个访问内置 bean 对象的后置处理程序的方法。
        public class InitHelloWorld implements BeanPostProcessor {
           public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {
              System.out.println("BeforeInitialization : " + beanName);
              return bean;  // you can return any other object as well
           }
           public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {
              System.out.println("AfterInitialization : " + beanName);
              return bean;  // you can return any other object as well
           }
        }
        
     Bean 定义模板
     你可以创建一个 Bean 定义模板，不需要花太多功夫它就可以被其他子 bean 定义使用。在定义一个 Bean 定义模板时，你不应该指定类的属性，而应该指定带 true 值的抽象属性，如下所示：
     
    <bean id="beanTeamplate" abstract="true">
    <bean id="helloIndia" class="com.tutorialspoint.HelloIndia" parent="beanTeamplate">
    
        

     Spring 中的事件处理
     
         ContextRefreshedEvent
         ApplicationContext 被初始化或刷新时，该事件被发布。这也可以在 ConfigurableApplicationContext 接口中使用 refresh() 方法来发生。
         ContextStartedEvent
         当使用 ConfigurableApplicationContext 接口中的 start() 方法启动 ApplicationContext 时，该事件被发布。你可以调查你的数据库，或者你可以在接受到这个事件后重启任何停止的应用程序。
         ContextStoppedEvent
         当使用 ConfigurableApplicationContext 接口中的 stop() 方法停止 ApplicationContext 时，发布这个事件。你可以在接受到这个事件后做必要的清理的工作。
         ContextClosedEvent
         当使用 ConfigurableApplicationContext 接口中的 close() 方法关闭 ApplicationContext 时，该事件被发布。一个已关闭的上下文到达生命周期末端；它不能被刷新或重启。
         RequestHandledEvent
         这是一个 web-specific 事件，告诉所有 bean HTTP 请求已经被服务。
     
     
     *Spring 框架的 AOP
        
        @AspectJ
        <aop:aspectj-autoproxy/>
        
        通知的类型
        Spring 方面可以使用下面提到的五种通知工作：
        
        通知	        描述
        前置通知	在一个方法执行之前，执行通知。
        后置通知	在一个方法执行之后，不考虑其结果，执行通知。
        返回后通知	在一个方法执行之后，只有在方法成功完成时，才能执行通知。
        抛出异常后通知	在一个方法执行之后，只有在方法退出抛出异常时，才能执行通知。
        环绕通知	在建议方法调用之前和之后，执行通知。
     
     JdbcTemplate
        
     编程式事务管理
        TransactionDefinition def = new DefaultTransactionDefinition();
        TransactionStatus status = transactionManager.getTransaction(def);
        transactionManager.commit(status);
        transactionManager.rollback(status);
     
     
     Spring web MVC 
        框架提供了模型-视图-控制的体系结构和可以用来开发灵活、松散耦合的 web 应用程序的组件。
        
        DispatcherServlet
            HandlerMapping、Controller 和 ViewResolver 是 WebApplicationContext 的一部分，
            而 WebApplicationContext 是带有一些对 web 应用程序必要的额外特性的 ApplicationContext 的扩展。
            
            
     *SpringMVC 流程
        流程说明（重要）：
        （1）客户端（浏览器）发送请求，直接请求到 DispatcherServlet。
        （2）DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。
        （3）解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。
        （4）HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑。
        （5）处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。
        （6）ViewResolver 会根据逻辑 View 查找实际的 View。
        （7）DispaterServlet 把返回的 Model 传给 View（视图渲染）。
        （8）把 View 返回给请求者（浏览器）
        
        
     SpringMVC 重要组件说明
     1、前端控制器DispatcherServlet（不需要工程师开发）,由框架提供（重要）
         作用：Spring MVC 的入口函数。接收请求，响应结果，相当于转发器，中央处理器。有了 DispatcherServlet 减少了其它组件之间的耦合度。
         用户请求到达前端控制器，它就相当于mvc模式中的c，DispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，
         DispatcherServlet的存在降低了组件之间的耦合性。
     2、处理器映射器HandlerMapping(不需要工程师开发),由框架提供
         作用：根据请求的url查找Handler。HandlerMapping负责根据用户请求找到Handler即处理器（Controller），
         SpringMVC提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。
     3、处理器适配器HandlerAdapter
         作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler 通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，
         通过扩展适配器可以对更多类型的处理器进行执行。
     4、处理器Handler(需要工程师开发)
         注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler Handler 是继DispatcherServlet前端控制器的
         后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。 由于Handler涉及到具体的用户业务请求，
         所以一般情况需要工程师根据业务需求开发Handler。
     5、视图解析器View resolver(不需要工程师开发),由框架提供
         作用：进行视图解析，根据逻辑视图名解析成真正的视图（view） View Resolver负责将处理结果生成View视图，View Resolver首先
         根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 
         springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。 一般情况下需要通过页面标签或
         页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。
     6、视图View(需要工程师开发)
         View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf...）
         注意：处理器Handler（也就是我们平常说的Controller控制器）以及视图层view都是需要我们自己手动开发的。其他的一些组件比如：前端控制器
         DispatcherServlet、处理器映射器HandlerMapping、处理器适配器HandlerAdapter等等都是框架提供给我们的，不需要自己手动开发。
     
     问题:AOP实现原理、动态代理和静态代理、Spring IOC的初始化过程、IOC原理、自己实现怎么实现一个IOC容器？
     
     *Spirng IoC容器的初始化过程
     https://www.cnblogs.com/chenjunjie12321/p/6124649.html
     
        分为三步：
        
        1.Resource定位（Bean的定义文件定位）
        2.将Resource定位好的资源载入到BeanDefinition
        3.将BeanDefiniton注册到容器中
        
     *Spring aop 原理(代理模式)
        https://www.cnblogs.com/lcngu/p/5339555.html
        
        代理的共有优点：业务类只需要关注业务逻辑本身，保证了业务类的重用性。 
            Java静态代理： 
            代理对象和目标对象实现了相同的接口，目标对象作为代理对象的一个属性，具体接口实现中，代理对象可以在调用目标对象相应方法前后加上其他业务处理逻辑。 
            缺点：一个代理类只能代理一个业务类。如果业务类增加方法时，相应的代理类也要增加方法。 
            Java动态代理： 
            Java动态代理是写一个类实现InvocationHandler接口，重写Invoke方法，在Invoke方法可以进行增强处理的逻辑的编写，
            这个公共代理类在运行的时候才能明确自己要代理的对象，同时可以实现该被代理类的方法的实现，然后在实现类方法的时候可以进行增强处理。 
            实际上：代理对象的方法 = 增强处理 + 被代理对象的方法
        
        *JDK和CGLIB生成动态代理类的区别： 
            1.JDK动态代理只能针对实现了接口的类生成代理（实例化一个类）。此时代理对象和目标对象实现了相同的接口，目标对象作为代理对象的一个属性，
            具体接口实现中，可以在调用目标对象相应方法前后加上其他业务处理逻辑 
            2.CGLIB是针对类实现代理，主要是对指定的类生成一个子类（没有实例化一个类），覆盖其中的方法 。 
            
            Spring AOP应用场景 
            性能检测，访问控制，日志管理，事务等。 
            默认的策略是如果目标类实现接口，则使用JDK动态代理技术，如果目标对象没有实现接口，则默认会采用CGLIB代理
        
        

                
     *总结
        1.bean的生命周期?
        2.ioc容器初始化流程?
        3.spring模块划分?
        4.spring事件类型?
        5.aop通知类型,原理?
        6.SpringMVC流程?

##14 kafka

###目录
    1.为什么需要分区?也就是说主题只有一个分区,难道不行吗?
        如果只有一个分区,那么我们的topic产生的所有数据都会存放到一个broker中,一台服务器的内存是有限的,当数据足够大时,
        会造成服务器的内存不足, 就会影响到我们的服务,所以我们需要分区,把数据分成多分存储在不同的broker中,以减轻服务器的存储压力.
        
        同时这也会造成一个问题: 如果将数据分区存储,如果其中一台服务器挂掉,则会造成这部分数据不可用.
        解决方法: 对数据进行备份,也就是副本机制,将同一份数据备份,放到不同的服务器上.
        
    2.日志为什么需要分段?
        为了提高读写效率,如果我们把所有的数据都存放在一个文件中,那么这个文件的将会很大,随之读写速度也是非常慢. 日志分段,
        那么我们读取的文件都比较小,速度将会很快.
        
        Kafka中的数据是以日志的形式存储的,Kafka作为消息中间件,只负责消息的临时存储,并不是永久存储,需要删除过期的数据.
        如果把所有的数据都存放在同一个文件,则删除过期的数据会很麻烦. 
        因为文件有日期属性,删除过期的数据只需要根据文件的日期属性删除就行.
        
        补充一下删除数据的两种方式:
            1 按照日期删除,如果只存储7天的日志,超过7天的则删除
            2 按照行数删除/按照文件的大小
            
    3.Kafka的Topic和分区内部是如何存储的,有什么特点?
      Topic是把数据分别存储到不同的分区中,也就是存放到不同的服务器中.
      分区内部的数据是存放在多个文件中的,每个分区都有一定数量的.inde文件和.log文件,其中.index和.log文件,这两个文件组成一个segment 段,
       这个段就存储着分区内的一部分数据,默认是1G.
      
    4.Kafka是依靠什么机制保持高可靠的,高可用?
      依靠副本机制
          kafka中的多个服务端节点会对其他服务端节点的主题分区的log文件进行复制.
          当集群中一个服务节点故障之后,这个故障的的服务器节点的所有请求都会被转移到其他正常的节点
          kafka中的每个主题分区都有一个主副本(leader)和0个或多个副本,副本会同步主副本中的数据,当主副本不可用时,kafka会选择一个副本接替工作, 但并不是所有的副本都能担任主副本,需要满足两个条件:
              该副本保持和zookeeper的通信.
              该副本中同步主副本中的数据是最多的.
              
    5.让你自己设计个消息队列,你会怎么设计,会考虑那些方面?
        1 该消息队列的使用场景? 是用于业务解耦,还是错峰流控等
        2 选择基于消息的系统模型,是否需要broken(消息队列服务端),
        3 消息在服务器中如何存储,生产者,服务器,消费者之间消息如何传递?
        4 如何在服务器之间实现RPC通信
        
        总结
            总而言之，消息队列不是万能的。对于需要强事务保证而且延迟敏感的，RPC是优于消息队列的。
            对于一些无关痛痒，或者对于别人非常重要但是对于自己不是那么关心的事情，可以利用消息队列去做。
            支持最终一致性的消息队列，能够用来处理延迟不那么敏感的“分布式事务”场景，而且相对于笨重的分布式事务，可能是更优的处理方式。
            当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的“漏斗”。在下游有能力处理的时候，再进行分发。
            如果下游有很多系统关心你的系统发出的通知的时候，果断地使用消息队列吧。
            
        扩展
            RPC : 远程过程调用协议RPC（Remote Procedure Call Protocol)
            
            RPC是指远程过程调用，也就是说两台服务器A，B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个
            内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。
            常用的分布式RPC框架有：dubbo、motan、rpcx、gRPC、thrift等等。
            
    6. kafka为什么那么快?
          kafka中的数据不支持修改.
          由系统层面的优化,缓存读取有预读取机制
          kafka使用了,分区,分布式系统.
          kafka 使用 zero copy 技术 (基于 linux 的 sendfile 函数)，可以减少传统数据传递时在 kernel 态和 user 态的 context 切换的空间和时间损耗。


    
    
###结束

    Kafka 生产者
    https://github.com/crossoverJie/JCSprout/blob/master/MD/kafka/kafka-product.md
    
            基本发送
            其实 send() 方法默认则是异步的，只要不手动调用 get() 方法。
            Future<RecordMetadata> send(ProducerRecord<K, V> producer, Callback callback);
            Callback 是一个回调接口，在消息发送完成之后可以回调我们自定义的实现。
            
        *发送流程
            初始化以及真正发送消息的 kafka-producer-network-thread IO 线程。
            将消息序列化。
            得到需要发送的分区。(路由)
            写入内部的一个缓存区中。
            初始化的 IO 线程不断的消费这个缓存来发送消息。
            
            路由分区
                指定分区
                可以在构建 ProducerRecord 为每条消息指定分区。
                
                自定义路由策略
                如果没有指定分区，则会调用 partitioner.partition 接口执行自定义分区策略。
                
                默认策略
                最后一种则是默认的路由策略，如果我们啥都没做就会执行该策略。
                该策略也会使得消息分配的比较均匀。
                
                    默认策略步骤
                    获取 Topic 分区数。
                    将内部维护的一个线程安全计数器 +1。
                    与分区数取模得到分区编号。
                    
            写入内部缓存
                在 send() 方法拿到分区后会调用一个 append() 函数,
                该函数中会调用一个 getOrCreateDeque() 写入到一个内部缓存中 batches。
                
            消费缓存
                在最开始初始化的 IO 线程其实是一个守护线程，它会一直消费这些数据。
                通过图中的几个函数会获取到之前写入的数据。这块内容可以不必深究，但其中有个 completeBatch 方法却非常关键。
                调用该方法时候肯定已经是消息发送完毕了，所以会调用 batch.done() 来完成之前我们在 send() 方法中定义的回调接口。
                  
                  
                            
        *Producer 参数解析
        
            acks
                acks 是一个影响消息吞吐量的一个关键参数。
                主要有 [all、-1, 0, 1] 这几个选项，默认为 1。
                由于 Kafka 不是采取的主备模式，而是采用类似于 Zookeeper 的主备模式。
                前提是 Topic 配置副本数量 replica > 1。
                
                当 acks = all/-1 时：
                意味着会确保所有的 follower 副本都完成数据的写入才会返回。
                这样可以保证消息不会丢失！
                但同时性能和吞吐量却是最低的。
                
                当 acks = 0 时：
                producer 不会等待副本的任何响应，这样最容易丢失消息但同时性能却是最好的！
        
                当 acks = 1 时：
                这是一种折中的方案，它会等待副本 Leader 响应，但不会等到 follower 的响应。
                一旦 Leader 挂掉消息就会丢失。但性能和消息安全性都得到了一定的保证。
    
            batch.size
                这个参数看名称就知道是内部缓存区的大小限制，对他适当的调大可以提高吞吐量。
                但也不能极端，调太大会浪费内存。小了也发挥不了作用，也是一个典型的时间和空间的权衡。
            
            retries(可能会导致消息重复和消息顺序不一致)
                retries 该参数主要是来做重试使用，当发生一些网络抖动都会造成重试。
                这个参数也就是限制重试次数。
                但也有一些其他问题。
                    1.因为是重发所以消息顺序可能不会一致，这也是上文提到就算是一个分区消息也不会是完全顺序的情况。
                    2.还是由于网络问题，本来消息已经成功写入了但是没有成功响应给 producer，进行重试时就可能会出现消息重复。
                    这种只能是消费者进行幂等处理。
            
        *高效的发送方式
            
            如果消息量真的非常大，同时又需要尽快的将消息发送到 Kafka。一个 producer 始终会收到缓存大小等影响。
            那是否可以创建多个 producer 来进行发送呢？
            配置一个最大 producer 个数。
            发送消息时首先获取一个 producer，获取的同时判断是否达到最大上限，没有就新建一个同时保存到内部的 List 中，保存时做好同步处理防止并发问题。
            获取发送者时可以按照默认的分区策略使用轮询的方式获取（保证使用均匀）。
            这样在大量、频繁的消息发送场景中可以提高发送效率减轻单个 producer 的压力。
            
        关闭 Producer
            最后则是 Producer 的关闭，Producer 在使用过程中消耗了不少资源（线程、内存、网络等）因此需要显式的关闭从而回收这些资源
            默认的 close() 方法和带有超时时间的方法都是在一定的时间后强制关闭。
            但在过期之前都会处理完剩余的任务。
            所以使用哪一个得视情况而定。
            
     
    kafka消费者
        https://www.cnblogs.com/huxi2b/p/6223228.html
     
        单线程消费
            由于数据散列在三个不同分区，所以单个线程需要遍历三个分区将数据拉取下来。
            取出的 100 条数据确实是分别遍历了三个分区。
            单线程消费虽然简单，但存在以下几个问题：
                效率低下。如果分区数几十上百个，单线程无法高效的取出数据。
                可用性很低。一旦消费线程阻塞，甚至是进程挂掉，那么整个消费程序都将出现问题。

        多线程消费
            在多线程之前不得不将消费模式分为两种进行探讨：消费组、独立消费者。
            
            独立消费者模式
                值得注意的是：独立消费者可以不设置 group.id 属性。
                通过 API 可以看出：我们可以手动指定需要消费哪些分区
                
                但这种方式有一个问题：可用性不高，当其中一个进程挂掉之后；该进程负责的分区数据没法转移给其他进程处理。
                
            消费组模式
            
                我们可以创建 N 个消费者实例（new KafkaConsumer()）,当这些实例都用同一个 group.id 来创建时，
                他们就属于同一个消费组。
                在同一个消费组中的消费实例可以收到消息，但一个分区的消息只会发往一个消费实例。
       
                消费组自平衡
                    消费组的优势
                    
                    我们可以在一个消费组中创建多个消费实例来达到高可用、高容错的特性，
                    不会出现单线程以及独立消费者挂掉之后数据不能消费的情况。同时基于多线程的方式也极大的提高了消费效率。
        
                    触发rebalance的情况
                    消费组中新增消费实例。
                    消费组中消费实例 down 掉。
                    订阅的 Topic 分区数发生变化。
                    如果是正则订阅 Topic 时，匹配的 Topic 数发生变化也会导致 Rebalance。
                    所以推荐使用这样的方式消费数据，同时扩展性也非常好。当性能不足新增分区时只需要启动新的消费实例加入到消费组中即可。
                    
                    
    为什么需要消息系统?
            削峰填谷
            解耦
            拓展性
            异步处理
            顺序性
            可恢复性(部分失效,仍然可用)
            
    相关概念
        1.producer：
        　　消息生产者，发布消息到 kafka 集群的终端或服务。
        2.broker：
        　　kafka 集群中包含的服务器。
        3.topic：
        　　每条发布到 kafka 集群的消息属于的类别，即 kafka 是面向 topic 的。
        4.partition：
        　　partition 是物理上的概念，每个 topic 包含一个或多个 partition。kafka 分配的单位是 partition。
        5.consumer：
        　　从 kafka 集群中消费消息的终端或服务。
        6.Consumer group：
        　　high-level consumer API 中，每个 consumer 都属于一个 consumer group，每条消息只能被 consumer group 中的一个 Consumer 消费，但可以被多个 consumer group 消费。
        7.replica：
        　　partition 的副本，保障 partition 的高可用。
        8.leader：
        　　replica 中的一个角色， producer 和 consumer 只跟 leader 交互。
        9.follower：
        　　replica 中的一个角色，从 leader 中复制数据。
        10.controller：
        　　kafka 集群中的其中一个服务器，用来进行 leader election 以及 各种 failover。
        12.zookeeper：
        　　kafka 通过 zookeeper 来存储集群的 meta 信息。

    
    位移管理(offset management)
        自动VS手动
        
            Kafka默认是定期帮你自动提交位移的(enable.auto.commit = true)，你当然可以选择手动提交位移实现自己控制。
            另外kafka会定期把group消费情况保存起来，做成一个offset map
            
        位移提交
        
            老版本的位移是提交到zookeeper中的，图就不画了，总之目录结构是：/consumers/<group.id>/offsets/<topic>/<partitionId>，
            但是zookeeper其实并不适合进行大批量的读写操作，尤其是写操作。因此kafka提供了另一种解决方案：增加__consumeroffsets topic，
            将offset信息写入这个topic，摆脱对zookeeper的依赖(指保存offset这件事情)。__consumer_offsets中的消息保存了每个
            consumer group某一时刻提交的offset信息。依然以上图中的consumer group为例，格式大概如下：
        

    
    Rebalance
    https://www.cnblogs.com/huxi2b/p/6223228.html
        
        什么是rebalance？
        
            rebalance本质上是一种协议，规定了一个consumer group下的所有consumer如何达成一致来分配订阅topic的每个分区。
            比如某个group下有20个consumer，它订阅了一个具有100个分区的topic。正常情况下，Kafka平均会为每个consumer分配5个分区。
            这个分配的过程就叫rebalance。
        
        什么时候rebalance？
        
            这也是经常被提及的一个问题。rebalance的触发条件有三种：
            
            组成员发生变更(新consumer加入组、已有consumer主动离开组或已有consumer崩溃了——这两者的区别后面会谈到)
            订阅主题数发生变更——这当然是可能的，如果你使用了正则表达式的方式进行订阅，那么新建匹配正则表达式的topic就会触发rebalance
            订阅主题的分区数发生变更
    
        如何进行组内分区分配?
            Kafka新版本consumer默认提供了两种分配策略：range和round-robin。当然Kafka采用了可插拔式的分配策略，
            你可以创建自己的分配器以实现不同的分配策略。实际上，由于目前range和round-robin两种分配器都有一些弊端，
            Kafka社区已经提出第三种分配器来实现更加公平的分配策略，只是目前还在开发中。我们这里只需要知道
            consumer group默认已经帮我们把订阅topic的分区分配工作做好了就行了。
            
        谁来执行rebalance和consumer group管理？
            
            Kafka提供了一个角色：coordinator来执行对于consumer group的管理。坦率说kafka对于coordinator的设计与修改是一个很长的故事。
            最新版本的coordinator也与最初的设计有了很大的不同。这里我只想提及两次比较大的改变。
            
            首先是0.8版本的coordinator，那时候的coordinator是依赖zookeeper来实现对于consumer group的管理的。
            Coordinator监听zookeeper的/consumers/<group>/ids的子节点变化以及/brokers/topics/<topic>数据变化来判断
            是否需要进行rebalance。group下的每个consumer都自己决定要消费哪些分区，并把自己的决定抢先在zookeeper中的
            /consumers/<group>/owners/<topic>/<partition>下注册。很明显，这种方案要依赖于zookeeper的帮助，
            而且每个consumer是单独做决定的，没有那种“大家属于一个组，要协商做事情”的精神。
            
            基于这些潜在的弊端，0.9版本的kafka改进了coordinator的设计，提出了group coordinator——每个consumer group
            都会被分配一个这样的coordinator用于组管理和位移管理。这个group coordinator比原来承担了更多的责任，
            比如组成员管理、位移提交保护机制等。当新版本consumer group的第一个consumer启动的时候，
            它会去和kafka server确定谁是它们组的coordinator。之后该group内的所有成员都会和该coordinator进行协调通信。
            显而易见，这种coordinator设计不再需要zookeeper了，性能上可以得到很大的提升。后面的所有部分我们都将讨论
            最新版本的coordinator设计。
            
        如何确定coordinator？
        
            上面简单讨论了新版coordinator的设计，那么consumer group如何确定自己的coordinator是谁呢？ 简单来说分为两步：
            
            确定consumer group位移信息写入__consumers_offsets的哪个分区。具体计算公式：
            　　__consumers_offsets partition# = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount)   
            注意：groupMetadataTopicPartitionCount由offsets.topic.num.partitions指定，默认是50个分区。
            该分区leader所在的broker就是被选定的coordinator
            
        协议(protocol)
        
            前面说过了， rebalance本质上是一组协议。group与coordinator共同使用它来完成group的rebalance。
            目前kafka提供了5个协议来处理与consumer group coordination相关的问题：
            
            Heartbeat请求：consumer需要定期给coordinator发送心跳来表明自己还活着
            LeaveGroup请求：主动告诉coordinator我要离开consumer group
            SyncGroup请求：group leader把分配方案告诉组内所有成员
            JoinGroup请求：成员请求加入组
            DescribeGroup请求：显示组的所有信息，包括成员信息，协议名称，分配方案，订阅信息等。通常该请求是给管理员使用
            Coordinator在rebalance的时候主要用到了前面4种请求。
            
        liveness
        
            consumer如何向coordinator证明自己还活着？ 通过定时向coordinator发送Heartbeat请求。如果超过了设定的超时时间，
            那么coordinator就认为这个consumer已经挂了。一旦coordinator认为某个consumer挂了，那么它就会开启新一轮rebalance，
            并且在当前其他consumer的心跳response中添加“REBALANCE_IN_PROGRESS”，告诉其他consumer：不好意思各位，你们重新申请加入组吧！
            
        
        *Rebalance过程
        
            终于说到consumer group执行rebalance的具体流程了。很多用户估计对consumer内部的工作机制也很感兴趣。
            下面就跟大家一起讨论一下。当然我必须要明确表示，rebalance的前提是coordinator已经确定了。
            
            总体而言，rebalance分为2步：Join和Sync
            
            1 Join， 顾名思义就是加入组。这一步中，所有成员都向coordinator发送JoinGroup请求，请求入组。
            一旦所有成员都发送了JoinGroup请求，coordinator会从中选择一个consumer担任leader的角色，
            并把组成员信息以及订阅信息发给leader——注意leader和coordinator不是一个概念。leader负责消费分配方案的制定。
            
            2 Sync，这一步leader开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。
            一旦完成分配，leader会将这个方案封装进SyncGroup请求中发给coordinator，非leader也会发SyncGroup请求，
            只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。
            这样组内的所有成员就都知道自己应该消费哪些分区了。
        
            值得注意的是， 在coordinator收集到所有成员请求前，它会把已收到请求放入一个叫purgatory(炼狱)的地方。
            记得国内有篇文章以此来证明kafka开发人员都是很有文艺范的，写得也是比较有趣，有兴趣可以去搜搜。
            然后是分发分配方案的过程，即SyncGroup请求：
        
            注意！！ consumer group的分区分配方案是在客户端执行的！Kafka将这个权利下放给客户端主要是因为这样做可以有更好的灵活性。
            比如这种机制下我可以实现类似于Hadoop那样的机架感知(rack-aware)分配方案，即为consumer挑选同一个机架下的分区数据，
            减少网络传输的开销。Kafka默认为你提供了两种分配策略：range和round-robin。由于这不是本文的重点，这里就不再详细展开了，
            你只需要记住你可以覆盖consumer的参数：partition.assignment.strategy来实现自己分配策略就好了。
    
        consumer group状态机
        
            和很多kafka组件一样，group也做了个状态机来表明组状态的流转。coordinator根据这个状态机会对consumer group做不同的处理，
            如下图所示(完全是根据代码注释手动画的，多见谅吧)
            
            简单说明下图中的各个状态：
            
            Dead：组内已经没有任何成员的最终状态，组的元数据也已经被coordinator移除了。
            这种状态响应各种请求都是一个response： UNKNOWN_MEMBER_ID
            Empty：组内无成员，但是位移信息还没有过期。这种状态只能响应JoinGroup请求
            PreparingRebalance：组准备开启新的rebalance，等待成员加入
            AwaitingSync：正在等待leader consumer将分配方案传给各个成员
            Stable：rebalance完成！可以开始消费了~
            至于各个状态之间的流程条件以及action，这里就不具体展开了。
            
    *kafka leader选举机制原理
    https://www.cnblogs.com/smartloli/p/9826923.html
    
        kafka在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。
        controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。
        同时controller也负责增删Topic以及Replica的重新分配。
        
        Kafka控制器，其实就是一个Kafka系统的Broker。它除了具有一般Broker的功能之外，还具有选举主题分区Leader节点的功能。
        在启动Kafka系统时，其中一个Broker会被选举为控制器，负责管理主题分区和副本状态，还会执行分区重新分配的管理任务。
        如果在Kafka系统运行过程中，当前的控制器出现故障导致不可用，那么Kafka系统会从其他正常运行的Broker中重新选举出新的控制器。
            
        控制器启动顺序
        
            在Kafka集群中，每个Broker在启动时会实例化一个KafkaController类。该类会执行一系列业务逻辑，选举出主题分区的Leader节点，步骤如下：
            
            第一个启动的代理节点，会在Zookeeper系统里面创建一个临时节点/controller，并写入该节点的注册信息，使该节点成为控制器；
            其他的代理节点陆续启动时，也会尝试在Zookeeper系统中创建/controller节点，但是由于/controller节点已经存在，
            所以会抛出“创建/controller节点失败异常”的信息。创建失败的代理节点会根据返回的结果，
            判断出在Kafka集群中已经有一个控制器被成功创建了，所以放弃创建/controller节点，这样就确保了Kafka集群控制器的唯一性；
            其他的代理节点，会在控制器上注册相应的监听器，各个监听器负责监听各自代理节点的状态变化。当监听到节点状态发生变化时，
            会触发相应的监听函数进行处理
    
        主题分区Leader节点的选举过程
            
            选举控制器的核心思路是：各个代理节点公平竞争抢占Zookeeper系统中创建/controller临时节点，
            最先创建成功的代理节点会成为控制器，并拥有选举主题分区Leader节点的功能。
            
            当Kafka系统实例化KafkaController类时，主题分区Leader节点的选举流程便会开始。其中涉及的核心类包含
            KafkaController、ZookeeperLeaderElector、LeaderChangeListener、SessionExpirationListener。
            
            KafkaController：在实例化ZookeeperLeaderElector类时，分别设置了两个关键的回调函数，
            即onControllerFailover和onControllerResignation；
            ZookeeperLeaderElector：实现主题分区的Leader节点选举功能，但是它并不会处理“代理节点与Zookeeper系统之间出现的会话超时”
            这种情况，它主要负责创建元数据存储路径、实例化变更监听器等，并通过订阅数据变更监听器来实时监听数据的变化，
            进而开始执行选举Leader的逻辑；
            LeaderChangeListener：如果节点数据发送变化，则Kafka系统中的其他代理节点可能已经成为Leader，
            接着Kafka控制器会调用onResigningAsLeader函数。当Kafka代理节点宕机或者被人为误删除时，
            则处于该节点上的Leader会被重新选举，通过调用onResigningAsLeader函数重新选择其他正常运行的代理节点成为新的Leader；
            SessionExpirationListener：当Kafka系统的代理节点和Zookeeper系统建立连接后，
            SessionExpirationListener中的handleNewSession函数会被调用，对于Zookeeper系统中会话过期的连接，会先进行一次判断
    
    下面主要对core目录模块进行说明，这块是kafka的核心。
    
        admin：管理员模块，操作和管理topic，paritions相关，包含create,delete topic,扩展patitions
        api：这块主要负责数据的组装，客户端和服务端数据交互的组装
        client：这个模块比较简单，只有一个类，主要是获取一些元数据，包括topic、broker等
        cluster：该模块定义了几个在kafka中比较重要的类：Broker，BrokerEndPoint，Cluster，EndPoint，Partition，Replica等，后续我们会对他们之间的关系进行分析
        common：通用类，定义了一些异常类等等
        consumer：comsumer处理模块，负责与消费者相关的操作
        controller：负责中央控制器选举，partition的leader选举，副本分配，副本重新分配，partition和replica扩容
        coordinator：协调器，rebalance的一些协调器，比如延迟心跳等
        javaapi：kafka提供出来的java生产消费的api
        log：文件存储模块，负责读写所有kafka的topic消息数据，也就是消息持久化模块
        message：封装多个消息组成一个“消息集”或压缩消息集
        metrics：内部状态监控模块
        network：kafka的网络处理模块，负责接受和处理客户端连接
        producer：生产者模块，包括同步和异步发送消息
        security.auth：安全认证模块
        serializer：序列化和反序列化工具
        server：kafka服务启动相关内容
        tools：工具模块，内容挺多，主要是与kafka相关的工具
        utils：通用工具模块，包括zk等等
        Kafka：程序入口
        
    
    kafka顺序写
        发到某个topic的消息会被均匀的分布到多个Partition上（随机或根据用户指定的回调函数进行分布），
        broker收到发布消息往对应Partition的最后一个segment上添加该消息，segment达到一定的大小后将不会再往该segment写数据，
        broker会创建新的segment。
        每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，
        这是Kafka高吞吐率的一个很重要的保证）。
        
        如何实现顺序写而非随机写。首先顺序写性能是随机写的万倍（300MB/S：30KB/S）；性能超过固态硬盘，是kafka高兴能的保证之一 ，
        其次还有buffer减少IO，以及零拷贝避免二次拷贝以及内核态到用户态的切换。但是我不懂顺序写是如何实现的，
        之前学习zookeeper知道follower的事物文件是先申请一块64M的连续磁盘空间，当不足4KB时再申请一块64M的连续磁盘空间，
        当有新事物来时，只在文件尾部做追加操作，可能达到磁盘顺序写的效果。好像中奖了。。
    
    Kafka集群会保留所有的消息，无论其被消费与否。两种策略删除旧数据：
    
        一基于时间的SLA(服务水平保证)，消息保存一定时间（通常为7天）后会被删除
        二是基于Partition文件大小，可以通过配置$KAFKA_HOME/config/server.properties
        
    消息的有序性
    
        总结：如果想保证消息的顺序，那就用一个 partition。 kafka 的每个 partition 只能同时被同一个 group 中的一个 consumer 消费
        
    
    充分利用Page Cache
    
        I/O Scheduler会将连续的小块写组装成大块的物理写从而提高性能
        
        I/O Scheduler会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间
        
        充分利用所有空闲内存（非JVM内存）。如果使用应用层Cache（即JVM堆内存），会增加GC负担
        
        读操作可直接在Page Cache内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过Page Cache）交换数据
        
        如果进程重启，JVM内的Cache会失效，但Page Cache仍然可用
        Broker收到数据后，写磁盘时只是将数据写入Page Cache，并不保证数据一定完全写入磁盘。从这一点看，可能会造成机器宕机时，
        Page Cache内的数据未写入磁盘从而造成数据丢失。但是这种丢失只发生在机器断电等造成操作系统不工作的场景，
        而这种场景完全可以由Kafka层面的Replication机制去解决。  
    
    支持多Disk Drive
    
        Broker的log.dirs配置项，允许配置多个文件夹。如果机器上有多个Disk Drive，可将不同的Disk挂载到不同的目录，
        然后将这些目录都配置到log.dirs里。Kafka会尽可能将不同的Partition分配到不同的目录，也即不同的Disk上，
        从而充分利用了多Disk的优势。
        
    零拷贝
    
        Kafka中存在大量的网络数据持久化到磁盘（Producer到Broker）和磁盘文件通过网络发送（Broker到Consumer）的过程。
        这一过程的性能直接影响Kafka的整体吞吐量。
        
        
        传统模式下的四次拷贝与四次上下文切换
        
        以将磁盘文件通过网络发送为例。传统模式下，一般使用如下伪代码所示的方法先将文件数据读入内存，然后通过Socket将内存中的数据发送出去。
        
        buffer = File.read
        Socket.send(buffer)
        这一过程实际上发生了四次数据拷贝。首先通过系统调用将文件数据读入到内核态Buffer（DMA拷贝），
        然后应用程序将内存态Buffer数据读入到用户态Buffer（CPU拷贝），接着用户程序通过Socket发送数据时将用户态Buffer数据
        拷贝到内核态Buffer（CPU拷贝），最后通过DMA拷贝将数据拷贝到NIC Buffer。同时，还伴随着四次上下文切换，如下图所示。
        
        sendfile和transferTo实现零拷贝
        
        Linux 2.4+内核通过sendfile系统调用，提供了零拷贝。数据通过DMA拷贝到内核态Buffer后，直接通过DMA拷贝到NIC Buffer，
        无需CPU拷贝。这也是零拷贝这一说法的来源。除了减少数据拷贝外，因为整个读文件-网络发送由一个sendfile调用完成，
        整个过程只有两次上下文切换，因此大大提高了性能。零拷贝过程如下图所示。
        
        从具体实现来看，Kafka的数据传输通过TransportLayer来完成，其子类PlaintextTransportLayer通过Java NIO的FileChannel的
        transferTo和transferFrom方法实现零拷贝，如下所示。
        
        @Override
        public long transferFrom(FileChannel fileChannel, long position, long count) throws IOException {
        return fileChannel.transferTo(position, count, socketChannel);
        }
        
        注： transferTo和transferFrom并不保证一定能使用零拷贝。实际上是否能使用零拷贝与操作系统相关，
        如果操作系统提供sendfile这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，
        否则并不能通过这两个方法本身实现零拷贝。
        
    批处理
    
        批处理是一种常用的用于提高I/O性能的方式。对Kafka而言，批处理既减少了网络传输的Overhead，又提高了写磁盘的效率。
    
    选举
    https://blog.csdn.net/qq_27384769/article/details/80115392
        Kafka 集群controller的选举过程如下 ：
            每个Broker都会在Controller Path (/controller)上注册一个Watch。
            当前Controller失败时，对应的Controller Path会自动消失（因为它是ephemeral Node），此时该Watch被fire，所有“活”着的Broker都会去竞选成为新的Controller（创建新的Controller Path),但是只会有一个竞选成功（这点由Zookeeper保证）。
            竞选成功者即为新的Leader，竞选失败者则重新在新的Controller Path上注册Watch。因为Zookeeper的Watch是一次性的，被fire一次之后即失效，所以需要重新注册。
        
        Kafka partition leader的选举过程如下 (由controller执行)：
            从Zookeeper中读取当前分区的所有ISR(in-sync replicas)集合
            调用配置的分区选择算法选择分区的leader
        
    
    
    数据压缩降低网络负载
    
        Kafka从0.7开始，即支持将数据压缩后再传输给Broker。除了可以将每条消息单独压缩然后传输外，Kafka还支持在批量发送时，
        将整个Batch的消息一起压缩后传输。数据压缩的一个基本原理是，重复数据越多压缩效果越好。
        因此将整个Batch的数据一起压缩能更大幅度减小数据量，从而更大程度提高网络传输效率。
        
    *ISR机制(in-sync Replica)
    https://blog.csdn.net/qq_37502106/article/details/80271800
        kafka不是完全同步，也不是完全异步，是一种ISR机制： 
            1. leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，而且是由leader动态维护 
            2. 如果一个flower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除 
            3. 当ISR中所有Replica都向Leader发送ACK时，leader才commit
        
    kafka进阶
        https://mp.weixin.qq.com/s/3i51S1jDXbqvi6fv1cuQSg?
        https://www.infoq.cn/article/kafka-analysis-part-1
        https://blog.csdn.net/stark_summer/article/details/50203133(kafka性能参数和压力测试揭秘)
        http://zqhxuyuan.github.io/2017/12/31/Kafka-Book-Resources/(Kafka技术内幕拾遗)
    
    *总结:
        1.为什么要使用消息系统?
        2.生产者发送流程?常用参数及意义?
        3.为什么要用消费者组?
        4.什么是rebalance?触发rebalance的情况?消费者组内分配分区的策略?谁负责管理rebalance和消费者组?
        5.如何确定coordinator?使用了哪些协议?
        6.consumer group状态机?
        7.controller选举?主题分区Leader节点的选举过程?
        8.分区和副本是在何处，以怎样的方式分配给Broker。
        9.kafka的Controller接收到Zookeeper的通知后做了哪些处理。
        10.分区的leader和follower是如何选举的。
        11.Data Replication何时Commit？
        12.Data Replication如何处理Replica恢复
        13.Data Replication如何处理Replica全部宕机
        14.消息中间件如何实现每秒几十万的高并发写入？    https://juejin.im/post/5c7bd09b6fb9a049ba424c15
        
        
        
        
    
##14 ZooKeeper

    ZooKeeper 概览
    https://github.com/llohellohe/zookeeper/blob/master/docs/overview.md
    https://www.jianshu.com/p/eec133595c68
    
        ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、
        命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。
        文件系统和通知机制 ZooKeeper的ZNode类似于文件系统，只不过每个节点还可以额外存放数据。
        当节点发生变化时（创建、删除、数据变更），可以通知各个客户端。
        
    为什么最好使用奇数台服务器构成 ZooKeeper 集群？
        假如我们的集群中有n台zookeeper服务器，那么也就是剩下的服务数必须大于n/2。先说一下结论，2n和2n-1的容忍度是一样的，
        都是n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有3台，那么最大允许宕掉1台
        zookeeper服务器，如果我们有4台的的时候也同样只允许宕掉1台。

    重要概念总结
        
        ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。
        为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），
        那么 ZooKeeper 本身仍然是可用的。
        ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，
        此限制也是保持znode中存储的数据量较小的进一步原因）。
        ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。
        （“读”多于“写”是协调服务的典型场景。）
        ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。
        持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。
        ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提供数据节点监听服务。
        
    ZooKeeper可以应用的场景
    https://ronghao.iteye.com/blog/1461798
    
        统一配置：把配置放在ZooKeeper的节点中维护，当配置变更时，客户端可以收到变更的通知，并应用最新的配置。
        集群管理：集群中的节点，创建ephemeral的节点，一旦断开连接，ephemeral的节点会消失，其它的集群机器可以收到消息。
        分布式锁：多个客户端发起节点创建操作，只有一个客户端创建成功，从而获得锁。
        
    zk提供的原语包含：
    
        create
        delete
        exists
        get data
        set data
        get chiledren
        sync
            
    ZooKeeper保证了：
    
        顺序一致性：客户端的操作会被按照顺序执行
        原子性：操作要不失败要不成功
        可靠性：一旦写入成功，数据就会被保持，直到下次覆盖。
        实时性
        单一系统镜像(single system image)：不管连接到zk集群的那台机器，客户端看到的视图都是一致的
        
    实现 ZooKeeper的组件包含
    
        Replicated Database是个内存数据库，保存了所有数据。
        更新会被写到磁盘，以便恢复。写也会被先序列化到磁盘后，在应用到内存数据库中。
        读的时候，会从各自server的内存数据库中读数据，写则是通过一致性协议完成（leader/follwer）的。
        
        
    分布式与数据复制
    
        Zookeeper作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处：
        1、 容错
        一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作；
        2、提高系统的扩展能力
        把负载分布到多个节点上，或者增加节点来提高系统的负载能力；
        3、提高性能
        让客户端本地访问就近的节点，提高用户访问速度。
        
        从客户端读写访问的透明度来看，数据复制集群系统分下面两种：
        1、写主(WriteMaster)
        对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离；
        2、写任意(Write Any)
        对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。
        
        对zookeeper来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，
        随着机器的增多吞吐能力肯定下降（这也是它建立observer的原因），而响应能力则取决于具体实现方式，
        是延迟复制保持最终一致性，还是立即复制快速响应。
        我们关注的重点还是在如何保证数据在集群所有机器的一致性，这就涉及到paxos算法。
        
    数据一致性与paxos算法
    
        据说Paxos算法的难理解与算法的知名度一样令人敬仰，所以我们先看如何保持数据的一致性，这里有个原则就是：
        在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。
        Paxos算法解决的什么问题呢，解决的就是保证每个节点执行相同的操作序列。好吧，这还不简单，master维护一个全局写队列，
        所有写操作都必须放入这个队列编号，那么无论我们写多少个节点，只要写操作是按编号来的，就能保证一致性。没错，就是这样，
        可是如果master挂了呢。
        Paxos算法通过投票来对写操作进行全局编号，同一时刻，只有一个写操作被批准，同时并发的写操作要去争取选票，
        只有获得过半数选票的写操作才会被批准（所以永远只会有一个写操作得到批准），其他的写操作竞争失败只好再发起一轮投票，
        就这样，在日复一日年复一年的投票中，所有写操作都被严格编号排序。编号严格递增，当一个节点接受了一个编号为100的写操作，
        之后又接受到编号为99的写操作（因为网络延迟等很多不可预见原因），它马上能意识到自己数据不一致了，
        自动停止对外服务并重启同步过程。任何一个节点挂掉都不会影响整个集群的数据一致性（总2n+1台，除非挂掉大于n台）。
    
    ZooKeeper Watcher
    
        Watcher接口 Watcher接口定义了process(WatchedEvent event) 方法，以及定义了接口Event。
        接口Event中定义了KeeperState和EventType。
        
        WatchedEvent WatchedEvent由KeeperState、EventType和path组成。
        
        它代表当前ZooKepper的连接状态，并且提供发生事件的znode路径以及时间类型。
        
        其中KeeperState代表ZooKeeper的连接状态，分别为：
            Disconnected
            NoSyncConnected
            SyncConnected
            AuthFailed
            ConnectedReadOnly
            SaslAuthenticated
            Expired
            
        EventType代表node的状态变更，分别为：
            None
            NodeCreated
            NodeDeleted
            NodeDataChanged，就算设置重复的数据也会有该事件
            NodeChildrenChanged
        
        Watcher 和 AsyncCallback 的区别
        
            Watcher：Watcher是用于监听节点，session 状态的，比如getData对数据节点a设置了watcher，那么当a的数据内容发生改变时，
            客户端会收到NodeDataChanged通知，然后进行watcher的回调。
            
            AsyncCallback:AsyncCallback是在以异步方式使用 ZooKeeper API 时，用于处理返回结果的。例如：getData同步调用的版本
            是：byte[] getData(String path, boolean watch,Stat stat)，异步调用的版本是：
            void getData(String path,Watcher watcher,AsyncCallback.DataCallback cb,Object ctx)，可以看到，
            前者是直接返回获取的结果，后者是通过AsyncCallback回调处理结果的。
        
        Watcher 的类型
            Watcher 主要是通过ClientWatchManager进行管理的。
            ClientWatchManager中有四种Watcher
            
                defaultWatcher：创建Zookeeper连接时传入的Watcher，用于监听 session 状态
                dataWatches：存放getData传入的Watcher
                existWatches：存放exists传入的Watcher，如果节点已存在，则Watcher会被添加到dataWatches
                childWatches：存放getChildren传入的Watcher
                
            从代码上可以发现，监听器是存在HashMap中的，key是节点名称path，value是Set<Watcher>
            private final Map<String, Set<Watcher>> dataWatches =
                    new HashMap<String, Set<Watcher>>();
            private final Map<String, Set<Watcher>> existWatches =
                    new HashMap<String, Set<Watcher>>();
            private final Map<String, Set<Watcher>> childWatches =
                    new HashMap<String, Set<Watcher>>();
            
            private volatile Watcher defaultWatcher;
            private final Map<String, Set<Watcher>> dataWatches =
                    new HashMap<String, Set<Watcher>>();
            private final Map<String, Set<Watcher>> existWatches =
                    new HashMap<String, Set<Watcher>>();
            private final Map<String, Set<Watcher>> childWatches =
                    new HashMap<String, Set<Watcher>>();
             
            private volatile Watcher defaultWatcher;
        
        通知的状态类型与事件类型
            在Watcher接口中，已经定义了所有的状态类型和事件类型
            
                KeeperState.Disconnected(0)此时客户端处于断开连接状态，和ZK集群都没有建立连接。
                EventType.None(-1)触发条件：一般是在与服务器断开连接的时候，客户端会收到这个事件。
                KeeperState. SyncConnected(3)此时客户端处于连接状态
                EventType.None(-1)触发条件：客户端与服务器成功建立会话之后，会收到这个通知。
                EventType. NodeCreated (1)触发条件：所关注的节点被创建。
                EventType. NodeDeleted (2)触发条件：所关注的节点被删除。
                EventType. NodeDataChanged (3)触发条件：所关注的节点的内容有更新。注意，这个地方说的内容是指数据的版本号dataVersion。
                因此，即使使用相同的数据内容来更新，还是会收到这个事件通知的。无论如何，调用了更新接口，就一定会更新dataVersion的。
                EventType. NodeChildrenChanged (4)触发条件：所关注的节点的子节点有变化。这里说的变化是指子节点的个数和组成，
                具体到子节点内容的变化是不会通知的。
                KeeperState. AuthFailed(4)认证失败
                EventType.None(-1)
                KeeperState. Expired(-112)session 超时
                EventType.None(-1)
                
            每次返回都会从HashMap中移除节点对应的Watcher，例如：addTo(dataWatches.remove(clientPath), result);，
            这就是为什么Watcher是一次性的原因（defaultWatcher除外）。值得注意的是，由于使用的是HashSet存储Watcher，
            重复添加同一个实例的Watcher也只会被触发一次。
        
        
        Zookeeper 客户端会启动两个常驻线程
        
            SendThread：负责 IO 操作，包括发送，接受响应，发送 ping 等。
            EventThread：负责处理事件，执行回调函数。
        
        
        总结
            Zookeeper 客户端中Watcher和AsyncCallback都是异步回调的方式，但它们回调的时机是不一样的，前者是由服务器发送事件触发客户端回调，
            后者是在执行了请求后得到响应后客户端主动触发的。它们的共同点在于都需要在获取了服务器响应之后，
            由SendThread写入EventThread的waitingEvents中，然后由EventThread逐个从事件队列中获取并处理。
        
    创建ZooKeeper Session
    
        ZooKeeper(String connectString, int sessionTimeout, Watcher watcher)
        就可以创建Zookeeper的一个Session。
        
        过程和原理
            初始化连接到ZooKeeper，并且注册一个监视器W
            W在接收到事件后，执行process()方法，根据事件的state()关闭或者重新启动额外的任务进程。
            如果发生事件的znode和注册的znode路径一致，则调用ZooKeeper的exist()方法，然后执行StatCallback这个回调方法。
            在StatCallback的回调方法中，获得znode对应的数据
            如果数据存在，则执行打印出响应的结果
        
    分布式进程的沟通方式 在分布式系统中的通常有两种通信方式：
        通过网络直接交换消息
        使用共享存储。
        ZooKeeper用的是共享存储的方式。
        
        在分布式系统中需要关注以下三个问题：
            消息延迟
            处理器速度
            时钟偏差
            
        Master-Worker架构 Master-Worker是典型的一种分布式系统的架构，它需要解决三个关键问题：
            Master Crash
            Worker Crash
            Master-Worker通信失败
        
        Master Crash 关键一：需要关注主Master crash后，备用master启动后。
        如何恢复到主Master crash时候的状态，由于主已经挂了，因此备用master不可能从主中恢复状态，而需要借助其它方式。
        关键二：避免出现脑裂的情况。
        比如某些情况下，由于主load很高，导致通信延迟严重，可能勿认为主挂了，从而启动另个主，这样会产生脑裂的情况，从而导致不一致。
        
        Worker Crash Master需要能检测出worker挂了，将任务重新分配。
        有时候还需要清理worker挂后产生的一些副作用，比如数据清理等。
        
    ZNode模式 目前ZNode包含持久模式和短暂模式ephemeral。
    
        ephemeral模式指的是这个节点在session断了之后就会消失。
        而持久模式的ZNode则会继续保持。
        在master-worker模式下，ephemeral模式可以用于检测master或者worker是否挂掉。
        如果使用持久模式的话，由于ZNode一直存在，无法感知到master或者worker是否已经crash。
        ephemeral模式的节点也可以被主动删除。
        除了持久模式和ephemeral模式外，ZNode还可以是有序的（自动分配自增ID到节点上，比如task-1,task-2,task-3）。
        因此ZNode一共有四种形态：
        
            持久
            持久有序
            ephemeral
            ephemeral有序

        
    分布式锁 
        1.利用节点名称的唯一性来实现共享锁
            ZooKeeper抽象出来的节点结构是一个和unix文件系统类似的小型的树状的目录结构。ZooKeeper机制规定：同一个目录下只能有一个唯一的文件名。
            例如：我们在Zookeeper目录/test目录下创建，两个客户端创建一个名为Lock节点，只有一个能够成功。
            算法思路: 利用名称唯一性，加锁操作时，只需要所有客户端一起创建/test/Lock节点，只有一个创建成功，成功者获得锁。解锁时，
            只需删除/test/Lock节点，其余客户端再次进入竞争创建节点，直到所有客户端都获得锁。
        2.利用临时顺序节点实现共享锁的一般做法
            Zookeeper中有一种节点叫做顺序节点，故名思议，假如我们在/lock/目录下创建节3个点，ZooKeeper集群会按照提起创建的顺序来创建节点，
            节点分别为/lock/0000000001、/lock/0000000002、/lock/0000000003。
            ZooKeeper中还有一种名为临时节点的节点，临时节点由某个客户端创建，当客户端与ZooKeeper集群断开连接，则开节点自动被删除。
            
            利用上面这两个特性，我们来看下获取实现分布式锁的基本逻辑：
                客户端调用create()方法创建名为“locknode/guid-lock-”的节点，需要注意的是，这里节点的创建类型需要设置为EPHEMERAL_SEQUENTIAL。
                客户端调用getChildren(“locknode”)方法来获取所有已经创建的子节点，同时在这个节点上注册上子节点变更通知的Watcher。
                客户端获取到所有子节点path之后，如果发现自己在步骤1中创建的节点是所有节点中序号最小的，那么就认为这个客户端获得了锁。
                如果在步骤3中发现自己并非是所有子节点中最小的，说明自己还没有获取到锁，就开始等待，直到下次子节点变更通知的时候，
                再进行子节点的获取，判断是否获取锁。
                释放锁的过程相对比较简单，就是删除自己创建的那个子节点即可。
        
            上面这个分布式锁的实现中，大体能够满足了一般的分布式集群竞争锁的需求。这里说的一般性场景是指集群规模不大，一般在10台机器以内。
            不过，细想上面的实现逻辑，我们很容易会发现一个问题，步骤4，“即获取所有的子点，判断自己创建的节点是否已经是序号最小的节点”，
            这个过程，在整个分布式锁的竞争过程中，大量重复运行，并且绝大多数的运行结果都是判断出自己并非是序号最小的节点，
            从而继续等待下一次通知——这个显然看起来不怎么科学。客户端无端的接受到过多的和自己不相关的事件通知，这如果在集群规模大的时候，
            会对Server造成很大的性能影响，并且如果一旦同一时间有多个节点的客户端断开连接，这个时候，
            服务器就会像其余客户端发送大量的事件通知——这就是所谓的惊群效应。而这个问题的根源在于，没有找准客户端真正的关注点。
            我们再来回顾一下上面的分布式锁竞争过程，它的核心逻辑在于：判断自己是否是所有节点中序号最小的。于是，很容易可以联想的到的是，
            每个节点的创建者只需要关注比自己序号小的那个节点。
        
        3.利用临时顺序节点实现共享锁的改进实现
            下面是改进后的分布式锁实现，和之前的实现方式唯一不同之处在于，这里设计成每个锁竞争者，只需要关注”locknode”节点下序号比
            自己小的那个节点是否存在即可。
            算法思路：对于加锁操作，可以让所有客户端都去/lock目录下创建临时顺序节点，如果创建的客户端发现自身创建节点序列号是
            /lock/目录下最小的节点，则获得锁。否则，监视比自己创建节点的序列号小的节点（比自己创建的节点小的最大节点），进入等待。
            对于解锁操作，只需要将自身创建的节点删除即可。
            
    如果创建了临时顺序节点，那么ZooKeeper会自动在节点后缀加上一个数字，并且在API接口的返回值中返回该数据节点的一个完整的节点路径。
    
    Watch和Notifications Watch可以避免每次主动去请求数据是否变化，而是当ZNode变化时，来通知。
    
        Watch是个一次性操作，每次收到通知后，必须重新watch，如果时间比较久或者ZNode更新频繁，在此时间之间，
        可能会有更新没有被通知到（还没来得急watch）。
        ZNode的创建、删除和修改都可以被watch到。
        遗留问题：ZK是否能得到每次节点通知？
        ZK有个保证是，每次通知被送到每个客户端前，ZNode不会发生下一次的变化。
        因此客户端可以保证看到的变化是顺序一致的。    
        
    版本号 
        每个ZNode都会带有一个版本号，每次节点变化，版本号就会变化。
        以此可以避免并发的更新问题（版本号不正确的更新会失败）。
        如果不管版本,可以设置为-1
        
    ZXID 为保证客户端重连到新的服务端时，不会看到过期的更新，保证至少新服务端上的更新比客户端之前看到的更新要新。
        ZK有个全局的事务ID，每次更新操作后，ZXID会自增。
        如果客户端连在S1上，并且执行了更新操作，那么ZXID将会+1,比如ZXID=2。
        当客户端尝试重连到S2时，若S2由于延迟等，导致上面的ZXID 依旧为1的话，客户端将重试连接到S3（ZXID>2）。
        
    
    Observer
        observer 是除了leader\follower的另种角色，它不参与投票过程。
        
    leader的选择机制
        zookeeper提供了三种方式：
        
            LeaderElection
            AuthFastLeaderElection
            FastLeaderElection
            
            默认的算法是FastLeaderElection，所以这篇主要分析它的选举机制。   
    
        选择机制中的概念
            服务器ID
            比如有三台服务器，编号分别是1,2,3。
            编号越大在选择算法中的权重越大。
            
            数据ID
            服务器中存放的最大数据ID.
            值越大说明数据越新，在选举算法中数据越新权重越大。
            
            逻辑时钟
            或者叫投票的次数，同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加，然后与接收到的其它服务器返回的投票信息中的数值相比，根据不同的值做出不同的判断。
            
            选举状态
            LOOKING，竞选状态。
            FOLLOWING，随从状态，同步leader状态，参与投票。
            OBSERVING，观察状态,同步leader状态，不参与投票。
            LEADING，领导者状态。
    
        选举消息内容
            在投票完成后，需要将投票信息发送给集群中的所有服务器，它包含如下内容。
            
            服务器ID
            数据ID
            逻辑时钟
            选举状态
    
        判断是否已经胜出
        默认是采用投票数大于半数则胜出的逻辑。
        
        选举流程简述
            目前有5台服务器，每台服务器均没有数据，它们的编号分别是1,2,3,4,5,按编号依次启动，它们的选择举过程如下：
            
            服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking。
            服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是LOOKING。
            服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为领导者，服务器1,2成为小弟。
            服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为小弟。
            服务器5启动，后面的逻辑同服务器4成为小弟。
    
    
        算法核心
        https://www.cnblogs.com/leesf456/p/6107600.html
        
        　　上图展示了FastLeaderElection模块是如何与底层网络I/O进行交互的。Leader选举的基本流程如下
        
        　　1. 自增选举轮次。Zookeeper规定所有有效的投票都必须在同一轮次中，在开始新一轮投票时，会首先对logicalclock进行自增操作。
        
        　　2. 初始化选票。在开始进行新一轮投票之前，每个服务器都会初始化自身的选票，并且在初始化阶段，每台服务器都会将自己推举为Leader。
        
        　　3. 发送初始化选票。完成选票的初始化后，服务器就会发起第一次投票。Zookeeper会将刚刚初始化好的选票放入sendqueue中，
            由发送器WorkerSender负责发送出去。
        
        　　4. 接收外部投票。每台服务器会不断地从recvqueue队列中获取外部选票。如果服务器发现无法获取到任何外部投票，
            那么就会立即确认自己是否和集群中其他服务器保持着有效的连接，如果没有连接，则马上建立连接，如果已经建立了连接，
            则再次发送自己当前的内部投票。
        
        　　5. 判断选举轮次。在发送完初始化选票之后，接着开始处理外部投票。在处理外部投票时，会根据选举轮次来进行不同的处理。
        
        　　　　· 外部投票的选举轮次大于内部投票。若服务器自身的选举轮次落后于该外部投票对应服务器的选举轮次，
                 那么就会立即更新自己的选举轮次(logicalclock)，并且清空所有已经收到的投票，然后使用初始化的投票来进行
                 PK以确定是否变更内部投票。最终再将内部投票发送出去。
        
        　　　　· 外部投票的选举轮次小于内部投票。若服务器接收的外选票的选举轮次落后于自身的选举轮次，
                 那么Zookeeper就会直接忽略该外部投票，不做任何处理，并返回步骤4。
        
        　　　　· 外部投票的选举轮次等于内部投票。此时可以开始进行选票PK。
        
        　　6. 选票PK。在进行选票PK时，符合任意一个条件就需要变更投票。
        
        　　　　· 若外部投票中推举的Leader服务器的选举轮次大于内部投票，那么需要变更投票。
        
        　　　　· 若选举轮次一致，那么就对比两者的ZXID，若外部投票的ZXID大，那么需要变更投票。
        
        　　　　· 若两者的ZXID一致，那么就对比两者的SID，若外部投票的SID大，那么就需要变更投票。
        
        　　7. 变更投票。经过PK后，若确定了外部投票优于内部投票，那么就变更投票，即使用外部投票的选票信息来覆盖内部投票，
            变更完成后，再次将这个变更后的内部投票发送出去。
        
        　　8. 选票归档。无论是否变更了投票，都会将刚刚收到的那份外部投票放入选票集合recvset中进行归档。recvset用于记录当前服务器
            在本轮次的Leader选举中收到的所有外部投票（按照服务队的SID区别，如{(1, vote1), (2, vote2)...}）。
        
        　　9. 统计投票。完成选票归档后，就可以开始统计投票，统计投票是为了统计集群中是否已经有过半的服务器认可了当前的内部投票，
            如果确定已经有过半服务器认可了该投票，则终止投票。否则返回步骤4。
        
        　　10. 更新服务器状态。若已经确定可以终止投票，那么就开始更新服务器状态，服务器首选判断当前被过半服务器认可的投票所对应的
            Leader服务器是否是自己，若是自己，则将自己的服务器状态更新为LEADING，若不是，则根据具体情况来确定自己是FOLLOWING或是OBSERVING。
        
        　　以上10个步骤就是FastLeaderElection的核心，其中步骤4-9会经过几轮循环，直到有Leader选举产生。
        
    网络I/O
        QuorumCnxManager：网络I/O
        
        　　每台服务器在启动的过程中，会启动一个QuorumPeerManager，负责各台服务器之间的底层Leader选举过程中的网络通信。
        
        　　(1) 消息队列。QuorumCnxManager内部维护了一系列的队列，用来保存接收到的、待发送的消息以及消息的发送器，除接收队列以外，其他队列都按照SID分组形成队列集合，如一个集群中除了自身还有3台机器，那么就会为这3台机器分别创建一个发送队列，互不干扰。
        
        　　　　· recvQueue：消息接收队列，用于存放那些从其他服务器接收到的消息。
        
        　　　　· queueSendMap：消息发送队列，用于保存那些待发送的消息，按照SID进行分组。
        
        　　　　· senderWorkerMap：发送器集合，每个SenderWorker消息发送器，都对应一台远程Zookeeper服务器，负责消息的发送，也按照SID进行分组。
        
        　　　　· lastMessageSent：最近发送过的消息，为每个SID保留最近发送过的一个消息。
        
        　　(2) 建立连接。为了能够相互投票，Zookeeper集群中的所有机器都需要两两建立起网络连接。QuorumCnxManager在启动时会创建一个
                ServerSocket来监听Leader选举的通信端口(默认为3888)。开启监听后，Zookeeper能够不断地接收到来自其他服务器的创建连接请求，
                在接收到其他服务器的TCP连接请求时，会进行处理。为了避免两台机器之间重复地创建TCP连接，Zookeeper只允许SID大的服务器主动
                和其他机器建立连接，否则断开连接。在接收到创建连接请求后，服务器通过对比自己和远程服务器的SID值来判断是否接收连接请求，
                如果当前服务器发现自己的SID更大，那么会断开当前连接，然后自己主动和远程服务器建立连接。一旦连接建立，
                就会根据远程服务器的SID来创建相应的消息发送器SendWorker和消息接收器RecvWorker，并启动。
        
        　　(3) 消息接收与发送。消息接收：由消息接收器RecvWorker负责，由于Zookeeper为每个远程服务器都分配一个单独的RecvWorker，
                因此，每个RecvWorker只需要不断地从这个TCP连接中读取消息，并将其保存到recvQueue队列中。消息发送：
                由于Zookeeper为每个远程服务器都分配一个单独的SendWorker，因此，每个SendWorker只需要不断地从对应的消息发送队列中
                获取出一个消息发送即可，同时将这个消息放入lastMessageSent中。在SendWorker中，一旦Zookeeper发现针对当前服务器的
                消息发送队列为空，那么此时需要从lastMessageSent中取出一个最近发送过的消息来进行再次发送，这是为了解决接收方在消息
                接收前或者接收到消息后服务器挂了，导致消息尚未被正确处理。同时，Zookeeper能够保证接收方在处理消息时，会对重复消息进行正确的处理。
        
    ZAB协议
    
        什么是 ZAB 协议？ ZAB 协议介绍
            
            ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。
            ZAB 协议定义：ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持 崩溃恢复 和 原子广播 协议。下面我们会重点讲这两个东西。
            
        Zookeeper 如何处理集群中的数据。所有客户端写入数据都是写入到 主进程（称为 Leader）中，然后，由 Leader 复制到
        备份进程（称为 Follower）中。从而保证数据一致性。从设计上看，和 Raft 类似。
        
        那么复制过程又是如何的呢？复制过程类似 2PC，ZAB 只需要 Follower 有一半以上返回 Ack 信息就可以执行提交，大大减小了同步阻塞。也提高了可用性。
        简单介绍完，开始重点介绍 消息广播 和 崩溃恢复。整个 Zookeeper 就是在这两个模式之间切换。 简而言之，当 Leader 服务可以正常使用，就进入消息广播模式，当 Leader 不可用时，则进入崩溃恢复模式。
    
    消息广播
        ZAB 协议的消息广播过程使用的是一个原子广播协议，类似一个 二阶段提交过程。对于客户端发送的写请求，全部由 Leader 接收，
        Leader 将请求封装成一个事务 Proposal，将其发送给所有 Follwer ，然后，根据所有 Follwer 的反馈，如果超过半数成功响应，
        则执行 commit 操作（先提交自己，再发送 commit 给所有 Follwer）。
        
        整个广播流程分为 3 步骤：
        
        1.将数据都复制到 Follwer 中
        2.等待 Follwer 回应 Ack，最低超过半数即成功
        3.当超过半数成功回应，则执行 commit ，同时提交自己
        
        还有一些细节：
        
            Leader 在收到客户端请求之后，会将这个请求封装成一个事务，并给这个事务分配一个全局递增的唯一 ID，称为事务ID（ZXID），
            ZAB 兮协议需要保证事务的顺序，因此必须将每一个事务按照 ZXID 进行先后排序然后处理。
            在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞。
            zookeeper集群中为保证任何所有进程能够有序的顺序执行，只能是 Leader 服务器接受写请求，即使是 Follower 服务器接受到客户端的请求，
            也会转发到 Leader 服务器进行处理。
            实际上，这是一种简化版本的 2PC，不能解决单点问题。等会我们会讲述 ZAB 如何解决单点问题（即 Leader 崩溃问题）。

    崩溃恢复
        刚刚我们说消息广播过程中，Leader 崩溃怎么办？还能保证数据一致吗？如果 Leader 先本地提交了，然后 commit 请求没有发送出去，怎么办？
        实际上，当 Leader 崩溃，即进入我们开头所说的崩溃恢复模式（崩溃即：Leader 失去与过半 Follwer 的联系）。下面来详细讲述。
        
        假设1：Leader 在复制数据给所有 Follwer 之后崩溃，怎么办？
        假设2：Leader 在收到 Ack 并提交了自己，同时发送了部分 commit 出去之后崩溃怎么办？
        
        针对这些问题，ZAB 定义了 2 个原则：
        
        ZAB 协议确保那些已经在 Leader 提交的事务最终会被所有服务器提交。
        ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务。
        所以，ZAB 设计了下面这样一个选举算法：
        能够确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务。
        
        针对这个要求，如果让 Leader 选举算法能够保证新选举出来的 Leader 服务器拥有集群总所有机器编号（即 ZXID 最大）的事务，
        那么就能够保证这个新选举出来的 Leader 一定具有所有已经提交的提案。
        而且这么做有一个好处是：可以省去 Leader 服务器检查事务的提交和丢弃工作的这一步操作。
        
    数据同步
        当崩溃恢复之后，需要在正式工作之前（接收客户端请求），Leader 服务器首先确认事务是否都已经被过半的 Follwer 提交了，
        即是否完成了数据同步。目的是为了保持数据一致。
        当所有的 Follwer 服务器都成功同步之后，Leader 会将这些服务器加入到可用服务器列表中。
        实际上，Leader 服务器处理或丢弃事务都是依赖着 ZXID 的，那么这个 ZXID 如何生成呢？
        答：在 ZAB 协议的事务编号 ZXID 设计中，ZXID 是一个 64 位的数字，其中低 32 位可以看作是一个简单的递增的计数器，
        针对客户端的每一个事务请求，Leader 都会产生一个新的事务 Proposal 并对该计数器进行 + 1 操作。
        而高 32 位则代表了 Leader 服务器上取出本地日志中最大事务 Proposal 的 ZXID，并从该 ZXID 中解析出对应的 epoch 值，
        然后再对这个值加一。
        高 32 位代表了每代 Leader 的唯一性，低 32 代表了每代 Leader 中事务的唯一性。同时，也能让 Follwer 通过高 32 位识别不同的 Leader。
        简化了数据恢复流程。
        基于这样的策略：当 Follower 链接上 Leader 之后，Leader 服务器会根据自己服务器上最后被提交的 ZXID 和 Follower 上的 ZXID 进行比对，
        比对结果要么回滚，要么和 Leader 同步。
    
    ZXID变化
        集群范围内，全局单调唯一递增；
        数据构成
        　　ZXID是一个64位的数字，高32位代表Leader周期,低32代表一个单调递增的计数器. 
        当有新的Leader产生时，Leader周期epoch加1，计数器从0开始；
    
    
    进阶
    https://blog.csdn.net/u010039929/article/details/70171754
    
    *总结
        1.zk保证了什么?数据结构?节点类型?
        2.WatchedEvent由什么组成?KeeperState包括哪些类型?EventType包括哪些类型?
        3.zk如何实现分布式锁?(两种方法)排它锁和共享锁?实现分布式队列?
        4.zab协议两种模式?
        5.zk选举流程?
        6.QuorumCnxManager：网络I/O?
    

##15 mybatis
    https://blog.csdn.net/u010890358/article/details/80665753

###目录
    -2.mybatis核心类
        SqlSessionFactory
            每个基于 MyBatis 的应用都是以一个 SqlSessionFactory 的实例为中心的。SqlSessionFactory 的实例可以通过 
            SqlSessionFactoryBuilder 获得。而 SqlSessionFactoryBuilder 则可以从 XML 配置文件或通过Java的方式构建出 
            SqlSessionFactory 的实例。SqlSessionFactory 一旦被创建就应该在应用的运行期间一直存在，建议使用单例模式或者静态单例模式。
            一个SqlSessionFactory对应配置文件中的一个环境（environment），如果你要使用多个数据库就配置多个环境分别对应一个SqlSessionFactory。
        SqlSession：
            SqlSession是一个接口，它有2个实现类，分别是DefaultSqlSession(默认使用)以及SqlSessionManager。SqlSession通过内部
            存放的执行器（Executor）来对数据进行CRUD。此外SqlSession不是线程安全的，因为每一次操作完数据库后都要调用close对其进行关闭，
            官方建议通过try-finally来保证总是关闭SqlSession。
        Executor：
            Executor（执行器）接口有两个实现类，其中BaseExecutor有三个继承类分别是BatchExecutor（重用语句并执行批量更新），
            ReuseExecutor（重用预处理语句prepared statements），SimpleExecutor（普通的执行器）。
            以上三个就是主要的Executor。通过下图可以看到Mybatis在Executor的设计上面使用了装饰者模式，
            我们可以用CachingExecutor来装饰前面的三个执行器目的就是用来实现缓存。
        MappedStatement：
            MappedStatement就是用来存放我们SQL映射文件中的信息包括sql语句，输入参数，输出参数等等。一个SQL节点对应一个MappedStatement对象。

        
    -1.mybatis流程
        1.第一步通过SqlSessionFactoryBuilder创建SqlSessionFactory
            首先在SqlSessionFactoryBuilder的build（）方法中可以看到MyBatis内部定义了一个类XMLConfigBuilder用来解析配置文件
            mybatis-config.xml。针对配置文件中的每一个节点进行解析并将数据存放到Configuration这个对象中，紧接着使用带有
            Configuration的构造方法发返回一个DefautSqlSessionFactory。
        2.第二步通过SqlSessionFactory创建SqlSession
        3.第三步通过SqlSession拿到Mapper对象的代理
        4.第四步通过MapperProxy调用Maper中相应的方法
        
        详细版本:
        mapper.xml -> mybatis-config.xml -> sqlSessionFactoryBuilder -> sqlSessionFactory -> sqlSession -> MapperProxy 
        -> DB
        
    0.mybatis常用配置项?
    https://blog.csdn.net/fageweiketang/article/details/80767532
        <settings>
            <setting name="cacheEnabled" value="true"/> (二级缓存)
            <setting name="lazyLoadingEnabled" value="true"/>   (懒加载)
            <setting name="multipleResultSetsEnabled" value="true"/>
            <setting name="useColumnLabel" value="true"/>
            <setting name="useGeneratedKeys" value="false"/>
            <setting name="autoMappingBehavior" value="PARTIAL"/>
            <setting name="autoMappingUnknownColumnBehavior" value="WARNING"/>
            <setting name="defaultExecutorType" value="SIMPLE"/>    (默认执行器)
            <setting name="defaultStatementTimeout" value="30"/>
            <setting name="defaultFetchSize" value="200"/>
            <setting name="safeRowBoundsEnabled" value="false"/>
            <setting name="mapUnderscoreToCamelCase" value="false"/>
            <setting name="localCacheScope" value="SESSION"/>
            <setting name="jdbcTypeForNull" value="OTHER"/>
            <setting name="lazyLoadTriggerMethods" value="equals,clone,hashCode,toString"/>
        </settings>
        
    1.#{}和${}的区别是什么？
        #{}解析传递进来的参数数据
        ${}对传递进来的参数原样拼接在SQL中
        #{}是预编译处理，${}是字符串替换。
        使用#{}可以有效的防止SQL注入，提高系统安全性。
        
    2.如何获取自动生成的(主)键值?
        通过LAST_INSERT_ID()获取刚插入记录的自增主键值，在insert语句执行后，执行select LAST_INSERT_ID()就可以获取自增主键。
            <insert id="insertUser" parameterType="cn.itcast.mybatis.po.User">
        		<selectKey keyProperty="id" order="AFTER" resultType="int">
        			select LAST_INSERT_ID()
        		</selectKey>
        		INSERT INTO USER(username,birthday,sex,address) VALUES(#{username},#{birthday},#{sex},#{address})
        	</insert>
        
    3.Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？
        Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能。
        Mybatis提供了9种动态sql标签：trim|where|set|foreach|if|choose|when|otherwise|bind。
        
    4.Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？
        如果配置了namespace那么当然是可以重复的，因为我们的Statement实际上就是namespace+id
        如果没有配置namespace的话，那么相同的id就会导致覆盖了。        
  
    5.为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？
        Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。
        而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。
        
    6.1+n问题是什么？应该怎样解决？
      1+n是执行一次查询获取n条主数据后，由于关联引起的执行n次查询从数据；它带来了性能问题；
      一般来说，通过懒加载,要使用的时候再查询,可以部分缓解1+n带来的性能问题
      
    *7.通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？
        Dao接口，就是人们常说的Mapper接口，接口的全限名，就是映射文件中的namespace的值，接口的方法名，就是映射文件
        中MappedStatement的id值，接口方法内的参数，就是传递给sql的参数。
        Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MappedStatement
        
        Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。
        Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，
        转而执行MappedStatement所代表的sql，然后将sql执行结果返回。
        
    8.Mybatis比IBatis比较大的几个改进是什么
        a.有接口绑定,包括注解绑定sql和xml绑定Sql ,
        b.动态sql由原来的节点配置变成OGNL表达式,
        c. 在一对一,一对多的时候引进了association,在一对多的时候引入了collection节点,不过都是在resultMap里面配置
        
    9.接口绑定有几种实现方式,分别是怎么实现的?
        接口绑定有两种实现方式：
            一种是通过注解绑定,就是在接口的方法上面加上@Select@Update等注解里面包含Sql语句来绑定
            另外一种就是通过xml里面写SQL来绑定,在这种情况下,要指定xml映射文件里面的namespace必须为接口的全路径名.
    
    10.Mybatis是如何进行分页的？分页插件的原理是什么？
        Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的
        参数来完成物理分页功能，也可以使用分页插件来完成物理分页。
        分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，
        添加对应的物理分页语句和物理分页参数。
        举例：select * from student，拦截sql后重写为：select t.* from （select * from student）t limit 0，10
        
        
    11.简述Mybatis的插件运行原理，以及如何编写一个插件
        Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，
            Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，
            就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。
        实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，
            别忘了在配置文件中配置你编写的插件。
            
    12.Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？
        Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。
            在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。
        它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，
            拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，
            于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。
        当然了，不光是Mybatis，几乎所有的包括Hibernate，支持延迟加载的原理都是一样的。
        
    13.Mybatis都有哪些Executor执行器？它们之间的区别是什么？
        Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。
        
        SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。
        ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，
            而是放置于Map<String, Statement>内，供下一次使用。简言之，就是重复使用Statement对象。
        BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行
            （executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。
        
        作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。
    14.mybatis一级缓存和二级缓存
        一级缓存的作用域是同一个SqlSession，在同一个sqlSession中两次执行相同的sql语句，第一次执行完毕会将数据库中查询的数据写到缓存（内存），
        第二次会从缓存中获取数据将不再从数据库查询，从而提高查询效率。当一个sqlSession结束后该sqlSession中的一级缓存也就不存在了
        。Mybatis默认开启一级缓存。
        
        二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession去操作数据库得到数据会存在二级缓存区域，
        多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。不同的sqlSession两次执行相同namespace下的sql语句且向sql中传递参数
        也相同即最终执行相同的sql语句，第一次执行完毕会将数据库中查询的数据写到缓存（内存），第二次会从缓存中获取数据将不再从数据库查询，
        从而提高查询效率。Mybatis默认没有开启二级缓存需要在setting全局参数中配置开启二级缓存.
        
###结束



    https://segmentfault.com/a/1190000013678579     (Mybatis常见面试题)
    Mybatis缓存(*)
        一级缓存的作用域是同一个SqlSession，在同一个sqlSession中两次执行相同的sql语句，第一次执行完毕会将数据库中查询的数据写到缓存（内存），
        第二次会从缓存中获取数据将不再从数据库查询，从而提高查询效率。当一个sqlSession结束后该sqlSession中的一级缓存也就不存在了
        。Mybatis默认开启一级缓存。
        二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession去操作数据库得到数据会存在二级缓存区域，
        多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。不同的sqlSession两次执行相同namespace下的sql语句且向sql中传递参数
        也相同即最终执行相同的sql语句，第一次执行完毕会将数据库中查询的数据写到缓存（内存），第二次会从缓存中获取数据将不再从数据库查询，
        从而提高查询效率。Mybatis默认没有开启二级缓存需要在setting全局参数中配置开启二级缓存
    
    如何获取自动生成的(主)键值?
        通过LAST_INSERT_ID()获取刚插入记录的自增主键值，在insert语句执行后，执行select LAST_INSERT_ID()就可以获取自增主键。
        
         <insert id="insertUser" parameterType="cn.itcast.mybatis.po.User">
                <selectKey keyProperty="id" order="AFTER" resultType="int">
                    select LAST_INSERT_ID()
                </selectKey>
                INSERT INTO USER(username,birthday,sex,address) VALUES(#{username},#{birthday},#{sex},#{address})
        </insert>
        
    Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？
        Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能。
        Mybatis提供了9种动态sql标签：trim|where|set|foreach|if|choose|when|otherwise|bind。
        其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能。
    
    通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？
        Dao接口，就是人们常说的Mapper接口，接口的全限名，就是映射文件中的namespace的值，接口的方法名，就是映射文件中MappedStatement
        的id值，接口方法内的参数，就是传递给sql的参数。
        Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MappedStatement
        
        举例：
        com.mybatis3.mappers.StudentDao.findStudentById，
        
        可以唯一找到namespace为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在Mybatis中，
        每一个<select>、<insert>、<update>、<delete>标签，都会被解析为一个MappedStatement对象。
        Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。
        
        Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，
        转而执行MappedStatement所代表的sql，然后将sql执行结果返回。
        
    接口绑定有几种实现方式,分别是怎么实现的?
        接口绑定有两种实现方式：
        
        一种是通过注解绑定,就是在接口的方法上面加上@Select@Update等注解里面包含Sql语句来绑定
        另外一种就是通过xml里面写SQL来绑定,在这种情况下,要指定xml映射文件里面的namespace必须为接口的全路径名.
        
    Mybatis是如何进行分页的？分页插件的原理是什么？
        Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的参数
        来完成物理分页功能，也可以使用分页插件来完成物理分页。
        
        分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，
        添加对应的物理分页语句和物理分页参数。
        
        举例：select * from student，拦截sql后重写为：select t.* from （select * from student）t limit 0，10
        
    简述Mybatis的插件运行原理，以及如何编写一个插件
        Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis使用JDK的
        动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是
        InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。
        
        实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，
        别忘了在配置文件中配置你编写的插件。
        
    Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？
        Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。
        在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。
        
        它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()
        方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性
        就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。
        
        当然了，不光是Mybatis，几乎所有的包括Hibernate，支持延迟加载的原理都是一样的。
        
    Mybatis都有哪些Executor执行器？它们之间的区别是什么？
        Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。
        
            SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。
            ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，
                而是放置于Map<String, Statement>内，供下一次使用。简言之，就是重复使用Statement对象。
            BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），
                等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行
                executeBatch()批处理。与JDBC批处理相同。
        作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。
        

    
    

##16 数据库

###目录
    0.  7种日志,索引,锁,引擎,sql优化,explain,主从,分库分表,扩容,事务隔离级别,大数据分页
    
    1.说一下mysql有哪些日志,有什么作用
        MySQL中有7种日志文件，分别是：重做日志（redo log）、回滚日志（undo log）、二进制日志（binlog）、错误日志（errorlog）、
        慢查询日志（slow query log）、一般查询日志（general log），中继日志（relay log）。
        
        1.redo log  (持久性体现)
            通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页
            (恢复数据页，且只能恢复到最后一次提交的位置)。为了保证事务的持久性.
            个人理解就是事务已经提交了,在写数据的时候没写完,恢复后要把数据恢复到最后提交事务的位置.
        2.undo log  (一致性体现)
            用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。事务还没提交,恢复到修改前的状态,事务的一致性.
            重做日志：每当有操作执行前，将数据真正更改时，先前相关操作写入重做日志。这样当断电，或者一些意外，导致后续任务无法完成时，
            系统恢复后，可以继续完成这些更改 
            撤消日志：当一些更改在执行一半时，发生意外，而无法完成，则可以根据撤消日志恢复到更改之前的壮态
    
            redo->记录所有操作，用于恢复（redo records all the database transaction used for recovery） 　　 
            undo->记录所有的前印象，用于回滚（undo is used to store uncommited data infor used for rollback） 　 
            redo->已递交的事务,实例恢复时要写到数据文件去的 
            undo->未递交的事务. 　 
        3.bin log
            作用：
                1，用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。
                2，用于数据库的基于时间点的还原。
                
        4.*binlog 和 redolog 的区别
              1，作用不同：redo log是保证事务的持久性的，是事务层面的，binlog作为还原的功能，是数据库层面的（当然也可以精确到事务层面的），
                  虽然都有还原的意思，但是其保护数据的层次是不一样的。
              2，内容不同：redo log是物理日志，是数据页面的修改之后的物理记录，binlog是逻辑日志，可以简单认为记录的就是sql语句
              3，另外，两者日志产生的时间，可以释放的时间，在可释放的情况下清理机制，都是完全不同的。
              4，恢复数据时候的效率，基于物理日志的redo log恢复数据的效率要高于语句逻辑日志的binlog
          
              关于事务提交时，redo log和binlog的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用binlog进行基于时间点还原的情况），是要严格一致的，
              MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，
              两个日志都提交成功（刷入磁盘），事务才算真正的完成。

        
    2.mvcc实现原理
        mvcc是多版本并发控制.适用了三列隐藏列,MVCC在大多数情况下代替了行锁，实现了对读的非阻塞，读不加锁，读写不冲突。缺点是每行记录都
        需要额外的存储空间，需要做更多的行维护和检查工作。并且解决了不可重复读和幻读的问题.多版本需要undolog协助.
        查询的时候只查询当前事务id比删除事务id小或者删除事务id为空的,还有一个条件是大于等于创建id
        
        在查询时要符合以下两个条件的记录才能被事务查询出来：
        1) 删除版本号未指定或者大于当前事务版本号，即查询事务开启后确保读取的行未被删除。(即上述事务id为2的事务查询时，依然能读取到事务id为3所删除的数据行)
        2) 创建版本号 小于或者等于 当前事务版本号 ，就是说记录创建是在当前事务中（等于的情况）或者在当前事务启动之前的其他事物进行的insert。
        
    3.什么是脏读？幻读？不可重复读？什么是事务的隔离级别？
        脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据
        不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，
            导致事务A多次读取同一数据时，结果 不一致。
        幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，
            当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。
            
    4.关键字执行顺序
        　　1. FROM：对FROM子句中前两个表执行笛卡尔积生成虚拟表vt1
        　　2. ON: 对vt1表应用ON筛选器只有满足 join_condition 为真的行才被插入vt2
        　　3. OUTER(join)：如果指定了 OUTER JOIN保留表(preserved table)中未找到的行将行作为外部行添加到vt2，生成t3，如果from包含两个以上表，则对上一个联结生成的结果表和下一个表重复执行步骤和步骤直接结束。
        　　4. WHERE：对vt3应用 WHERE 筛选器只有使 where_condition 为true的行才被插入vt4
        　　5. GROUP BY：按GROUP BY子句中的列列表对vt4中的行分组生成vt5
        　　6. CUBE|ROLLUP：把超组(supergroups)插入vt6，生成vt6
        　　7. HAVING：对vt6应用HAVING筛选器只有使 having_condition 为true的组才插入vt7
        　　8. SELECT：处理select列表产生vt8
        　　9. DISTINCT：将重复的行从vt8中去除产生vt9
        　　10. ORDER BY：将vt9的行按order by子句中的列列表排序生成一个游标vc10
            11.Limit
            
            CASE 
            WHEN wf.Fstep = 1 AND wf.Fstatus = 'Reject' and wf.Foper_reason IS NULL THEN '未填写驳回理由'
            WHEN wf.Fstep = 1 AND wf.Fstatus = 'Reject' and wf.Foper_reason IS NOT NULL THEN wf.Foper_reason
            END reason,
            
    5.大数据情况下如何做分页？
        SELECT * FROM student LIMIT 10000 , 10;
        实际底层执行是从第一行开始找到10010行，再抛弃前面的一万行。所以当用户往后翻到很多页的时候，
        offset这个值可能就比较大，实际执行效率就会很慢。
        
        优化方法1: (如果不是按id分就有问题,要用方法2)
            where语句限制一半，limit控制行数
            
            SELECT * FROM student WHERE ID >= 10000 LIMIT 10;
            这样的好处就是不仅效率高了，而且能刚好拿十条。但是这里依然还是有一个问题:不适用于所有情况。具体不适用与什么情况呢？
            简单来说就是id和行数不能对应的情况。比如ID本身是无规律离散的，那么计算这个起始ID就不能简单的pageIndex * pageSize了。
            
        优化方法2:
            利用"子查询/连接+索引"快速定位元组的位置,然后再读取元组. 道理同方法5
            先用子查询查出符合查询条件的id,然后用外层查询limit
                    如(id是主键/唯一键,蓝色字体时变量):
                    利用子查询示例:
                    SELECT * FROM your_table WHERE id <= 
                    (SELECT id FROM your_table ORDER BY id desc LIMIT ($page-1)*$pagesize,1)
                    ORDER BY id desc LIMIT $pagesize
                    
    6.如何设计可以动态扩容缩容的分库分表方案？       
      1.设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32库 * 32表，对于大部分公司来说，可能几年都够了。
      2.路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表
      3.扩容的时候，申请增加更多的数据库服务器，装好 mysql，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。
      4.由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。
      5.我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。
      6.重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。
      
      免迁移扩容(只有分库,没有分表的情况)
          采用双倍扩容策略，避免数据迁移。扩容前每个节点的数据，有一半要迁移至一个新增节点中，对应关系比较简单。 
          具体操作如下(假设已有 2 个节点 A/B，要双倍扩容至 A/A2/B/B2 这 4 个节点)：
      
          无需停止应用服务器；
          新增两个数据库 A2/B2 作为从库，设置主从同步关系为：A=>A2、B=>B2，直至主从数据同步完毕(早期数据可手工同步)；
          调整分片规则并使之生效： 
          原 ID%2=0 => A 改为 ID%4=0 => A, ID%4=2 => A2； 
          原 ID%2=1 => B 改为 ID%4=1 => B, ID%4=3 => B2。
          解除数据库实例的主从同步关系，并使之生效；
          此时，四个节点的数据都已完整，只是有冗余(多存了和自己配对的节点的那部分数据)，择机清除即可(过后随时进行，不影响业务)。
          
      *双写迁移方案   (这个比较实用)
      　　简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，都除了对老库增删改，都加上对新库的增删改，这就是所谓双写，同时写俩库，老库和新库。
      　　然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据gmt_modified这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。
      　　接着导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。
      　　接着当数据完全一致了，就ok了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间。

    8.分库分表后面临的问题?
        事务支持 
        跨库join
        跨节点的count,order by,group by以及聚合函数问题 
        数据迁移，容量规划，扩容等问题
        ID问题
            雪花算法,一位0表示正数,41时间戳,10位机房和机器,12位同毫秒时的增量
        跨分片的排序分页
        
    9.主从同步/读写分离
                  
          MySQL 主从复制原理的是啥？
              主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，
              写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，
              也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。
              
              这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。
              所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，
              从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，
              甚至几百毫秒才能读取到。
              
              而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，
              有些数据可能就丢失了。
    10.主从数据不一致,怎么解决?
          *所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；
          一个是并行复制，用来解决主从同步延时问题。
          
          1.(防止数据丢失)这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，
          从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。
          
          2.(减少延迟)所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。
            减少主库的写压力
            一般来说，如果主从延迟较为严重，有以下解决方案：
                分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
                打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。
                重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。
                如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你这么搞导致读写分离的意义就丧失了。
          
            show status
                查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。
              
          
          3.(数据不一致)基于gtid主从复制
              GTID：
              1）全局事务标识：global transaction identifiers。
              2）GTID是一个事务一一对应，并且全局唯一ID。
              3）一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱或者主从不一致。
              4）GTID用来代替传统复制方法，不再使用MASTER_LOG_FILE+MASTER_LOG_POS开启复制。而是使用MASTER_AUTO_POSTION=1的方式开始复制。
              5）MySQL-5.6.5开始支持的，MySQL-5.6.10后开始完善。
              6）在传统的slave端，binlog是不用开启的，但是在GTID中slave端的binlog是必须开启的，目的是记录执行过的GTID（强制）。
              
              组成：
              1）GTID = source_id：transaction_id
              2）source_id：用于鉴别原服务器，即mysql服务器唯一的的server_uuid，由于GTID会传递到slave，所以也可以理解为源ID。
              3）transaction_id：为当前服务器上已提交事务的一个序列号，通常从1开始自增长的序列，一个数值对应一个事务。
              
              原理：
              1）master更新数据时，会在事务前产生GTID，一同记录到binlog日志中。
              2）slave端的i/o 线程将变更的binlog，写入到本地的relay log中。
              3）sql线程从relay log中获取GTID，然后对比slave端的binlog是否有记录。
              4）如果有记录，说明该GTID的事务已经执行，slave会忽略。
              5）如果没有记录，slave就会从relay log中执行该GTID的事务，并记录到binlog。
              6）在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。
              
    11.mysql中in 和exists 区别
       mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。
       一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。
       
       1.如果查询的两个表大小相当，那么用in和exists差别不大。
       2.如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。
       3.not in 和not exists如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。
         所以无论那个表大，用not exists都比not in要快。
    
    12.mysql如何实现避免幻读
       在快照读读情况下，mysql通过mvcc来避免幻读。   
       在当前读读情况下，mysql通过next-key来避免幻读
       
       next-key = X锁加GAP锁
       快照读 = select * from table
       当前读 = select * from table lock in share mode = select * from table for update
       
    13.讲一下MySQL的索引
        https://juejin.im/post/5b55b842f265da0f9e589e79
            *聚集和非聚集索引
               聚集索引就是以主键创建的索引
               非聚集索引就是以非主键创建的索引
               
               聚集索引在叶子节点存储的是表中的数据
               非聚集索引在叶子节点存储的是主键和索引列
               使用非聚集索引查询出数据时，拿到叶子上的主键再去查到想要查找的数据。(拿到主键再查找这个过程叫做回表)
               非聚集索引也叫做二级索引
               
           索引主要数据结构
               Hash索引  
                    底层数据结构是哈希表,对于记录大多数查询是单条记录的场景,可以采用
                    *主流的还是使用B+树索引比较多，对于哈希索引，InnoDB是自适应哈希索引的
                    （hash索引的创建由InnoDB存储引擎引擎自动优化创建，我们干预不了）！
               B+tree索引 
                    两种引擎的的底层实现是不同的,myisam 叶子节点是内存地址的引用,innoDB是主键加记录的链表
               
           哈希索引的缺点    
               哈希索引也没办法利用索引完成排序
               不支持最左匹配原则
               在有大量重复键值情况下，哈希索引的效率也是极低的---->哈希碰撞问题。
               不支持范围查询
           
           覆盖索引
               创建多列索引中也涉及到了一种特殊的索引-->覆盖索引
               
               我们前面知道了，如果不是聚集索引，叶子节点存储的是主键+列值
               最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢
               覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！
               
           *最左匹配原则：
               索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。
               如果是联合索引，那么key也由多个列组成，同时，索引只能用于查找key是否存在（相等），
               遇到范围查询(>、<、between、like左匹配)等就不能进一步匹配了，后续退化为线性查找。
               因此，列的排列顺序决定了可命中索引的列数。
               
               例子：
               如有索引(a, b, c, d)，查询条件a = 1 and b = 2 and c > 3 and d = 4，则会在每个节点依次命中
               a、b、c，无法命中d。(很简单：索引命中只能是相等的情况，不能是范围匹配)
       
                *为什么有最左前缀原则
                    只有最左前缀是有序的,可以通过索引快速定位,,其他的都要全表扫描
           =、in自动优化顺序
               不需要考虑=、in等的顺序，mysql会自动优化这些条件的顺序，以匹配尽可能多的索引列。
               例子：
               
               如有索引(a, b, c, d)，查询条件c > 3 and b = 2 and a = 1 and d < 4与a = 1 and c > 3 and b = 2 and d < 4等
               顺序都是可以的，MySQL会自动优化为a = 1 and b = 2 and c > 3 and d < 4，依次命中a、b、c。
           
           *索引总结
               索引在数据库中是一个非常重要的知识点！上面谈的其实就是索引最基本的东西，要创建出好的索引要顾及到很多的方面：
               
               1，最左前缀匹配原则。这是非常重要、非常重要、非常重要（重要的事情说三遍）的原则，MySQL会一直向右匹配直到遇到范围查询（>,<,BETWEEN,LIKE）就停止匹配。
               3，尽量选择区分度高的列作为索引，区分度的公式是 COUNT(DISTINCT col) / COUNT(*)。表示字段不重复的比率，比率越大我们扫描的记录数就越少。
               4，索引列不能参与计算，尽量保持列“干净”。比如，FROM_UNIXTIME(create_time) = '2016-06-06' 就不能使用索引，原因很简单，B+树中存储的都是数据表中的字段值，但是进行检索时，需要把所有元素都应用函数才能比较，显然这样的代价太大。所以语句要写成 ： create_time = UNIX_TIMESTAMP('2016-06-06')。
               5，尽可能的扩展索引，不要新建立索引。比如表中已经有了a的索引，现在要加（a,b）的索引，那么只需要修改原来的索引即可。
               6，单个多列组合索引和多个单列索引的检索查询效果不同，因为在执行SQL时，MySQL只能使用一个索引，会从多个单列索引中选择一个限制最为严格的索引。
           
           选择索引和编写索引的原则
               1.一次查询尽可能多的包括所需要的行
               2.尽量用覆盖索引
               3.顺序I/O性能高
               
           
    14.怎么对进行sql优化
           1.负向查询不能使用索引
               select name from user where id not in (1,3,4);
               应该修改为:
               
               select name from user where id in (2,5,6);
           2.前导模糊查询不能使用索引
               如:
               
               select name from user where name like '%zhangsan'
               非前导则可以:
               
               select name from user where name like 'zhangsan%'
               建议可以考虑使用 Lucene 等全文索引工具来代替频繁的模糊查询。
               
           3.数据区分不明显的不建议创建索引
               如 user 表中的性别字段，可以明显区分的才建议创建索引，如身份证等字段。
               
           4.字段的默认值不要为 null
               这样会带来和预期不一致的查询结果。
               
           5.在字段上进行计算不能命中索引
               select name from user where FROM_UNIXTIME(create_time) < CURDATE();
               应该修改为:
               
               select name from user where create_time < FROM_UNIXTIME(CURDATE());
           6.最左前缀问题
               如果给 user 表中的 username pwd 字段创建了复合索引那么使用以下SQL 都是可以命中索引:
               
               select username from user where username='zhangsan' and pwd ='axsedf1sd'
               
               select username from user where pwd ='axsedf1sd' and username='zhangsan'
               
               select username from user where username='zhangsan'
               但是使用
               
               select username from user where pwd ='axsedf1sd'
               是不能命中索引的。
               
           7.如果明确知道只有一条记录返回
               select name from user where username='zhangsan' limit 1
               可以提高效率，可以让数据库停止游标移动。
               
           8.不要让数据库帮我们做强制类型转换
               select name from user where telno=18722222222
               这样虽然可以查出数据，但是会导致全表扫描。
               
               需要修改为
               
               select name from user where telno='18722222222'
               如果需要进行 join 的字段两表的字段类型要相同
               不然也不会命中索引。
               
    15.mysql引擎有哪些,有哪些区别
        · MyISAM：默认的MySQL插件式存储引擎(5.1以前)5.1版本以后默认引擎是innoDB，它是在Web、数据仓储和其他应用环境下最常使用的存储引擎之一。
            注意，通过更改STORAGE_ENGINE配置变量，能够方便地更改MySQL服务器的默认存储引擎。
        · InnoDB：用于事务处理应用程序，具有众多特性，包括ACID事务支持。(提供行级锁)
        · BDB：可替代InnoDB的事务引擎，支持COMMIT、ROLLBACK和其他事务特性。支持页锁
        · Memory：将所有数据保存在RAM中，在需要快速查找引用和其他类似数据的环境下，可提供极快的访问。
        · Merge：允许MySQL DBA或开发人员将一系列等同的MyISAM表以逻辑方式组合在一起，并作为1个对象引用它们。对于诸如数据仓储等VLDB环境十分适合。
        · Archive：为大量很少引用的历史、归档、或安全审计信息的存储和检索提供了完美的解决方案。
        · Federated：能够将多个分离的MySQL服务器链接起来，从多个物理服务器创建一个逻辑数据库。十分适合于分布式环境或数据集市环境。
        · Cluster/NDB：MySQL的簇式数据库引擎，尤其适合于具有高性能查找要求的应用程序，这类查找需求还要求具有最高的正常工作时间和可用性。
        · Other：其他存储引擎包括CSV（引用由逗号隔开的用作数据库表的文件），Blackhole（用于临时禁止对数据库的应用程序输入），以及Example引擎（可为快速创建定制的插件式存储引擎提供帮助）。
        一般来说不使用事务的话，请使用MyISAM引擎，使用事务的话，一般使用InnoDB
        
    16.InnoDB支持哈希索引吗？
       主流的还是使用B+树索引比较多，对于哈希索引，InnoDB是自适应哈希索引的（hash索引的创建由InnoDB存储引擎引擎自动优化创建，我们干预不了）！
       
    17.讲一讲mysql的锁
        锁
                锁的分类
                    按使用方式
                        乐观锁
                        悲观锁
                    按粒度
                        表级锁(MyIsam),又分为表共享锁和表排他锁
                        行级锁(InnoDB),又分共享锁和排他锁,都是间隙锁
                        页级锁(DBD) 
                        
                    这些锁数据库隐式帮我们加了
                    对于UPDATE、DELETE、INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)
                    MyISAM在执行查询语句SELECT前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT等）前，
                    会自动给涉及的表加写锁，这个过程并不需要用户干预
                
                锁介绍
                    表锁
                        开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突概率高，并发度最低
                    行锁
                        开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高
                    
                    不同的存储引擎支持的锁粒度是不一样的：
                        InnoDB行锁和表锁都支持！
                        MyISAM只支持表锁！
                    
                    *InnoDB只有通过索引条件检索数据才使用行级锁，否则，InnoDB将使用表锁
                    也就是说，InnoDB的行锁是基于索引的！
                    
                    表锁下又分为两种模式：
                        表读锁（Table Read Lock）
                        表写锁（Table Write Lock）
                        在表读锁和表写锁的环境下：读读不阻塞，读写阻塞，写写阻塞！
                  
               *InnoDB和MyISAM有两个本质的区别：
                   InnoDB支持行锁
                   InnoDB支持事务 
                
               为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁：
                   意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。
                   意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。
                   意向锁也是数据库隐式帮我们做了，不需要程序员操心！
               
               尽可能少遇到死锁：
               
                   1）以固定的顺序访问表和行。比如对两个job批量更新的情形，简单方法是对id列表先排序，后执行，这样就避免了交叉等待锁的情形；将两个事务的sql顺序调整为一致，也能避免死锁。
                   2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。
                   3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。
                   4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。
                   5）为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。
               
               MVCC(Multi-Version Concurrency Control)多版本并发控制，可以简单地认为：MVCC就是行级锁的一个变种(升级版)。
               
                   事务的隔离级别就是通过锁的机制来实现，只不过隐藏了加锁细节
                   在表锁中我们读写是阻塞的，基于提升并发性能的考虑，MVCC一般读写是不阻塞的(所以说MVCC很多情况下避免了加锁的操作)
                   MVCC实现的读写不阻塞正如其名：多版本并发控制--->通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot)，
                   并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度来看，好像是数据库可以提供同一数据的多个版本。
               
               间隙锁GAP
                    *间隙锁只会在Repeatable read隔离级别下使用~
                    当我们用范围条件检索数据而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合范围条件的已有数据记录的索引项加锁；
                    对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”。InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。

    18.explain和profile
    
      https://juejin.im/post/5c0891c7e51d4538994aefbd
      explain命令输出的结果有10列：id、select_type、table、type、possible_keys、key、key_len、ref、rows、Extra
      
      id
         如果id相同执行顺序由上至下。
         如果id不相同，id的序号会递增，id值越大优先级越高，越先被执行。
         (一般有子查询的SQL语句id就会不同)
      select_type
         SIMPLLE：简单查询，该查询不包含 UNION 或子查询
         PRIMARY：如果查询包含UNION 或子查询，则最外层的查询被标识为PRIMARY
         SUBQUERY：子查询中的第一个select语句(该子查询不在from子句中)
         DERIVED：包含在from子句中子查询(也称为派生表)
      table
         该列显示了对应行正在访问哪个表(有别名就显示别名)。
         当from子句中有子查询时，table列是 <derivenN>格式，表示当前查询依赖 id=N的查询，于是先执行 id=N 的查询
      *type
         该列称为关联类型或者访问类型，它指明了MySQL决定如何查找表中符合条件的行，同时是我们判断查询是否高效的重要依据。
         ALL：全表扫描，这个类型是性能最差的查询之一。通常来说，我们的查询不应该出现 ALL 类型，因为这样的查询，在数据量最大的情况下，对数据库的性能是巨大的灾难。
         index：全索引扫描，和 ALL 类型类似，只不过 ALL 类型是全表扫描，而 index 类型是扫描全部的索引，主要优点是避免了排序，但是开销仍然非常大。如果在 Extra 列看到 Using index，说明正在使用覆盖索引，只扫描索引的数据，它比按索引次序全表扫描的开销要少很多。
         range：范围扫描，就是一个有限制的索引扫描，它开始于索引里的某一点，返回匹配这个值域的行。这个类型通常出现在 =、<>、>、>=、<、<=、IS NULL、<=>、BETWEEN、IN() 的操作中，key 列显示使用了哪个索引，当 type 为该值时，则输出的 ref 列为 NULL，并且 key_len 列是此次查询中使用到的索引最长的那个。
         ref：一种索引访问，也称索引查找，它返回所有匹配某个单个值的行。此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了最左前缀规则索引的查询。
         eq_ref：使用这种索引查找，最多只返回一条符合条件的记录。在使用唯一性索引或主键查找时会出现该值，非常高效。
         const、system：该表至多有一个匹配行，在查询开始时读取，或者该表是系统表，只有一行匹配。其中 const 用于在和 primary key 或 unique 索引中有固定值比较的情形。
         NULL：在执行阶段不需要访问表。
         
      possible_keys
         这一列显示查询可能使用哪些索引来查找
      
      key
         这一列显示MySQL实际决定使用的索引。如果没有选择索引，键是NULL。
      
      key_len
         这一列显示了在索引里使用的字节数，当key列的值为 NULL 时，则该列也是 NULL
      
      ref
         这一列显示了哪些字段或者常量被用来和key配合从表中查询记录出来。
      
      rows
         这一列显示了估计要找到所需的行而要读取的行数，这个值是个估计值，原则上值越小越好。
      
      extra
         Using index：使用覆盖索引，表示查询索引就可查到所需数据，不用扫描表数据文件，往往说明性能不错。
         Using Where：在存储引擎检索行后再进行过滤，使用了where从句来限制哪些行将与下一张表匹配或者是返回给用户。
         Using temporary：在查询结果排序时会使用一个临时表，一般出现于排序、分组和多表 join 的情况，查询效率不高，建议优化。
         Using filesort：对结果使用一个外部索引排序，而不是按索引次序从表里读取行，一般有出现该值，都建议优化去掉，因为这样的查询 CPU 资源消耗大。
                 

###结束

    连接池
        druid
            配置        缺省值       	说明
            name	 	配置这个属性的意义在于，如果存在多个数据源，监控的时候可以通过名字来区分开来。 
            如果没有配置，将会生成一个名字，格式是："DataSource-" + System.identityHashCode(this)
            jdbcUrl	 	连接数据库的url，不同数据库不一样。例如： 
            mysql : jdbc:mysql://10.20.153.104:3306/druid2 
            oracle : jdbc:oracle:thin:@10.20.149.85:1521:ocnauto
            username	 	连接数据库的用户名
            password	 	连接数据库的密码。如果你不希望密码直接写在配置文件中，可以使用ConfigFilter。详细看这里：https://github.com/alibaba/druid/wiki/%E4%BD%BF%E7%94%A8ConfigFilter
            driverClassName	根据url自动识别	这一项可配可不配，如果不配置druid会根据url自动识别dbType，然后选择相应的driverClassName(建议配置下)
            initialSize	0	初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时
            maxActive	8	最大连接池数量
            maxIdle	8	已经不再使用，配置了也没效果
            minIdle	 	最小连接池数量
            maxWait	 	获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。
            poolPreparedStatements	false	是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql下建议关闭。
            maxOpenPreparedStatements	-1	要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题，可以把这个数值配置大一些，比如说100
            validationQuery	 	用来检测连接是否有效的sql，要求是一个查询语句。如果validationQuery为null，testOnBorrow、testOnReturn、testWhileIdle都不会其作用。
            testOnBorrow	true	申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。
            testOnReturn	false	归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能
            testWhileIdle	false	建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。
            timeBetweenEvictionRunsMillis	 	有两个含义： 
            1) Destroy线程会检测连接的间隔时间2) testWhileIdle的判断依据，详细看testWhileIdle属性的说明
            numTestsPerEvictionRun	 	不再使用，一个DruidDataSource只支持一个EvictionRun
            minEvictableIdleTimeMillis	 	 
            connectionInitSqls	 	物理连接初始化的时候执行的sql
            exceptionSorter	根据dbType自动识别	当数据库抛出一些不可恢复的异常时，抛弃连接
            filters	 	属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有： 
            监控统计用的filter:stat日志用的filter:log4j防御sql注入的filter:wall
            proxyFilters	 	
            类型是List<com.alibaba.druid.filter.Filter>，如果同时配置了filters和proxyFilters，是组合关系，并非替换关系
    
    *什么是MVCC
        https://blog.csdn.net/w2064004678/article/details/83012387  (讲得比较清楚)
        基础概念
            MVCC，Multi-Version Concurrency Control，多版本并发控制。MVCC 是一种并发控制的方法，一般在数据库管理系统中，
            实现对数据库的并发访问；在编程语言中实现事务内存。
            
            如果有人从数据库中读数据的同时，有另外的人写入数据，有可能读数据的人会看到『半写』或者不一致的数据。有很多种方法来解决这个问题，
            叫做并发控制方法。最简单的方法，通过加锁，让所有的读者等待写者工作完成，但是这样效率会很差。MVCC 使用了一种不同的手段，
            每个连接到数据库的读者，在某个瞬间看到的是数据库的一个快照，写者写操作造成的变化在写操作完成之前（或者数据库事务提交之前）
            对于其他的读者来说是不可见的。
        
        基本原理
            MVCC的实现，通过保存数据在某个时间点的快照来实现的。这意味着一个事务无论运行多长时间，在同一个事务里能够看到数据一致的视图。
            根据事务开始的时间不同，同时也意味着在同一个时刻不同事务看到的相同表里的数据可能是不同的。
            
        MVCC优缺点
            MVCC在大多数情况下代替了行锁，实现了对读的非阻塞，读不加锁，读写不冲突。缺点是每行记录都需要额外的存储空间，
            需要做更多的行维护和检查工作。
            
        隐藏列
            在每一行数据中额外保存两个隐藏的列：当前行创建时的版本号和删除时的版本号（可能为空，其实还有一列称为回滚指针，用于事务回滚，
            不在本文范畴）。这里的版本号并不是实际的时间值，而是系统版本号。每开始新的事务，系统版本号都会自动递增。事务开始时刻的系统
            版本号会作为事务的版本号，用来和查询每行记录的版本号进行比较。
            
        适用事务隔离级别
            MVCC手段只适用于Msyql隔离级别中的读已提交（Read committed）和可重复读（Repeatable Read）.
            避免了不可重复读和幻读,因为是快照读，别的事务进行添加等操作打上的id号不是早于当前事务的就不会被select
        

    
    MySQL中有六种日志文件，分别是：重做日志（redo log）、回滚日志（undo log）、二进制日志（binlog）、错误日志（errorlog）、
    慢查询日志（slow query log）、一般查询日志（general log），中继日志（relay log）。
    
    redo log和 undo log
    http://www.importnew.com/28039.html
        redo log 
            作用：
                确保事务的持久性。
                防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。
                很重要一点，redo log是什么时候写盘的？前面说了是在事物开始之后逐步写盘的。
                之所以说重做日志是在事务开始之后逐步写入重做日志文件，而不一定是事务提交才写入重做日志缓存，
                
        回滚日志（undo log）     
            作用：
                保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读
            内容：
                逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，这一点是不同于redo log的。
            什么时候产生：
                事务开始之前，将当前是的版本生成undo log，undo 也会产生 redo 来保证undo log的可靠性      
                undo是在事务开始之前保存的被修改数据的一个版本，产生undo日志的时候，同样会伴随类似于保护事务持久化机制的redolog的产生。

                
        1.redo log通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页
        (恢复数据页，且只能恢复到最后一次提交的位置)。
        2.undo用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。
        重做日志：每当有操作执行前，将数据真正更改时，先前相关操作写入重做日志。这样当断电，或者一些意外，导致后续任务无法完成时，
        系统恢复后，可以继续完成这些更改 
        撤消日志：当一些更改在执行一半时，发生意外，而无法完成，则可以根据撤消日志恢复到更改之前的壮态

        redo->记录所有操作，用于恢复（redo records all the database transaction used for recovery） 　　 
        undo->记录所有的前印象，用于回滚（undo is used to store uncommited data infor used for rollback） 　 
        redo->已递交的事务,实例恢复时要写到数据文件去的 
        undo->未递交的事务. 　 
        redo的原因是：每次commit时，将数据的修改立即写到online redo中，但是并不一定同时将该数据的修改写到数据文件中。
        因为该数据已经提交，但是只存在联机日志文件中，所以在恢复时需要将数据从联机日志文件中找出来，重新应用一下，
        使已经更改数据在数据文件中也改过来！ 
        undo的原因是：在oracle正常运行时，为了提高效率，加入用户还没有commit,但是空闲内存不多时，会由DBWR进程将脏块
        写入到数据文件中，以便腾出宝贵的内存供其它进程使用。这就是需要UNDO的原因。因为还没有发出commit语句，但是
        oracle的dbwr进程已经将没有提交的数据写到数据文件中去了。
        
    binlog
        作用：
        1，用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。
        2，用于数据库的基于时间点的还原。
        
        这里与redo log很明显的差异就是redo log并不一定是在事务提交的时候刷新到磁盘，redo log是在事务开始之后就开始逐步写入磁盘。
        
        
        MySQL的二进制日志可以说是MySQL最重要的日志了，它记录了所有的DDL和DML(除了数据查询语句)语句，以事件形式记录，还包含语句所执行的
        消耗的时间，MySQL的二进制日志是事务安全型的。
    
        一般来说开启二进制日志大概会有1%的性能损耗(参见MySQL官方中文手册 5.1.24版)。二进制有两个最重要的使用场景: 
            其一：MySQL Replication在Master端开启binlog，Mster把它的二进制日志传递给slaves来达到master-slave数据一致的目的。 
            其二：自然就是数据恢复了，通过使用mysqlbinlog工具来使恢复数据。
        
        二进制日志包括两类文件：二进制日志索引文件（文件名后缀为.index）用于记录所有的二进制文件，二进制日志文件（文件名后缀为.00000*）
        记录数据库所有的DDL和DML(除了数据查询语句)语句事件。 

    *binlog 和 redolog 的区别
        1，作用不同：redo log是保证事务的持久性的，是事务层面的，binlog作为还原的功能，是数据库层面的（当然也可以精确到事务层面的），
            虽然都有还原的意思，但是其保护数据的层次是不一样的。
        2，内容不同：redo log是物理日志，是数据页面的修改之后的物理记录，binlog是逻辑日志，可以简单认为记录的就是sql语句
        3，另外，两者日志产生的时间，可以释放的时间，在可释放的情况下清理机制，都是完全不同的。
        4，恢复数据时候的效率，基于物理日志的redo log恢复数据的效率要高于语句逻辑日志的binlog
    
        关于事务提交时，redo log和binlog的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用binlog进行基于时间点还原的情况），是要严格一致的，
        MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，
        两个日志都提交成功（刷入磁盘），事务才算真正的完成。
        
    什么是脏读？幻读？不可重复读？什么是事务的隔离级别？
        脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据
        不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，
        导致事务A多次读取同一数据时，结果 不一致。
        幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，
        当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。
    
    SQL的生命周期？关键字的先后顺序？
        1.应用服务器与数据库服务器建立一个连接
        2.数据库进程拿到请求sql
        3.解析并生成执行计划，执行
        4.读取数据到内存并进行逻辑处理
        5.通过步骤一的连接，发送结果到客户端
        
        
    *大数据情况下如何做分页？
    *https://www.cnblogs.com/geningchao/p/6649907.html
        ---方法3: 基于索引再排序
        ---语句样式: MySQL中,可用如下方法: SELECT * FROM 表名称 WHERE id_pk > (pageNum*10) ORDER BY id_pk ASC LIMIT M
        ---适应场景: 适用于数据量多的情况(元组数上万). 最好ORDER BY后的列对象是主键或唯一所以,使得ORDERBY操作能利用索引被消除但结果集是稳定的(稳定的含义,参见方法1)
        ---原因: 索引扫描,速度会很快. 但MySQL的排序操作,只有ASC没有DESC(DESC是假的,未来会做真正的DESC,期待...).
        
        ---方法4: 基于索引使用prepare（第一个问号表示pageNum，第二个？表示每页元组数）
        ---语句样式: MySQL中,可用如下方法: PREPARE stmt_name FROM SELECT * FROM 表名称 WHERE id_pk > (？* ？) ORDER BY id_pk ASC LIMIT M
        ---适应场景: 大数据量
        ---原因: 索引扫描,速度会很快. prepare语句又比一般的查询语句快一点。
        
        ---方法5: 利用MySQL支持ORDER操作可以利用索引快速定位部分元组,避免全表扫描
        比如: 读第1000到1019行元组(pk是主键/唯一键).
        SELECT * FROM your_table WHERE pk>=1000 ORDER BY pk ASC LIMIT 0,20
        
        ---方法6: 利用"子查询/连接+索引"快速定位元组的位置,然后再读取元组. 道理同方法5
        如(id是主键/唯一键,蓝色字体时变量):
        利用子查询示例:
        SELECT * FROM your_table WHERE id <= 
        (SELECT id FROM your_table ORDER BY id desc LIMIT ($page-1)*$pagesize ORDER BY id desc LIMIT $pagesize
        
        利用连接示例:
        SELECT * FROM your_table AS t1 
        JOIN (SELECT id FROM your_table ORDER BY id desc LIMIT ($page-1)*$pagesize AS t2 
        WHERE t1.id <= t2.id ORDER BY t1.id desc LIMIT $pagesize;
        
        mysql大数据量使用limit分页，随着页码的增大，查询效率越低下。

    *某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么?他们的原理知道么？
        垂直分表
            也就是“大表拆小表”，基于列字段进行的。一般是表中的字段较多，将不常用的， 数据较大，长度较长（比如text类型字段）的拆分到“扩展表“。
             一般是针对那种几百列的大表，也避免查询时，数据量太大造成的“跨页”问题。
            
            垂直分库针对的是一个系统中的不同业务进行拆分，比如用户User一个库，商品Producet一个库，订单Order一个库。 切分后，
            要放在多个服务器上，而不是一个服务器上。为什么？ 我们想象一下，一个购物网站对外提供服务，会有用户，商品，
            订单等的CRUD。没拆分之前， 全部都是落到单一的库上的，这会让数据库的单库处理能力成为瓶颈。按垂直分库后，
            如果还是放在一个数据库服务器上， 随着用户量增大，这会让单个数据库的处理能力成为瓶颈，还有单个服务器的磁盘空间，
            内存，tps等非常吃紧。 所以我们要拆分到多个服务器上，这样上面的问题都解决了，以后也不会面对单机资源问题。
            
            数据库业务层面的拆分，和服务的“治理”，“降级”机制类似，也能对不同业务的数据分别的进行管理，维护，监控，扩展等。
            数据库往往最容易成为应用系统的瓶颈，而数据库本身属于“有状态”的，相对于Web和应用服务器来讲，是比较难实现“横向扩展”的。
            数据库的连接资源比较宝贵且单机处理能力也有限，在高并发场景下，垂直分库一定程度上能够突破IO、连接数及单机硬件资源的瓶颈。
            
        水平分表
            针对数据量巨大的单张表（比如订单表），按照某种规则（RANGE,HASH取模等），切分到多张表里面去。 但是这些表还是在同一个库中，
            所以库级别的数据库操作还是有IO瓶颈。不建议采用。
            
            水平分库分表
                将单张表的数据切分到多个服务器上去，每个服务器具有相应的库与表，只是表中数据集合不同。 水平分库分表能够有效的缓解单机和
                单库的性能瓶颈和压力，突破IO、连接数、硬件资源等的瓶颈。
                
                水平分库分表切分规则
                RANGE从
                    0到10000一个表，10001到20000一个表；
                HASH取模
                    一个商场系统，一般都是将用户，订单作为主表，然后将和它们相关的作为附表，这样不会造成跨库事务之类的问题。
                     取用户id，然后hash取模，分配到不同的数据库上。
                地理区域
                    比如按照华东，华南，华北这样来区分业务，七牛云应该就是如此。
                时间
                    按照时间切分，就是将6个月前，甚至一年前的数据切出去放到另外的一张表，因为随着时间流逝，这些表的数据 
                    被查询的概率变小，所以没必要和“热数据”放在一起，这个也是“冷热数据分离”。
        
        
        *如何设计可以动态扩容缩容的分库分表方案？       
            1.设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32库 * 32表，对于大部分公司来说，可能几年都够了。
            2.路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表
            3.扩容的时候，申请增加更多的数据库服务器，装好 mysql，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。
            4.由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。
            5.我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。
            6.重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。
                    
        分库分表后面临的问题
            事务支持 
                分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 
                如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。
                
            跨库join
                只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。
                解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,
                根据这些id发起第二次请求得到关联数据。 分库分表方案产品
                
            跨节点的count,order by,group by以及聚合函数问题 
                这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。
                解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。
                和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。
                但如果结果集很大，对应用程序内存的消耗是一个问题。
                
            数据迁移，容量规划，扩容等问题
                来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，
                避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，
                这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。
                
            ID问题
                一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，
                某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,
                以便进行SQL路由. 一些常见的主键生成策略
                
                Twitter的分布式自增ID算法Snowflake
                    在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，
                    实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。
                    
                    snowflake 算法是 twitter 开源的分布式 id 生成算法，就是把一个 64 位的 long 型的 id，1 个 bit 是不用的，
                    用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。
                    1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。
                    41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示69年的时间。
                    10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2^5个机房（32个机房），每个机房里可以代表 2^5 个机器（32台机器）。
                    12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 2^12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。
                    0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000
                
            跨分片的排序分页
                一般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，
                而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，
                并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。
            
        *主从同步/读写分离
            
            MySQL 主从复制原理的是啥？
                主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，
                写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，
                也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。
                
                这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。
                所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，
                从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，
                甚至几百毫秒才能读取到。
                
                而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，
                有些数据可能就丢失了。
                
                *所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；
                一个是并行复制，用来解决主从同步延时问题。
                
                这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，
                从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。
                
                所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。
            
            show status

                查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。
                
                一般来说，如果主从延迟较为严重，有以下解决方案：
                
                分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
                打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。
                重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。
                如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你这么搞导致读写分离的意义就丧失了。

            
                *基于gtid主从复制
                    GTID：
                    1）全局事务标识：global transaction identifiers。
                    2）GTID是一个事务一一对应，并且全局唯一ID。
                    3）一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱或者主从不一致。
                    4）GTID用来代替传统复制方法，不再使用MASTER_LOG_FILE+MASTER_LOG_POS开启复制。而是使用MASTER_AUTO_POSTION=1的方式开始复制。
                    5）MySQL-5.6.5开始支持的，MySQL-5.6.10后开始完善。
                    6）在传统的slave端，binlog是不用开启的，但是在GTID中slave端的binlog是必须开启的，目的是记录执行过的GTID（强制）。
                    
                    组成：
                    1）GTID = source_id：transaction_id
                    2）source_id：用于鉴别原服务器，即mysql服务器唯一的的server_uuid，由于GTID会传递到slave，所以也可以理解为源ID。
                    3）transaction_id：为当前服务器上已提交事务的一个序列号，通常从1开始自增长的序列，一个数值对应一个事务。
                    
                    原理：
                    1）master更新数据时，会在事务前产生GTID，一同记录到binlog日志中。
                    2）slave端的i/o 线程将变更的binlog，写入到本地的relay log中。
                    3）sql线程从relay log中获取GTID，然后对比slave端的binlog是否有记录。
                    4）如果有记录，说明该GTID的事务已经执行，slave会忽略。
                    5）如果没有记录，slave就会从relay log中执行该GTID的事务，并记录到binlog。
                    6）在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。


                
            
        mysql中in 和exists 区别
            mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。
            一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。
            
            1.如果查询的两个表大小相当，那么用in和exists差别不大。
            2.如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。
            3.not in 和not exists如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。
              所以无论那个表大，用not exists都比not in要快。
            
    
    
    关掉连接，释放资源
    **索引(聚簇索引,非聚簇索引 根据引擎不同区分)
        索引的数据结构(b+树)
        https://www.cnblogs.com/dreamworlds/p/5398535.html
        
    *聚集和非聚集索引
        聚集索引就是以主键创建的索引
        非聚集索引就是以非主键创建的索引
        
        聚集索引在叶子节点存储的是表中的数据
        非聚集索引在叶子节点存储的是主键和索引列
        使用非聚集索引查询出数据时，拿到叶子上的主键再去查到想要查找的数据。(拿到主键再查找这个过程叫做回表)
        非聚集索引也叫做二级索引
        
    索引主要数据结构
        Hash索引  底层数据结构是哈希表,对于记录大多数查询是单条记录的场景,可以采用
        Btree索引 两种引擎的的底层实现是不同的,myisam 叶子节点是内存地址的引用,innoDB是主键加记录的链表
        
    哈希索引的缺点    
        哈希索引也没办法利用索引完成排序
        不支持最左匹配原则
        在有大量重复键值情况下，哈希索引的效率也是极低的---->哈希碰撞问题。
        不支持范围查询
    
    覆盖索引
        创建多列索引中也涉及到了一种特殊的索引-->覆盖索引
        
        我们前面知道了，如果不是聚集索引，叶子节点存储的是主键+列值
        最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢
        覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！
        
    *最左匹配原则：
    
        索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。
        如果是联合索引，那么key也由多个列组成，同时，索引只能用于查找key是否存在（相等），
        遇到范围查询(>、<、between、like左匹配)等就不能进一步匹配了，后续退化为线性查找。
        因此，列的排列顺序决定了可命中索引的列数。
        
        例子：
        如有索引(a, b, c, d)，查询条件a = 1 and b = 2 and c > 3 and d = 4，则会在每个节点依次命中
        a、b、c，无法命中d。(很简单：索引命中只能是相等的情况，不能是范围匹配)

    =、in自动优化顺序
        不需要考虑=、in等的顺序，mysql会自动优化这些条件的顺序，以匹配尽可能多的索引列。
        例子：
        
        如有索引(a, b, c, d)，查询条件c > 3 and b = 2 and a = 1 and d < 4与a = 1 and c > 3 and b = 2 and d < 4等
        顺序都是可以的，MySQL会自动优化为a = 1 and b = 2 and c > 3 and d < 4，依次命中a、b、c。
    
    *索引总结
        索引在数据库中是一个非常重要的知识点！上面谈的其实就是索引最基本的东西，要创建出好的索引要顾及到很多的方面：
        
        1，最左前缀匹配原则。这是非常重要、非常重要、非常重要（重要的事情说三遍）的原则，MySQL会一直向右匹配直到遇到范围查询（>,<,BETWEEN,LIKE）就停止匹配。
        3，尽量选择区分度高的列作为索引，区分度的公式是 COUNT(DISTINCT col) / COUNT(*)。表示字段不重复的比率，比率越大我们扫描的记录数就越少。
        4，索引列不能参与计算，尽量保持列“干净”。比如，FROM_UNIXTIME(create_time) = '2016-06-06' 就不能使用索引，原因很简单，B+树中存储的都是数据表中的字段值，但是进行检索时，需要把所有元素都应用函数才能比较，显然这样的代价太大。所以语句要写成 ： create_time = UNIX_TIMESTAMP('2016-06-06')。
        5，尽可能的扩展索引，不要新建立索引。比如表中已经有了a的索引，现在要加（a,b）的索引，那么只需要修改原来的索引即可。
        6，单个多列组合索引和多个单列索引的检索查询效果不同，因为在执行SQL时，MySQL只能使用一个索引，会从多个单列索引中选择一个限制最为严格的索引。
    
    选择索引和编写索引的原则
        1.一次查询尽可能多的包括所需要的行
        2.尽量用覆盖索引
        3.顺序I/O性能高
        
    
    *sql优化
        负向查询不能使用索引
        select name from user where id not in (1,3,4);
        应该修改为:
        
        select name from user where id in (2,5,6);
        前导模糊查询不能使用索引
        如:
        
        select name from user where name like '%zhangsan'
        非前导则可以:
        
        select name from user where name like 'zhangsan%'
        建议可以考虑使用 Lucene 等全文索引工具来代替频繁的模糊查询。
        
        数据区分不明显的不建议创建索引
        如 user 表中的性别字段，可以明显区分的才建议创建索引，如身份证等字段。
        
        字段的默认值不要为 null
        这样会带来和预期不一致的查询结果。
        
        在字段上进行计算不能命中索引
        select name from user where FROM_UNIXTIME(create_time) < CURDATE();
        应该修改为:
        
        select name from user where create_time < FROM_UNIXTIME(CURDATE());
        最左前缀问题
        如果给 user 表中的 username pwd 字段创建了复合索引那么使用以下SQL 都是可以命中索引:
        
        select username from user where username='zhangsan' and pwd ='axsedf1sd'
        
        select username from user where pwd ='axsedf1sd' and username='zhangsan'
        
        select username from user where username='zhangsan'
        但是使用
        
        select username from user where pwd ='axsedf1sd'
        是不能命中索引的。
        
        如果明确知道只有一条记录返回
        select name from user where username='zhangsan' limit 1
        可以提高效率，可以让数据库停止游标移动。
        
        不要让数据库帮我们做强制类型转换
        select name from user where telno=18722222222
        这样虽然可以查出数据，但是会导致全表扫描。
        
        需要修改为
        
        select name from user where telno='18722222222'
        如果需要进行 join 的字段两表的字段类型要相同
        不然也不会命中索引。
        
    
    *索引和锁基础知识
    *https://juejin.im/post/5b55b842f265da0f9e589e79    (必看重要)
        索引提高检索速度
        索引降低增删改的速度
        
    **锁
        锁的分类
            按使用方式
                乐观锁
                悲观锁
            按粒度
                表级锁(MyIsam),又分为表共享锁和表排他锁
                行级锁(InnoDB),又分共享锁和排他锁,都是间隙锁
                页级锁(DBD) 
                
                
        定心丸：即使我们不会这些锁知识，我们的程序在一般情况下还是可以跑得好好的。因为这些锁数据库隐式帮我们加了
            对于UPDATE、DELETE、INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)
            MyISAM在执行查询语句SELECT前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT等）前，
            会自动给涉及的表加写锁，这个过程并不需要用户干预
        
        锁介绍
            表锁
                开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突概率高，并发度最低
            行锁
                开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高
            
            不同的存储引擎支持的锁粒度是不一样的：
                InnoDB行锁和表锁都支持！
                MyISAM只支持表锁！
            
            *InnoDB只有通过索引条件检索数据才使用行级锁，否则，InnoDB将使用表锁
            也就是说，InnoDB的行锁是基于索引的！
            
            表锁下又分为两种模式：
                表读锁（Table Read Lock）
                表写锁（Table Write Lock）
                在表读锁和表写锁的环境下：读读不阻塞，读写阻塞，写写阻塞！
        
          
       *InnoDB和MyISAM有两个本质的区别：
           InnoDB支持行锁
           InnoDB支持事务 
        
       为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁：
       
           意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。
           意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。
           意向锁也是数据库隐式帮我们做了，不需要程序员操心！
       
       尽可能少遇到死锁：
       
           1）以固定的顺序访问表和行。比如对两个job批量更新的情形，简单方法是对id列表先排序，后执行，这样就避免了交叉等待锁的情形；将两个事务的sql顺序调整为一致，也能避免死锁。
           2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。
           3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。
           4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。
           5）为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。
       
       MVCC(Multi-Version Concurrency Control)多版本并发控制，可以简单地认为：MVCC就是行级锁的一个变种(升级版)。
       
           事务的隔离级别就是通过锁的机制来实现，只不过隐藏了加锁细节
           在表锁中我们读写是阻塞的，基于提升并发性能的考虑，MVCC一般读写是不阻塞的(所以说MVCC很多情况下避免了加锁的操作)
           MVCC实现的读写不阻塞正如其名：多版本并发控制--->通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot)，
           并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度来看，好像是数据库可以提供同一数据的多个版本。
       
       间隙锁GAP
            间隙锁只会在Repeatable read隔离级别下使用~
            当我们用范围条件检索数据而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合范围条件的已有数据记录的索引项加锁；
            对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”。InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。
       

       
    explain和profile
       https://juejin.im/post/5c0891c7e51d4538994aefbd
       explain命令输出的结果有10列：id、select_type、table、type、possible_keys、key、key_len、ref、rows、Extra
       
       id
          如果id相同执行顺序由上至下。
          如果id不相同，id的序号会递增，id值越大优先级越高，越先被执行。
          (一般有子查询的SQL语句id就会不同)
       select_type
          SIMPLLE：简单查询，该查询不包含 UNION 或子查询
          PRIMARY：如果查询包含UNION 或子查询，则最外层的查询被标识为PRIMARY
          SUBQUERY：子查询中的第一个select语句(该子查询不在from子句中)
          DERIVED：包含在from子句中子查询(也称为派生表)
       table
          该列显示了对应行正在访问哪个表(有别名就显示别名)。
          当from子句中有子查询时，table列是 <derivenN>格式，表示当前查询依赖 id=N的查询，于是先执行 id=N 的查询
       *type
          该列称为关联类型或者访问类型，它指明了MySQL决定如何查找表中符合条件的行，同时是我们判断查询是否高效的重要依据。
          ALL：全表扫描，这个类型是性能最差的查询之一。通常来说，我们的查询不应该出现 ALL 类型，因为这样的查询，在数据量最大的情况下，对数据库的性能是巨大的灾难。
          index：全索引扫描，和 ALL 类型类似，只不过 ALL 类型是全表扫描，而 index 类型是扫描全部的索引，主要优点是避免了排序，但是开销仍然非常大。如果在 Extra 列看到 Using index，说明正在使用覆盖索引，只扫描索引的数据，它比按索引次序全表扫描的开销要少很多。
          range：范围扫描，就是一个有限制的索引扫描，它开始于索引里的某一点，返回匹配这个值域的行。这个类型通常出现在 =、<>、>、>=、<、<=、IS NULL、<=>、BETWEEN、IN() 的操作中，key 列显示使用了哪个索引，当 type 为该值时，则输出的 ref 列为 NULL，并且 key_len 列是此次查询中使用到的索引最长的那个。
          ref：一种索引访问，也称索引查找，它返回所有匹配某个单个值的行。此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了最左前缀规则索引的查询。
          eq_ref：使用这种索引查找，最多只返回一条符合条件的记录。在使用唯一性索引或主键查找时会出现该值，非常高效。
          const、system：该表至多有一个匹配行，在查询开始时读取，或者该表是系统表，只有一行匹配。其中 const 用于在和 primary key 或 unique 索引中有固定值比较的情形。
          NULL：在执行阶段不需要访问表。
          
       possible_keys
          这一列显示查询可能使用哪些索引来查找
       
       key
          这一列显示MySQL实际决定使用的索引。如果没有选择索引，键是NULL。
       
       key_len
          这一列显示了在索引里使用的字节数，当key列的值为 NULL 时，则该列也是 NULL
       
       ref
          这一列显示了哪些字段或者常量被用来和key配合从表中查询记录出来。
       
       rows
          这一列显示了估计要找到所需的行而要读取的行数，这个值是个估计值，原则上值越小越好。
       
       extra
          Using index：使用覆盖索引，表示查询索引就可查到所需数据，不用扫描表数据文件，往往说明性能不错。
          Using Where：在存储引擎检索行后再进行过滤，使用了where从句来限制哪些行将与下一张表匹配或者是返回给用户。
          Using temporary：在查询结果排序时会使用一个临时表，一般出现于排序、分组和多表 join 的情况，查询效率不高，建议优化。
          Using filesort：对结果使用一个外部索引排序，而不是按索引次序从表里读取行，一般有出现该值，都建议优化去掉，因为这样的查询 CPU 资源消耗大。
          
          
          
          
          
       
    
        
    *总结
    https://github.com/Snailclimb/JavaGuide/blob/master/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/MySQL%20Index.md
    *https://mp.weixin.qq.com/s?__biz=MzU3MTQwNDEyMg==&mid=2247483712&idx=1&sn=69d682415757d739437a81e46614010a&chksm=fce1fb75cb967263ec343596a94f52fcff84aeb56f95ced0534fdb739c2eab453f027f07a1c8&token=1140785409&lang=zh_CN#rd
    *https://juejin.im/post/5ba1f32ee51d450e805b43f2#heading-5
        1.为什么索引能提高查询速度?
        2.索引的优点这么多,为什么不每列都建立索引呢?
        3.索引是如何提高查询速度的?
        4.sql可以从哪些地方进行优化?
        5.mysql索引主要使用哪两种数据结构?
        6.myIsam和innoDB Btree实现方式的区别?
        7.什么是覆盖索引?
        8.选择索引和编写索引的原则?
        9.什么是脏读？幻读？不可重复读？什么是事务的隔离级别？
        10.大数据情况下如何做分页？
        11.某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么?他们的原理知道么？
        12.你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？
        13.你能回答下MySQL有哪些引擎和各种引擎的执行原理，以及MySQL的呈现原理
        14.如何设计可以动态扩容缩容的分库分表方案？
        
        
             
##17 缓存

###目录
    -1
    0. LRU和LFU,redis实现分布式锁,redis锁和zk锁的优劣,SDS的优点,redis有哪些数据结构,Redis服务器的数据库数据结构,
        Redis对过期键的处理,内存淘汰策略,Redis持久化策略(RDB和AOF),Redis单线程为什么快？,redis主从(完整重同步,部分同步),
        redis的哨兵(Sentinal)机制,缓存雪崩,缓存穿透,缓存与数据库双写一致,Redis Cluster,guava cache,Redis与Memcached的区别

    1.LRU和LFU
        https://blog.csdn.net/weixin_41835916/article/details/81907022
        LFU
            算法根据数据的历史访问频率来淘汰数据，其核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。 
            LFU的每个数据块都有一个引用计数，所有数据块按照引用计数排序，具有相同引用计数的数据块则按照时间排序。 
        
    2.redis实现分布式锁
        实现的原理
        　　Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系。
            redis的SETNX命令可以方便的实现分布式锁。
        加锁方法步骤
            1.SETNX lock.foo <current Unix time + lock timeout + 1> 
                value为:当前时间戳 + 超时时间
                如果返回1表示获取到锁,返回0表示没获取到锁
            2.如果为0,没获取到锁,get key 检查是否超时,
            3.如果超时getSet返回旧值,和前面get获取的值比较,如果相等说明获取到锁了
            4.如果没有获取到锁,sleep一会,重新进入循环(直到超过设置的超时时间)
            5.如果超时return false
            
        释放锁步骤
            delete(key)
            
        说明:
            C3发送SETNX lock.foo 想要获得锁，由于C0还持有锁，所以Redis返回给C3一个0 
            C3发送GET lock.foo 以检查锁是否超时了，如果没超时，则等待或重试。 
            反之，如果已超时，C3通过下面的操作来尝试获得锁： 
            GETSET lock.foo <current Unix time + lock timeout + 1> 
            通过GETSET，C3拿到的时间戳如果仍然是超时的，那就说明，C3如愿以偿拿到锁了。 
            如果在C3之前，有个叫C4的客户端比C3快一步执行了上面的操作，那么C3拿到的时间戳是个未超时的值，这时，C3没有如期获得锁，
            需要再次等待或重试。留意一下，尽管C3没拿到锁，但它改写了C4设置的锁的超时值，不过这一点非常微小的误差带来的影响可以忽略不计。 
            
    3.redis锁和zk锁的优劣
        https://blog.csdn.net/A_BlackMoon/article/details/85834810
        redis分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能；
        zk分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。
        另外一点就是，如果是redis获取锁的那个客户端bug了或者挂了，那么只能等待超时时间之后才能释放锁；而zk的话，因为创建的是临时znode，
        只要客户端挂了，znode就没了，此时就自动释放锁。
        redis分布式锁大家每发现好麻烦吗？遍历上锁，计算时间等等。zk的分布式锁语义清晰实现简单。
        所以先不分析太多的东西，就说这两点，我个人实践认为zk的分布式锁比redis的分布式锁牢靠、而且模型简单易用。
        
    4.SDS的优点
        SDS与C的字符串表示比较
            sdshdr数据结构中用len属性记录了字符串的长度。那么获取字符串的长度时，时间复杂度只需要O(1)。
            SDS不会发生溢出的问题，如果修改SDS时，空间不足。先会扩展空间，再进行修改！(内部实现了动态扩展机制)。
            SDS可以减少内存分配的次数(空间预分配机制)。在扩展空间时，除了分配修改时所必要的空间，还会分配额外的空闲空间(free 属性)。
            SDS是二进制安全的，所有SDS API都会以处理二进制的方式来处理SDS存放在buf数组里的数据。
    
    5.redis有哪些数据结构
        string-->简单的key-value
        list-->有序列表(底层是双向链表)-->可做简单队列
        set-->无序列表(去重)-->提供一系列的交集、并集、差集的命令
        hash-->哈希表-->存储结构化数据
        sortset-->有序集合映射(member-score)-->排行榜
        
    6.Redis服务器的数据库
        Redis服务器用redisServer结构体来表示，其中redisDb是一个数组，用来保存所有的数据库，dbnum代表数据库的数量(这个可以配置，默认是16)
        
        Redis的数据库就是使用字典(哈希表)来作为底层实现的，对数据库的增删改查都是构建在字典(哈希表)的操作之上的。
        typedef struct redisDb { 
            int id;         // 数据库ID标识
            dict *dict;     // 键空间，存放着所有的键值对              
            dict *expires;  // 过期哈希表，保存着键的过期时间                          
            dict *watched_keys; // 被watch命令监控的key和相应client    
            long long avg_ttl;  // 数据库内所有键的平均TTL（生存时间）     
        } redisDb;

    7.Redis对过期键的处理
          因为我们的内存是有限的。所以我们会干掉不常用的数据，保留常用的数据。这就需要我们设置一下键的过期(生存)时间了。
              设置键的生存时间可以通过EXPIRE或者PEXPIRE命令。
              设置键的过期时间可以通过EXPIREAT或者PEXPIREAT命令。
              其实EXPIRE、PEXPIRE、EXPIREAT这三个命令都是通过PEXPIREAT命令来实现的。
              我们在redisDb结构体中还发现了dict *expires;属性，存放所有键过期的时间。
          
          既然有设置过期(生存)时间的命令，那肯定也有移除过期时间，查看剩余生存时间的命令了：
              PERSIST(移除过期时间)
              TTL(Time To Live)返回剩余生存时间，以秒为单位
              PTTL以毫秒为单位返回键的剩余生存时间 
              
          *过期策略
              那这些过期键到了过期的时间，就会立马被删除掉吗？？
                  删除策略可分为三种
                  
                      定时删除(对内存友好，对CPU不友好),到时间点上就把所有过期的键删除了。
                      惰性删除(对CPU极度友好，对内存极度不友好),每次从键空间取键的时候，判断一下该键是否过期了，如果过期了就删除。
                      定期删除(折中),每隔一段时间去删除过期键，限制删除的执行时长和频率。
                  
                      *Redis采用的是惰性删除+定期删除两种策略，所以说，在Redis里边如果过期键到了过期的时间了，未必被立马删除的！
                  
          *内存淘汰机制 
              如果定期删除漏掉了很多过期key，也没及时去查(没走惰性删除)，大量过期key堆积在内存里，导致redis内存块耗尽了，咋整？
                  我们可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。
                  
                  allkeys-lru
                      使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。
                      可以将内存最大使用量设置为热点数据占用的内存量，然后启用allkeys-lru淘汰策略，
                      将最近最少使用的数据淘汰
                  
                  volatile-lru
                      过期键中最近最少使用的淘汰  
                      
                      
    8.*Redis持久化策略(RDB和AOF)
          Redis是基于内存的，如果不想办法将数据保存在硬盘上，一旦Redis重启(退出/故障)，内存的数据将会全部丢失。
          
              我们肯定不想Redis里头的数据由于某些故障全部丢失(导致所有请求都走MySQL)，
              即便发生了故障也希望可以将Redis原有的数据恢复过来，这就是持久化的作用。
          
          Redis提供了两种不同的持久化方法来讲数据存储到硬盘里边：
          
              RDB(基于快照)，将某一时刻的所有数据保存到一个RDB文件中。
              AOF(append-only-file)，当Redis服务器执行写命令的时候，将执行的写命令保存到AOF文件中。
          
          RDB(快照持久化)
              RDB持久化可以手动执行，也可以根据服务器配置定期执行。RDB持久化所生成的RDB文件是一个经过压缩的二进制文件，
              Redis可以通过这个文件还原数据库的数据。
          
              有两个命令可以生成RDB文件：
                  SAVE会阻塞Redis服务器进程，服务器不能接收任何请求，直到RDB文件创建完毕为止。
                  BGSAVE创建出一个子进程，由子进程来负责创建RDB文件，服务器进程可以继续接收请求。
              
              Redis服务器在启动的时候，如果发现有RDB文件，就会自动载入RDB文件(不需要人工干预)
              服务器在载入RDB文件期间，会处于阻塞状态，直到载入工作完成。
              
              除了手动调用SAVE或者BGSAVE命令生成RDB文件之外，我们可以使用配置的方式来定期执行：
              在默认的配置下，如果以下的条件被触发，就会执行BGSAVE命令
                  save 900 1              #在900秒(15分钟)之后，至少有1个key发生变化，
                  save 300 10            #在300秒(5分钟)之后，至少有10个key发生变化
                  save 60 10000        #在60秒(1分钟)之后，至少有10000个key发生变化
              复制代码原理大概就是这样子的(结合上面的配置来看)：
              
              struct redisServer{
                // 修改计数器
                long long dirty;
              
                // 上一次执行保存的时间
                time_t lastsave;
              
                // 参数的配置
                struct saveparam *saveparams;
              };
              
              复制代码遍历参数数组，判断修改次数和时间是否符合，如果符合则调用besave()来生成RDB文件
              总结：通过手动调用SAVE或者BGSAVE命令或者配置条件触发，将数据库某一时刻的数据快照，生成RDB文件实现持久化。
              
              
          AOF(文件追加)
              从前面的示例看出，我们写了三条命令，AOF文件就保存了三条命令。如果我们的命令是这样子的：
              
              redis > RPUSH list "Java" "3y"
              (integer)2
              
              redis > RPUSH list "Java3y"
              integer(3)
              
              redis > RPUSH list "yyy"
              integer(4)
              
              复制代码同样地，AOF也会保存3条命令。我们会发现一个问题：上面的命令是可以合并起来成为1条命令的，并不需要3条。这样就可以让AOF文件的体积变得更小。
              AOF重写由Redis自行触发(参数配置)，也可以用BGREWRITEAOF命令手动触发重写操作。
              
          *    要值得说明的是：AOF重写不需要对现有的AOF文件进行任何的读取、分析。AOF重写是通过读取服务器当前数据库的数据来实现的！
              新的AOF文件的命令如下，没有一条是多余的！
              
          *AOF后台重写
              Redis将AOF重写程序放到子进程里执行(BGREWRITEAOF命令)，像BGSAVE命令一样fork出一个子进程来完成重写AOF的操作，从而不会影响到主进程。
              AOF后台重写是不会阻塞主进程接收请求的，新的写命令请求可能会导致当前数据库和重写后的AOF文件的数据不一致！
              为了解决数据不一致的问题，Redis服务器设置了一个AOF重写缓冲区，这个缓存区会在服务器创建出子进程之后使用。
              
              
    9.RDB和AOF对过期键的策略
          RDB持久化对过期键的策略：
              执行SAVE或者BGSAVE命令创建出的RDB文件，程序会对数据库中的过期键检查，已过期的键不会保存在RDB文件中。
              载入RDB文件时，程序同样会对RDB文件中的键进行检查，过期的键会被忽略。
          
          AOF持久化对过期键的策略：
              如果数据库的键已过期，但还没被惰性/定期删除，AOF文件不会因为这个过期键产生任何影响(也就说会保留)，当过期的键被删除了以后，
              会追加一条DEL命令来显示记录该键被删除了
              重写AOF文件时，程序会对RDB文件中的键进行检查，过期的键会被忽略。
          
          复制模式：
              主服务器来控制从服务器统一删除过期键(保证主从服务器数据的一致性)
              
    10.*RDB和AOF用哪个(优点和缺点)？
          rdb是Redis DataBase缩写 功能核心函数rdbSave(生成RDB文件)和rdbLoad（从文件加载内存）
          Aof是Append-only file缩写 
          
          RDB和AOF并不互斥，它俩可以同时使用。
          
              RDB的优点：载入时恢复数据快、文件体积小。
              RDB的缺点：会一定程度上丢失数据(因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。)
              AOF的优点：丢失数据少(默认配置只丢失一秒的数据)。
              AOF的缺点：恢复数据相对较慢，文件体积大
              
              aof文件比rdb更新频率高，优先使用aof还原数据。
              aof比rdb更安全也更大
              rdb性能比aof好
              如果两个都配了优先加载AOF
          
          如果Redis服务器同时开启了RDB和AOF持久化，服务器会优先使用AOF文件来还原数据(因为AOF更新频率比RDB更新频率要高，还原的数据更完善)
          
          可能涉及到RDB和AOF的配置：
          
              redis持久化，两种方式
              1、rdb快照方式
              2、aof日志方式
              
              ----------rdb快照------------
              save 900 1
              save 300 10
              save 60 10000
              
              stop-writes-on-bgsave-error yes
              rdbcompression yes
              rdbchecksum yes
              dbfilename dump.rdb
              dir /var/rdb/
              
              -----------Aof的配置-----------
              appendonly no # 是否打开 aof日志功能
              
              appendfsync always #每一个命令都立即同步到aof，安全速度慢
              appendfsync everysec
              appendfsync no 写入工作交给操作系统，由操作系统判断缓冲区大小，统一写入到aof  同步频率低，速度快
              
              no-appendfsync-on-rewrite yes 正在导出rdb快照的时候不要写aof
              auto-aof-rewrite-percentage 100
              auto-aof-rewrite-min-size 64mb 
          
              ./bin/redis-benchmark -n 20000
              
    11.Redis单线程为什么快？
       1）纯内存操作
       2）核心是基于非阻塞的IO多路复用机制
       3）单线程避免了多线程的频繁上下文切换问题
       
    12.redis主从     (完整重同步,部分同步)   
       主从架构的特点
           下面我们来看看Redis的主从架构特点：
           
           主服务器负责接收写请求
           从服务器负责接收读请求
           从服务器的数据由主服务器复制过去。主从服务器的数据是一致的
           
       主从架构的好处：
           读写分离(主服务器负责写，从服务器负责读)
           高可用(某一台从服务器挂了，其他从服务器还能继续接收请求，不影响服务)
           处理更多的并发量(每台从服务器都可以接收读请求，读QPS就上去了)
          
       redis复制
           在Redis中，用户可以通过执行SALVEOF命令或者设置salveof选项，让一个服务器去复制(replicate)另一个服务器，
           我们称呼被复制的服务器为主服务器(master)，而对主服务器进行复制的服务器则被称为从服务器(salve)
           
           1.同步(sync)
               将从服务器的数据库状态更新至主服务器的数据库状态
           2.命令传播(command propagate)
               主服务器的数据库状态被修改，导致主从服务器的数据库状态不一致，让主从服务器的数据库状态重新回到一致状态

           从服务器对主服务器的同步又可以分为两种情况：
           
               初次同步：从服务器没有复制过任何的主服务器，或者从服务器要复制的主服务器跟上次复制的主服务器不一样。
               断线后同步：处于命令传播阶段的主从服务器因为网络原因中断了复制，从服务器通过自动重连重新连接主服务器，并继续复制主服务器
           
           *Redis从2.8版本开始，使用PSYNC命令来替代SYNC命令执行复制时同步的操作。
           
           PSYNC命令具有完整重同步和部分重同步两种模式(其实就跟上面所说的初次复制和断线后复制差不多个意思)。
           完整重同步
               下面先来看看完整重同步是怎么实现的：
               
               从服务器向主服务器发送PSYNC命令
               收到PSYNC命令的主服务器执行BGSAVE命令，在后台生成一个RDB文件。并用一个缓冲区来记录从现在开始执行的所有写命令。
               当主服务器的BGSAVE命令执行完后，将生成的RDB文件发送给从服务器，从服务器接收和载入RBD文件。将自己的数据库状态更新至与主服务器执行BGSAVE命令时的状态。
               主服务器将所有缓冲区的写命令发送给从服务器，从服务器执行这些写命令，达到数据最终一致性。
           
           部分重同步
               接下来我们来看看部分重同步，部分重同步可以让我们断线后重连只需要同步缺失的数据(而不是Redis2.8之前的同步全部数据)，这是符合逻辑的！
               部分重同步功能由以下部分组成：
               
               主从服务器的复制偏移量
               主服务器的复制积压缓冲区
               服务器运行的ID(run ID)
               
    13.redis的哨兵(Sentinal)机制
       https://juejin.im/post/5c061de7e51d451dcd3c2ab5
                    
           Redis提供了哨兵(Sentinal)机制供我们解决上面的情况。如果主服务器挂了，我们可以将从服务器升级为主服务器，
           等到旧的主服务器(挂掉的那个)重连上来，会将它(挂掉的主服务器)变成从服务器。
           这个过程叫做主备切换(故障转移)                
           
           主服务器挂了，主从复制操作就中止了，并且哨兵系统是可以察觉出主服务挂了。
           Redis提供哨兵机制可以将选举一台从服务器变成主服务器
           然后旧的主服务器如果重连了，会变成从服务器
           
           首先我们要知道的是：Sentinel本质上只是一个运行在特殊模式下的Redis服务器。
           因为Sentinel做的事情和Redis服务器是不一样的，所以它们的初始化是有所区别的
           (比如，Sentinel在初始化的时候并不会载入AOF/RDB文件，因为Sentinel根本就不用数据库)。
           在启动的时候会将普通Redis服务器的代码替换成Sentinel专用代码。(所以Sentinel虽然作为Redis服务器，
           但是它不能执行SET、DBSIZE等等命令，因为命令表的代码被替换了
           
           Sentinel会创建两个连向主服务器的网络连接：
           
           命令连接(发送和接收命令)
           订阅连接(订阅主服务器的_sentinel_:hello频道)

       判断主服务器是否下线了
       判断主服务器是否下线有两种情况：
       
           主观下线
               Sentinel会以每秒一次的频率向与它创建命令连接的实例(包括主从服务器和其他的Sentinel)发送PING命令，通过PING命令返回的信息判断实例是否在线
               如果一个主服务器在down-after-milliseconds毫秒内连续向Sentinel发送无效回复，那么当前Sentinel就会主观认为该主服务器已经下线了。
       
       
           客观下线
               当Sentinel将一个主服务器判断为主观下线以后，为了确认该主服务器是否真的下线，它会向同样监视该主服务器的Sentinel询问，看它们是否也认为该主服务器是否下线。
               如果足够多的Sentinel认为该主服务器是下线的，那么就判定该主服务为客观下线，并对主服务器执行故障转移操作。
       
       
       选举领头Sentinel和故障转移
           选举领头Sentinel的规则也比较多，总的来说就是先到先得(哪个快，就选哪个)
           
       选举出领头的Sentinel之后，领头的Sentinel会对已下线的主服务器执行故障转移操作，包括三个步骤：
       
           在已下线主服务器属下的从服务器中，挑选一个转换为主服务器
           让已下线主服务器属下的所有从服务器改为复制新的主服务器
           已下线的主服务器重新连接时，让他成为新的主服务器的从服务器
       
           挑选某一个从服务器作为主服务器也是有策略的，大概如下：
       
           （1）跟master断开连接的时长
           （2）slave优先级
           （3）复制offset
           （4）run id
       
       tips:目前为止的主从+哨兵架构可以说Redis是高可用的，但要清楚的是：Redis还是会丢失数据的
       
           异步复制导致的数据丢失
               有部分数据还没复制到从服务器，主服务器就宕机了，此时这些部分数据就丢失了
           
           
           脑裂导致的数据丢失
               有时候主服务器脱离了正常网络，跟其他从服务器不能连接。此时哨兵可能就会认为主服务器下线了
               (然后开启选举，将某个从服务器切换成了主服务器)，但是实际上主服务器还运行着。这个时候，
               集群里就会有两个服务器(也就是所谓的脑裂)。
               虽然某个从服务器被切换成了主服务器，但是可能客户端还没来得及切换到新的主服务器，客户端还继续写向旧主服务器写数据。
               旧的服务器重新连接时，会作为从服务器复制新的主服务器(这意味着旧数据丢失)。
               
    14.缓存雪崩
         如何解决缓存雪崩？
           1.对缓存数据设置相同的过期时间，导致某段时间内缓存失效，请求全部走数据库。
               解决方法：在缓存的时候给过期时间加上一个随机值，这样就会大幅度的减少缓存在同一时间过期。
           
           2.对于“Redis挂掉了，请求全部走数据库”这种情况，我们可以有以下的思路：
               事发前：实现Redis的高可用(主从架构+Sentinel 或者Redis Cluster)，尽量避免Redis挂掉这种情况发生。
               事发中：万一Redis真的挂了，我们可以设置本地缓存(ehcache)+限流(hystrix)，
               尽量避免我们的数据库被干掉(起码能保证我们的服务还是能正常工作的)
               事发后：redis持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据。
               
    15.什么是缓存穿透?   
           缓存穿透是指查询一个一定不存在的数据。由于缓存不命中，并且出于容错考虑，如果从数据库查不到数据则不写入缓存，
           这将导致这个不存在的数据每次请求都要到数据库去查询，失去了缓存的意义。
           请求的数据在缓存大量不命中，导致请求走数据库。
           
           解决缓存穿透也有两种方案：
               1.由于请求的参数是不合法的(每次都请求不存在的参数)，于是我们可以使用布隆过滤器(BloomFilter)或者压缩filter提前拦截，
                不合法就不让这个请求到数据库层！
               2.当我们从数据库找不到的时候，我们也将这个空对象设置到缓存里边去。下次再请求的时候，就可以从缓存里边获取了。
                这种情况我们一般会将空对象设置一个较短的过期时间。
           
    *16.缓存与数据库双写一致
           一般我们对读操作的时候有这么一个固定的套路：
               如果我们的数据在缓存里边有，那么就直接取缓存的。
               如果缓存里没有我们想要的数据，我们会先去查询数据库，然后将数据库查出来的数据写到缓存中。
               最后将数据返回给请求
               
       什么是缓存与数据库双写一致问题？    
           对于缓存在更新时而言，都是建议执行删除操作！
           
           如果原子性被破坏了：
           对比两种策略
           https://blog.csdn.net/hukaijun/article/details/81010475
               我们可以发现，两种策略各自有优缺点：
           
               1.先更新数据库,再跟新缓存   (这种线程不安全)
                    （1）线程A更新了数据库
                    （2）线程B更新了数据库
                    （3）线程B更新了缓存
                    （4）线程A更新了缓存
                    脏数据覆盖了新数据
                    
               2.先删除缓存，再更新数据库
                   在高并发下表现不如意，在原子性被破坏时表现优异
           
               *3,先更新数据库，再删除缓存(Cache Aside Pattern设计模式)  (推荐,体现懒加载思想,不浪费空间)
                   在高并发下表现优异，在原子性被破坏时表现不如意
                   这种方法也有数据不一致问题,解决方法,在更新操作后,新启动一个线程删除,sleep(操作时间)后删除缓存,
                   如果删除失败则丢进消息队列,重试到删除为止.
                   
    17.Redis Cluster深入与实践
                
            AOF文件刷新的方式，有三种，参考配置参数appendfsync ：
                appendfsync always每提交一个修改命令都调用fsync刷新到AOF文件，非常非常慢，但也非常安全；
                appendfsync everysec每秒钟都调用fsync刷新到AOF文件，很快，但可能会丢失一秒以内的数据；
                appendfsync no依靠OS进行刷新，redis不主动刷新AOF，这样最快，但安全性就差。默认并推荐每秒刷新，这样在速度和安全上都做到了兼顾。
        
            Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。
            Redis在master是非阻塞模式，也就是说在slave执行数据同步的时候，master是可以接受客户端的请求的，
            并不影响同步数据的一致性，然而在slave端是阻塞模式的，slave在同步master数据时，并不能够响应客户端的查询。
        
            *一致性哈希
            *https://www.cnblogs.com/lpfuture/p/5796398.html
                集群要实现的目的是要将不同的 key 分散放置到不同的 redis 节点，这里我们需要一个规则或者算法，
                通常的做法是获取 key 的哈希值，然后根据节点数来求模，但这种做法有其明显的弊端，当我们需要增加或减少一个节点时，会造成大量的 key 无法命中，这种比例是相当高的，所以就有人提出了一致性哈希的概念。
                
                一致性哈希有四个重要特征：
                    均衡性：也有人把它定义为平衡性，是指哈希的结果能够尽可能分布到所有的节点中去，这样可以有效的利用每个节点上的资源。
                    单调性：对于单调性有很多翻译让我非常的不解，而我想要的是当节点数量变化时哈希的结果应尽可能的保护已分配的内容不会被重新分派到新的节点。
                    分散性和负载：这两个其实是差不多的意思，就是要求一致性哈希算法对 key 哈希应尽可能的避免重复。
                
                *一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器
                为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，
                每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。
                例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、
                “Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点
        
        
            从节点选举
                
                发送授权请求的是一个从节点，并且它所属的主节点处于 FAIL 状态。
                *在已下线主节点的所有从节点中，这个从节点的节点 ID 在排序中是最小的。
                从节点处于正常的运行状态：它没有被标记为 FAIL 状态，也没有被标记为 PFAIL 状态。
            
            
            从redis 3.0之后版本支持redis-cluster集群，Redis-Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。
            特点：
                无中心架构（不存在哪个节点影响性能瓶颈），少了 proxy 层。
                数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。
                可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。
                高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做备份数据副本
                实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave到 Master的角色提升。
            
            缺点：
                资源隔离性较差，容易出现相互影响的情况。
                数据通过异步复制,不保证数据的强一致性
            
                1、所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。
                2、节点的fail是通过集群中超过半数的节点检测失效时才生效。
                3、客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。
                4、redis-cluster把所有的物理节点映射到[0-16383]slot上（不一定是平均分配）,cluster 负责维护node<->slot<->value。
                5、Redis集群预分好16384个桶，当需要在 Redis 集群中放置一个 key-value 时，根据 CRC16(key) mod 16384的值，决定将一个key放到哪个桶中。

    18.guava cache
            有两种方法可以实现"如果有缓存则返回；否则运算、缓存、然后返回"。
                CacheLoader
                Callable：所有类型的Guava Cache，不管有没有自动加载功能，都支持get(K, Callable<V>方法。

            *缓存回收
                上面的简单例子中，内存一直会被占用，这是不合理的，我们需要按照一定的规则将缓存清除，释放内存，防止发生OOM。
                guava提供了三种缓存回收的策略：基于容量回收、定时回收和基于引用回收。
                
                1.基于容量的回收:
                    规定缓存项的数目不超过固定值，只需使用CacheBuilder.maximumSize(long)。缓存将尝试回收最近没有使用或总体上很少
                    使用的缓存项。——警告：在缓存项的数目达到限定值之前，即缓存项的数目逼近限定值时缓存就可能进行回收操作。这个size指
                    的是cache中的条目数，不是内存大小或是其他.
                    
                    定一个权重函数，并且用CacheBuilder.maximumWeight(long)指定最大总重。
                    缓存回收也是在重量逼近限定值时就进行了，还要知道重量是在缓存创建时计算的，因此要考虑重量计算的复杂度。
        
                    CacheBuilder.maximumSize(long)，CacheBuilder.maximumWeight(long)是互斥的，只能二选一。
                    
                    CacheBuilder.maximumSize(long)中如果每个条目占用内存空间都是相同的，就等价于限制了缓存空间的总大小；
                    如果每个缓存条目大小不定，那么就没有办法限制总的内存大小。
                    CacheBuilder.maximumWeight(long)可以用来控制内存。比如我们将总权重设置为1G(代表内存空间大小)，
                    而每个缓存条目的权重都是缓存值实际占用的内存空间大小
                    
                2.基于时间的回收
                    guava 提供两种定时回收的方法
                    expireAfterAccess(long, TimeUnit):缓存项在给定时间内没有被读/写访问，则回收。请注意这种缓存的回收顺序
                    和基于大小回收一样。
                    expireAfterWrite(long, TimeUnit):缓存项在给定时间内没有被写访问（创建或覆盖），则回收。如果认为缓存数据
                    总是在固定时候后变得陈旧不可用，这种回收方式是可取的。
                    
                3.基于引用的回收
                    手动清除缓存
                    任何时候，你都可以显式地清除缓存项，而不是等到它被回收：
                    个别清除：Cache.invalidate(key)
                    批量清除：Cache.invalidateAll(keys)
                    清除所有缓存项：Cache.invalidateAll()
                
            *监听器
                .removalListener(new RemovalListener<Object, Object>() {
                            @Override
                            public void onRemoval(RemovalNotification<Object, Object> notification) {
                                System.out.println("remove key[" + notification.getKey() + "],value[" + notification.getValue() + "],remove reason[" + notification.getCause() + "]");
                            }
                        })
                        
            统计
                CacheBuilder.recordStats()用来开启Guava Cache的统计功能。统计打开后，Cache.stats()方法会返回对象以提供如下统计信息：
                hitRate()：缓存命中率；
                averageLoadPenalty()：加载新值的平均时间，单位为纳秒；
                evictionCount()：缓存项被回收的总数，不包括显式清除
                统计信息对于调整缓存设置是至关重要的，在性能要求高的应用中我们建议密切关注这些数据。
                其他
                asMap()方法获得缓存数据的ConcurrentMap<K, V>快照
                cleanUp()清空缓存
            
            *原理
                JVM 缓存
                    首先是 JVM 缓存，也可以认为是堆缓存。
                    其实就是创建一些全局变量，如 Map、List 之类的容器用于存放数据。
                    这样的优势是使用简单但是也有以下问题：
                        只能显式的写入，清除数据。
                        不能按照一定的规则淘汰数据，如 LRU，LFU，FIFO 等。
                        清除数据时的回调通知。
                        其他一些定制功能等。
                
                Ehcache、Guava Cache
                    所以出现了一些专门用作 JVM 缓存的开源工具出现了，如本文提到的 Guava Cache。
                    它具有上文 JVM 缓存不具有的功能，如自动清除数据、多种清除算法、清除回调等。
                    但也正因为有了这些功能，这样的缓存必然会多出许多东西需要额外维护，自然也就增加了系统的消耗。
                
                Guava Cache 其核心数据结构大体上和 ConcurrentHashMap 一致，具体细节上会有些区别。功能上，ConcurrentMap会一直
                保存所有添加的元素，直到显式地移除。相对地， Guava Cache 为了限制内存占用，通常都设定为自动回收元素。在某些场景下，
                尽管它不回收元素，也是很有用的，因为它会自动加载缓存。
                
                Guava Cache的实现,核心数据结构和算法都是和JDK 1.6版本的 ConcurrentHashMap 一致.因此,如果你熟悉
                ConcurrentHashMap实现原理,对Cache是很容易明白的.
                
    19.Redis与Memcached的区别
        1 Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash等数据结构的存储。
        2 虚拟内存--Redis当物理内存用完时，可以将一些很久没用到的value 交换到磁盘；
        3 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。
        
    20.scan代替keys命令,keys会阻塞其他命令.
        ScanParams scanParams = new ScanParams();
                scanParams.match("DL*");
                scanParams.count(5);
                jedis.select(1);
                // scan(curso,params) cursor 表示开始遍历的游标   params 是ScanParams 对象，此对象可以设置 每次返回的数量，以及遍历时的正则表达式
                // 需要注意的是，对元素的模式匹配工作是在命令从数据集中取出元素之后，向客户端返回元素之前的这段时间内进行的，
                //  所以如果被迭代的数据集中只有少量元素和模式相匹配，那么迭代命令或许会在多次执行中都不返回任何元素。  
                ScanResult<String> scan = jedis.scan("0", scanParams);
                System.out.println("scan：返回用于下次遍历的游标"+scan.getStringCursor());
                System.out.println("scan：返回结果"+scan.getResult());
        



###结束

    guava cache
    https://blog.csdn.net/u012881904/article/details/79263787
    
    redis分布式锁实现
    https://www.cnblogs.com/0201zcr/p/5942748.html
    
    redis
    *https://juejin.im/post/5be3ba7a51882516bc477e1d    (必看)
        Redis常见的数据类型：string、list、hash、set、sortset。
        
        SDS简单动态字符串
            struct sdshdr{
                // 字节数组，用于保存字符串
                char buf[];
                // 记录buf数组中已使用的字节数量，也是字符串的长度
                int len;
                // 记录buf数组未使用的字节数量
                int free;
            }
        
        使用SDS的好处
            SDS与C的字符串表示比较
                sdshdr数据结构中用len属性记录了字符串的长度。那么获取字符串的长度时，时间复杂度只需要O(1)。
                SDS不会发生溢出的问题，如果修改SDS时，空间不足。先会扩展空间，再进行修改！(内部实现了动态扩展机制)。
                SDS可以减少内存分配的次数(空间预分配机制)。在扩展空间时，除了分配修改时所必要的空间，还会分配额外的空闲空间(free 属性)。
                SDS是二进制安全的，所有SDS API都会以处理二进制的方式来处理SDS存放在buf数组里的数据。
        
        redis数据结构
            string-->简单的key-value
            list-->有序列表(底层是双向链表)-->可做简单队列
            set-->无序列表(去重)-->提供一系列的交集、并集、差集的命令
            hash-->哈希表-->存储结构化数据
            sortset-->有序集合映射(member-score)-->排行榜
            
        Redis服务器的数据库
            Redis服务器用redisServer结构体来表示，其中redisDb是一个数组，用来保存所有的数据库，dbnum代表数据库的数量(这个可以配置，默认是16)
            
            Redis的数据库就是使用字典(哈希表)来作为底层实现的，对数据库的增删改查都是构建在字典(哈希表)的操作之上的。
            typedef struct redisDb { 
                int id;         // 数据库ID标识
                dict *dict;     // 键空间，存放着所有的键值对              
                dict *expires;  // 过期哈希表，保存着键的过期时间                          
                dict *watched_keys; // 被watch命令监控的key和相应client    
                long long avg_ttl;  // 数据库内所有键的平均TTL（生存时间）     
            } redisDb;
            
            
        *Redis对过期键的处理
            因为我们的内存是有限的。所以我们会干掉不常用的数据，保留常用的数据。这就需要我们设置一下键的过期(生存)时间了。
                设置键的生存时间可以通过EXPIRE或者PEXPIRE命令。
                设置键的过期时间可以通过EXPIREAT或者PEXPIREAT命令。
                其实EXPIRE、PEXPIRE、EXPIREAT这三个命令都是通过PEXPIREAT命令来实现的。
                我们在redisDb结构体中还发现了dict *expires;属性，存放所有键过期的时间。
            
            既然有设置过期(生存)时间的命令，那肯定也有移除过期时间，查看剩余生存时间的命令了：
                PERSIST(移除过期时间)
                TTL(Time To Live)返回剩余生存时间，以秒为单位
                PTTL以毫秒为单位返回键的剩余生存时间 
                
            *过期策略
                那这些过期键到了过期的时间，就会立马被删除掉吗？？
                    删除策略可分为三种
                    
                        定时删除(对内存友好，对CPU不友好),到时间点上就把所有过期的键删除了。
                        惰性删除(对CPU极度友好，对内存极度不友好),每次从键空间取键的时候，判断一下该键是否过期了，如果过期了就删除。
                        定期删除(折中),每隔一段时间去删除过期键，限制删除的执行时长和频率。
                    
                        Redis采用的是惰性删除+定期删除两种策略，所以说，在Redis里边如果过期键到了过期的时间了，未必被立马删除的！
                    
            *内存淘汰机制 
                如果定期删除漏掉了很多过期key，也没及时去查(没走惰性删除)，大量过期key堆积在内存里，导致redis内存块耗尽了，咋整？
                    我们可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。
                    
                    allkeys-lru
                        使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。
                        可以将内存最大使用量设置为热点数据占用的内存量，然后启用allkeys-lru淘汰策略，
                        将最近最少使用的数据淘汰
                    
                    volatile-lru
                        过期键中最近最少使用的淘汰
                        
            
        *Redis持久化策略(RDB和AOF)
            Redis是基于内存的，如果不想办法将数据保存在硬盘上，一旦Redis重启(退出/故障)，内存的数据将会全部丢失。
            
                我们肯定不想Redis里头的数据由于某些故障全部丢失(导致所有请求都走MySQL)，
                即便发生了故障也希望可以将Redis原有的数据恢复过来，这就是持久化的作用。
            
            Redis提供了两种不同的持久化方法来讲数据存储到硬盘里边：
            
                RDB(基于快照)，将某一时刻的所有数据保存到一个RDB文件中。
                AOF(append-only-file)，当Redis服务器执行写命令的时候，将执行的写命令保存到AOF文件中。
            
            RDB(快照持久化)
                RDB持久化可以手动执行，也可以根据服务器配置定期执行。RDB持久化所生成的RDB文件是一个经过压缩的二进制文件，
                Redis可以通过这个文件还原数据库的数据。
            
                有两个命令可以生成RDB文件：
                    SAVE会阻塞Redis服务器进程，服务器不能接收任何请求，直到RDB文件创建完毕为止。
                    BGSAVE创建出一个子进程，由子进程来负责创建RDB文件，服务器进程可以继续接收请求。
                
                Redis服务器在启动的时候，如果发现有RDB文件，就会自动载入RDB文件(不需要人工干预)
                服务器在载入RDB文件期间，会处于阻塞状态，直到载入工作完成。
                
                除了手动调用SAVE或者BGSAVE命令生成RDB文件之外，我们可以使用配置的方式来定期执行：
                在默认的配置下，如果以下的条件被触发，就会执行BGSAVE命令
                    save 900 1              #在900秒(15分钟)之后，至少有1个key发生变化，
                    save 300 10            #在300秒(5分钟)之后，至少有10个key发生变化
                    save 60 10000        #在60秒(1分钟)之后，至少有10000个key发生变化
                复制代码原理大概就是这样子的(结合上面的配置来看)：
                
                struct redisServer{
                	// 修改计数器
                	long long dirty;
                
                	// 上一次执行保存的时间
                	time_t lastsave;
                
                	// 参数的配置
                	struct saveparam *saveparams;
                };
                
                复制代码遍历参数数组，判断修改次数和时间是否符合，如果符合则调用besave()来生成RDB文件
                总结：通过手动调用SAVE或者BGSAVE命令或者配置条件触发，将数据库某一时刻的数据快照，生成RDB文件实现持久化。
                
                
            AOF(文件追加)
                从前面的示例看出，我们写了三条命令，AOF文件就保存了三条命令。如果我们的命令是这样子的：
                
                redis > RPUSH list "Java" "3y"
                (integer)2
                
                redis > RPUSH list "Java3y"
                integer(3)
                
                redis > RPUSH list "yyy"
                integer(4)
                
                复制代码同样地，AOF也会保存3条命令。我们会发现一个问题：上面的命令是可以合并起来成为1条命令的，并不需要3条。这样就可以让AOF文件的体积变得更小。
                AOF重写由Redis自行触发(参数配置)，也可以用BGREWRITEAOF命令手动触发重写操作。
                
                要值得说明的是：AOF重写不需要对现有的AOF文件进行任何的读取、分析。AOF重写是通过读取服务器当前数据库的数据来实现的！
                新的AOF文件的命令如下，没有一条是多余的！
                
            AOF后台重写
                Redis将AOF重写程序放到子进程里执行(BGREWRITEAOF命令)，像BGSAVE命令一样fork出一个子进程来完成重写AOF的操作，从而不会影响到主进程。
                AOF后台重写是不会阻塞主进程接收请求的，新的写命令请求可能会导致当前数据库和重写后的AOF文件的数据不一致！
                为了解决数据不一致的问题，Redis服务器设置了一个AOF重写缓冲区，这个缓存区会在服务器创建出子进程之后使用。
                
                
            RDB和AOF对过期键的策略
                RDB持久化对过期键的策略：
                    执行SAVE或者BGSAVE命令创建出的RDB文件，程序会对数据库中的过期键检查，已过期的键不会保存在RDB文件中。
                    载入RDB文件时，程序同样会对RDB文件中的键进行检查，过期的键会被忽略。
                
                AOF持久化对过期键的策略：
                    如果数据库的键已过期，但还没被惰性/定期删除，AOF文件不会因为这个过期键产生任何影响(也就说会保留)，当过期的键被删除了以后，会追加一条DEL命令来显示记录该键被删除了
                    重写AOF文件时，程序会对RDB文件中的键进行检查，过期的键会被忽略。
                
                复制模式：
                    主服务器来控制从服务器统一删除过期键(保证主从服务器数据的一致性)
                
            *RDB和AOF用哪个？
                rdb是Redis DataBase缩写 功能核心函数rdbSave(生成RDB文件)和rdbLoad（从文件加载内存）
                Aof是Append-only file缩写 
                
                RDB和AOF并不互斥，它俩可以同时使用。
                
                    RDB的优点：载入时恢复数据快、文件体积小。
                    RDB的缺点：会一定程度上丢失数据(因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。)
                    AOF的优点：丢失数据少(默认配置只丢失一秒的数据)。
                    AOF的缺点：恢复数据相对较慢，文件体积大
                    
                    aof文件比rdb更新频率高，优先使用aof还原数据。
                    aof比rdb更安全也更大
                    rdb性能比aof好
                    如果两个都配了优先加载AOF
                
                如果Redis服务器同时开启了RDB和AOF持久化，服务器会优先使用AOF文件来还原数据(因为AOF更新频率比RDB更新频率要高，还原的数据更完善)
                
                可能涉及到RDB和AOF的配置：
                
                    redis持久化，两种方式
                    1、rdb快照方式
                    2、aof日志方式
                    
                    ----------rdb快照------------
                    save 900 1
                    save 300 10
                    save 60 10000
                    
                    stop-writes-on-bgsave-error yes
                    rdbcompression yes
                    rdbchecksum yes
                    dbfilename dump.rdb
                    dir /var/rdb/
                    
                    -----------Aof的配置-----------
                    appendonly no # 是否打开 aof日志功能
                    
                    appendfsync always #每一个命令都立即同步到aof，安全速度慢
                    appendfsync everysec
                    appendfsync no 写入工作交给操作系统，由操作系统判断缓冲区大小，统一写入到aof  同步频率低，速度快
                    
                    no-appendfsync-on-rewrite yes 正在导出rdb快照的时候不要写aof
                    auto-aof-rewrite-percentage 100
                    auto-aof-rewrite-min-size 64mb 
                
                    ./bin/redis-benchmark -n 20000
                
            redis单线程为什么这么快?
                I/O多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，
                而这些文件描述符其中的任意一个进入读就绪状态、等等，select()函数就可以返回。
                select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。
                
                文件事件处理器使用I/O多路复用程序来同时监听多个Socket。当被监听的Socket准备好执行连接应答(accept)、
                读取(read)等等操作时，与操作相对应的文件事件就会产生，根据文件事件来为Socket关联对应的事件处理器，从而实现功能。  
                
                要值得注意的是：Redis中的I/O多路复用程序会将所有产生事件的Socket放到一个队列里边，
                然后通过这个队列以有序、同步、每次一个Socket的方式向文件事件分派器传送套接字。
                也就是说：当上一个Socket处理完毕后，I/O多路复用程序才会向文件事件分派器传送下一个Socket。
                
            Redis单线程为什么快？
                1）纯内存操作
                2）核心是基于非阻塞的IO多路复用机制
                3）单线程避免了多线程的频繁上下文切换问题
                
            
        redis主从        
            主从架构的特点
                下面我们来看看Redis的主从架构特点：
                
                主服务器负责接收写请求
                从服务器负责接收读请求
                从服务器的数据由主服务器复制过去。主从服务器的数据是一致的
                
            主从架构的好处：
                读写分离(主服务器负责写，从服务器负责读)
                高可用(某一台从服务器挂了，其他从服务器还能继续接收请求，不影响服务)
                处理更多的并发量(每台从服务器都可以接收读请求，读QPS就上去了)
               
            redis复制
                在Redis中，用户可以通过执行SALVEOF命令或者设置salveof选项，让一个服务器去复制(replicate)另一个服务器，
                我们称呼被复制的服务器为主服务器(master)，而对主服务器进行复制的服务器则被称为从服务器(salve)
                
                1.同步(sync)
                    将从服务器的数据库状态更新至主服务器的数据库状态
                2.命令传播(command propagate)
                    主服务器的数据库状态被修改，导致主从服务器的数据库状态不一致，让主从服务器的数据库状态重新回到一致状态
 
                从服务器对主服务器的同步又可以分为两种情况：
                
                    初次同步：从服务器没有复制过任何的主服务器，或者从服务器要复制的主服务器跟上次复制的主服务器不一样。
                    断线后同步：处于命令传播阶段的主从服务器因为网络原因中断了复制，从服务器通过自动重连重新连接主服务器，并继续复制主服务器
                
                *Redis从2.8版本开始，使用PSYNC命令来替代SYNC命令执行复制时同步的操作。
                
                PSYNC命令具有完整重同步和部分重同步两种模式(其实就跟上面所说的初次复制和断线后复制差不多个意思)。
                完整重同步
                    下面先来看看完整重同步是怎么实现的：
                    
                    从服务器向主服务器发送PSYNC命令
                    收到PSYNC命令的主服务器执行BGSAVE命令，在后台生成一个RDB文件。并用一个缓冲区来记录从现在开始执行的所有写命令。
                    当主服务器的BGSAVE命令执行完后，将生成的RDB文件发送给从服务器，从服务器接收和载入RBD文件。将自己的数据库状态更新至与主服务器执行BGSAVE命令时的状态。
                    主服务器将所有缓冲区的写命令发送给从服务器，从服务器执行这些写命令，达到数据最终一致性。
                
                部分重同步
                    接下来我们来看看部分重同步，部分重同步可以让我们断线后重连只需要同步缺失的数据(而不是Redis2.8之前的同步全部数据)，这是符合逻辑的！
                    部分重同步功能由以下部分组成：
                    
                    主从服务器的复制偏移量
                    主服务器的复制积压缓冲区
                    服务器运行的ID(run ID)
                    
            哨兵(Sentinal)机制
            https://juejin.im/post/5c061de7e51d451dcd3c2ab5
             
                Redis提供了哨兵(Sentinal)机制供我们解决上面的情况。如果主服务器挂了，我们可以将从服务器升级为主服务器，
                等到旧的主服务器(挂掉的那个)重连上来，会将它(挂掉的主服务器)变成从服务器。
                这个过程叫做主备切换(故障转移)                
                
                主服务器挂了，主从复制操作就中止了，并且哨兵系统是可以察觉出主服务挂了。
                Redis提供哨兵机制可以将选举一台从服务器变成主服务器
                然后旧的主服务器如果重连了，会变成从服务器
                
                首先我们要知道的是：Sentinel本质上只是一个运行在特殊模式下的Redis服务器。
                因为Sentinel做的事情和Redis服务器是不一样的，所以它们的初始化是有所区别的
                (比如，Sentinel在初始化的时候并不会载入AOF/RDB文件，因为Sentinel根本就不用数据库)。
                在启动的时候会将普通Redis服务器的代码替换成Sentinel专用代码。(所以Sentinel虽然作为Redis服务器，
                但是它不能执行SET、DBSIZE等等命令，因为命令表的代码被替换了
                
                Sentinel会创建两个连向主服务器的网络连接：
                
                命令连接(发送和接收命令)
                订阅连接(订阅主服务器的_sentinel_:hello频道)

            判断主服务器是否下线了
            判断主服务器是否下线有两种情况：
            
                主观下线
                    Sentinel会以每秒一次的频率向与它创建命令连接的实例(包括主从服务器和其他的Sentinel)发送PING命令，通过PING命令返回的信息判断实例是否在线
                    如果一个主服务器在down-after-milliseconds毫秒内连续向Sentinel发送无效回复，那么当前Sentinel就会主观认为该主服务器已经下线了。
            
            
                客观下线
                    当Sentinel将一个主服务器判断为主观下线以后，为了确认该主服务器是否真的下线，它会向同样监视该主服务器的Sentinel询问，看它们是否也认为该主服务器是否下线。
                    如果足够多的Sentinel认为该主服务器是下线的，那么就判定该主服务为客观下线，并对主服务器执行故障转移操作。
            
            
            选举领头Sentinel和故障转移
                选举领头Sentinel的规则也比较多，总的来说就是先到先得(哪个快，就选哪个)
                
            选举出领头的Sentinel之后，领头的Sentinel会对已下线的主服务器执行故障转移操作，包括三个步骤：
            
                在已下线主服务器属下的从服务器中，挑选一个转换为主服务器
                让已下线主服务器属下的所有从服务器改为复制新的主服务器
                已下线的主服务器重新连接时，让他成为新的主服务器的从服务器
            
                挑选某一个从服务器作为主服务器也是有策略的，大概如下：
            
                （1）跟master断开连接的时长
                （2）slave优先级
                （3）复制offset
                （4）run id
            
            tips:目前为止的主从+哨兵架构可以说Redis是高可用的，但要清楚的是：Redis还是会丢失数据的
            
                异步复制导致的数据丢失
                    有部分数据还没复制到从服务器，主服务器就宕机了，此时这些部分数据就丢失了
                
                
                脑裂导致的数据丢失
                    有时候主服务器脱离了正常网络，跟其他从服务器不能连接。此时哨兵可能就会认为主服务器下线了
                    (然后开启选举，将某个从服务器切换成了主服务器)，但是实际上主服务器还运行着。这个时候，
                    集群里就会有两个服务器(也就是所谓的脑裂)。
                    虽然某个从服务器被切换成了主服务器，但是可能客户端还没来得及切换到新的主服务器，客户端还继续写向旧主服务器写数据。
                    旧的服务器重新连接时，会作为从服务器复制新的主服务器(这意味着旧数据丢失)。
                
        缓存雪崩
            如果缓存数据设置的过期时间是相同的，并且Redis恰好将这部分数据全部删光了。这就会导致在这段时间内，这些缓存同时失效，全部请求到数据库中。
            这就是缓存雪崩：
            Redis挂掉了，请求全部走数据库。
            对缓存数据设置相同的过期时间，导致某段时间内缓存失效，请求全部走数据库。
            缓存雪崩如果发生了，很可能就把我们的数据库搞垮，导致整个服务瘫痪！
            
        如何解决缓存雪崩？
            对缓存数据设置相同的过期时间，导致某段时间内缓存失效，请求全部走数据库。
                解决方法：在缓存的时候给过期时间加上一个随机值，这样就会大幅度的减少缓存在同一时间过期。
            
            对于“Redis挂掉了，请求全部走数据库”这种情况，我们可以有以下的思路：
                事发前：实现Redis的高可用(主从架构+Sentinel 或者Redis Cluster)，尽量避免Redis挂掉这种情况发生。
                事发中：万一Redis真的挂了，我们可以设置本地缓存(ehcache)+限流(hystrix)，
                尽量避免我们的数据库被干掉(起码能保证我们的服务还是能正常工作的)
                事发后：redis持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据。
                
                
        什么是缓存穿透?   
            缓存穿透是指查询一个一定不存在的数据。由于缓存不命中，并且出于容错考虑，如果从数据库查不到数据则不写入缓存，
            这将导致这个不存在的数据每次请求都要到数据库去查询，失去了缓存的意义。
            请求的数据在缓存大量不命中，导致请求走数据库。
            
            解决缓存穿透也有两种方案：
            
                由于请求的参数是不合法的(每次都请求不存在的参数)，于是我们可以使用布隆过滤器(BloomFilter)或者压缩filter提前拦截，不合法就不让这个请求到数据库层！
                当我们从数据库找不到的时候，我们也将这个空对象设置到缓存里边去。下次再请求的时候，就可以从缓存里边获取了。
                这种情况我们一般会将空对象设置一个较短的过期时间。
            
        缓存与数据库双写一致
            一般我们对读操作的时候有这么一个固定的套路：
                如果我们的数据在缓存里边有，那么就直接取缓存的。
                如果缓存里没有我们想要的数据，我们会先去查询数据库，然后将数据库查出来的数据写到缓存中。
                最后将数据返回给请求
                
        什么是缓存与数据库双写一致问题？    
            对于缓存在更新时而言，都是建议执行删除操作！
            
            正常的情况是这样的：
                先操作数据库，成功；
                再删除缓存，也成功；
            
            如果原子性被破坏了：
                第一步成功(操作数据库)，第二步失败(删除缓存)，会导致数据库里是新数据，而缓存里是旧数据。
                如果第一步(操作数据库)就失败了，我们可以直接返回错误(Exception)，不会出现数据不一致。
            
            对比两种策略
                我们可以发现，两种策略各自有优缺点：
            
                先删除缓存，再更新数据库
                    在高并发下表现不如意，在原子性被破坏时表现优异
            
                先更新数据库，再删除缓存(Cache Aside Pattern设计模式)
                    在高并发下表现优异，在原子性被破坏时表现不如意
            
        Redis Cluster深入与实践
        
            AOF文件刷新的方式，有三种，参考配置参数appendfsync ：
                appendfsync always每提交一个修改命令都调用fsync刷新到AOF文件，非常非常慢，但也非常安全；
                appendfsync everysec每秒钟都调用fsync刷新到AOF文件，很快，但可能会丢失一秒以内的数据；
                appendfsync no依靠OS进行刷新，redis不主动刷新AOF，这样最快，但安全性就差。默认并推荐每秒刷新，这样在速度和安全上都做到了兼顾。
        
            Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。
            Redis在master是非阻塞模式，也就是说在slave执行数据同步的时候，master是可以接受客户端的请求的，
            并不影响同步数据的一致性，然而在slave端是阻塞模式的，slave在同步master数据时，并不能够响应客户端的查询。
        
            *一致性哈希
            *https://www.cnblogs.com/lpfuture/p/5796398.html
                集群要实现的目的是要将不同的 key 分散放置到不同的 redis 节点，这里我们需要一个规则或者算法，
                通常的做法是获取 key 的哈希值，然后根据节点数来求模，但这种做法有其明显的弊端，当我们需要增加或减少一个节点时，会造成大量的 key 无法命中，这种比例是相当高的，所以就有人提出了一致性哈希的概念。
                
                一致性哈希有四个重要特征：
                    均衡性：也有人把它定义为平衡性，是指哈希的结果能够尽可能分布到所有的节点中去，这样可以有效的利用每个节点上的资源。
                    单调性：对于单调性有很多翻译让我非常的不解，而我想要的是当节点数量变化时哈希的结果应尽可能的保护已分配的内容不会被重新分派到新的节点。
                    分散性和负载：这两个其实是差不多的意思，就是要求一致性哈希算法对 key 哈希应尽可能的避免重复。
                
                *一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器
                为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，
                每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。
                例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、
                “Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点
        
        
            从节点选举
                
                发送授权请求的是一个从节点，并且它所属的主节点处于 FAIL 状态。
                *在已下线主节点的所有从节点中，这个从节点的节点 ID 在排序中是最小的。
                从节点处于正常的运行状态：它没有被标记为 FAIL 状态，也没有被标记为 PFAIL 状态。
            
            
            从redis 3.0之后版本支持redis-cluster集群，Redis-Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。
            特点：
                无中心架构（不存在哪个节点影响性能瓶颈），少了 proxy 层。
                数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。
                可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。
                高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做备份数据副本
                实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave到 Master的角色提升。
            
            缺点：
                资源隔离性较差，容易出现相互影响的情况。
                数据通过异步复制,不保证数据的强一致性
            
                1、所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。
                2、节点的fail是通过集群中超过半数的节点检测失效时才生效。
                3、客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。
                4、redis-cluster把所有的物理节点映射到[0-16383]slot上（不一定是平均分配）,cluster 负责维护node<->slot<->value。
                5、Redis集群预分好16384个桶，当需要在 Redis 集群中放置一个 key-value 时，根据 CRC16(key) mod 16384的值，决定将一个key放到哪个桶中。

        guava cache
            有两种方法可以实现"如果有缓存则返回；否则运算、缓存、然后返回"。
                CacheLoader
                Callable：所有类型的Guava Cache，不管有没有自动加载功能，都支持get(K, Callable<V>方法。

            *内存一直被占用？可怕：缓存回收
                上面的简单例子中，内存一直会被占用，这是不合理的，我们需要按照一定的规则将缓存清除，释放内存，防止发生OOM。
                guava提供了三种缓存回收的策略：基于容量回收、定时回收和基于引用回收。
                
                基于容量的回收:
                    规定缓存项的数目不超过固定值，只需使用CacheBuilder.maximumSize(long)。缓存将尝试回收最近没有使用或总体上很少
                    使用的缓存项。——警告：在缓存项的数目达到限定值之前，即缓存项的数目逼近限定值时缓存就可能进行回收操作。这个size指
                    的是cache中的条目数，不是内存大小或是其他.
                    
                    定一个权重函数，并且用CacheBuilder.maximumWeight(long)指定最大总重。
                    缓存回收也是在重量逼近限定值时就进行了，还要知道重量是在缓存创建时计算的，因此要考虑重量计算的复杂度。
        
                    CacheBuilder.maximumSize(long)，CacheBuilder.maximumWeight(long)是互斥的，只能二选一。
                    
                    CacheBuilder.maximumSize(long)中如果每个条目占用内存空间都是相同的，就等价于限制了缓存空间的总大小；
                    如果每个缓存条目大小不定，那么就没有办法限制总的内存大小。
                    CacheBuilder.maximumWeight(long)可以用来控制内存。比如我们将总权重设置为1G(代表内存空间大小)，
                    而每个缓存条目的权重都是缓存值实际占用的内存空间大小
                    
                基于时间的回收
                    guava 提供两种定时回收的方法
                    expireAfterAccess(long, TimeUnit):缓存项在给定时间内没有被读/写访问，则回收。请注意这种缓存的回收顺序
                    和基于大小回收一样。
                    expireAfterWrite(long, TimeUnit):缓存项在给定时间内没有被写访问（创建或覆盖），则回收。如果认为缓存数据
                    总是在固定时候后变得陈旧不可用，这种回收方式是可取的。
                    
                基于引用的回收
                    手动清除缓存
                    任何时候，你都可以显式地清除缓存项，而不是等到它被回收：
                    个别清除：Cache.invalidate(key)
                    批量清除：Cache.invalidateAll(keys)
                    清除所有缓存项：Cache.invalidateAll()
                
            *监听器
                .removalListener(new RemovalListener<Object, Object>() {
                            @Override
                            public void onRemoval(RemovalNotification<Object, Object> notification) {
                                System.out.println("remove key[" + notification.getKey() + "],value[" + notification.getValue() + "],remove reason[" + notification.getCause() + "]");
                            }
                        })
                        
            统计
                CacheBuilder.recordStats()用来开启Guava Cache的统计功能。统计打开后，Cache.stats()方法会返回对象以提供如下统计信息：
                hitRate()：缓存命中率；
                averageLoadPenalty()：加载新值的平均时间，单位为纳秒；
                evictionCount()：缓存项被回收的总数，不包括显式清除
                统计信息对于调整缓存设置是至关重要的，在性能要求高的应用中我们建议密切关注这些数据。
                其他
                asMap()方法获得缓存数据的ConcurrentMap<K, V>快照
                cleanUp()清空缓存
            
            *原理
                JVM 缓存
                    首先是 JVM 缓存，也可以认为是堆缓存。
                    其实就是创建一些全局变量，如 Map、List 之类的容器用于存放数据。
                    这样的优势是使用简单但是也有以下问题：
                        只能显式的写入，清除数据。
                        不能按照一定的规则淘汰数据，如 LRU，LFU，FIFO 等。
                        清除数据时的回调通知。
                        其他一些定制功能等。
                
                Ehcache、Guava Cache
                    所以出现了一些专门用作 JVM 缓存的开源工具出现了，如本文提到的 Guava Cache。
                    它具有上文 JVM 缓存不具有的功能，如自动清除数据、多种清除算法、清除回调等。
                    但也正因为有了这些功能，这样的缓存必然会多出许多东西需要额外维护，自然也就增加了系统的消耗。
                
                Guava Cache 其核心数据结构大体上和 ConcurrentHashMap 一致，具体细节上会有些区别。功能上，ConcurrentMap会一直
                保存所有添加的元素，直到显式地移除。相对地， Guava Cache 为了限制内存占用，通常都设定为自动回收元素。在某些场景下，
                尽管它不回收元素，也是很有用的，因为它会自动加载缓存。
                
                Guava Cache的实现,核心数据结构和算法都是和JDK 1.6版本的 ConcurrentHashMap 一致.因此,如果你熟悉
                ConcurrentHashMap实现原理,对Cache是很容易明白的.
                
    *总结
        *redis常见面试题
        *https://juejin.im/post/5b99d4bce51d450e7a24b66e    (必读)
        为什么要用redis而不用map做缓存?
        redis常用的数据结构?
        redis过期策略?内存淘汰策略?
        Redis持久化策略?两种的优缺点?怎么选择?
        Redis单线程为什么快？
        redis主从复制?
        哨兵机制?
        什么是缓存雪崩？如何解决缓存雪崩？
        什么是缓存穿透?如何解决缓存穿透？
        什么是缓存与数据库双写一致问题？
        一致性hash算法?
        
        
        
        
        
##18.dubbo
    http://dubbo.apache.org/zh-cn/docs/user/demos/thread-model.html (必读)


    服务治理和配置管理
        服务治理
            服务治理主要作用是改变运行时服务的行为和选址逻辑，达到限流，权重配置等目的，主要有以下几个功能：
            
            应用级别的服务治理
                此Dubbo2.7版本中增加了应用粒度的服务治理操作，对于条件路由(包括黑白名单)，动态配置(包括权重，负载均衡)都可以做应用级别的配置：
                上图是条件路由的配置，可以按照应用名，服务名两个维度来填写，也可以按照这两个维度来查询。
                
            标签路由
                路由方法包括:条件路由和标签路由
                调用的时候，客户端可以通过setAttachment的方式，来设置不同的标签名称，比如本例中，setAttachment(tag1)，
                客户端的选址范围就在如图所示的三台机器中，可以通过这种方式来实现流量隔离，灰度发布等功能。
                
            黑白名单
                黑白名单是条件路由的一部分，规则存储和条件路由放在一起，为了方便配置所以单独拿出来，同样可以通过服务和应用两个维度，指定黑名单和白名单
                
            动态配置
                动态配置是和路由规则平行的另一类服务治理治理功能，主要作用是在不重启服务的情况下，动态改变调用行为，从Dubbo2.7版本开始，
                支持服务和应用两个维度的配置，采用yaml格式
                
            权重调节
                权重调节是动态配置的子功能，主要作用是改变服务端的权重，更大的权重会有更大的几率被客户端选中作为服务提供者，
                从而达到流量分配的目的
                
    dubbo流程
        
        启动时检查
            Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，以便上线时，能及早发现问题，
            默认 check="true"。
            可以通过 check="false" 关闭检查，比如，测试时，有些服务不关心，或者出现了循环依赖，必须有一方先启动。
            另外，如果你的 Spring 容器是懒加载的，或者通过 API 编程延迟引用服务，请关闭 check，否则服务临时不可用时，
            会抛出异常，拿到 null 引用，如果 check="false"，总是会返回引用，当服务恢复时，能自动连上。

        *集群容错
            在集群调用失败时，Dubbo 提供了多种容错方案，缺省为 failover(故障转移) 重试。
            各节点关系：
            
            这里的 Invoker 是 Provider 的一个可调用 Service 的抽象，Invoker 封装了 Provider 地址及 Service 接口信息
            Directory 代表多个 Invoker，可以把它看成 List<Invoker> ，但与 List 不同的是，它的值可能是动态变化的，比如注册中心推送变更
            Cluster 将 Directory 中的多个 Invoker 伪装成一个 Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个
            Router 负责从多个 Invoker 中按路由规则选出子集，比如读写分离，应用隔离等
            LoadBalance 负责从多个 Invoker 中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选
            
            集群容错模式
                Failover Cluster
                失败自动切换，当出现失败，重试其它服务器 [1]。通常用于读操作，但重试会带来更长延迟。可通过 retries="2" 来设置重试次数(不含第一次)。
                
                Failfast Cluster
                快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。
                
                Failsafe Cluster
                失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。
                
                Failback Cluster
                失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。
                
                Forking Cluster
                并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks="2" 来设置最大并行数。
                
                Broadcast Cluster
                广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。

            集群模式配置
                按照以下示例在服务提供方和消费方配置集群模式
                
                <dubbo:service cluster="failsafe" />
                或
                <dubbo:reference cluster="failsafe" />
        
        *负载均衡
            在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。
            负载均衡策略
                Random LoadBalance
                    随机，按权重设置随机概率。
                    在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。
                RoundRobin LoadBalance
                    轮询，按公约后的权重设置轮询比率。
                    存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。
                LeastActive LoadBalance
                    最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。
                    使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。
                ConsistentHash LoadBalance
                    一致性 Hash，相同参数的请求总是发到同一提供者。
                    当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。
                    算法参见：http://en.wikipedia.org/wiki/Consistent_hashing
                    缺省只对第一个参数 Hash，如果要修改，请配置 <dubbo:parameter key="hash.arguments" value="0,1" />
                    缺省用 160 份虚拟节点，如果要修改，请配置 <dubbo:parameter key="hash.nodes" value="320" />

     
        线程模型
            如果事件处理的逻辑能迅速完成，并且不会发起新的 IO 请求，比如只是在内存中记个标识，则直接在 IO 线程上处理更快，因为减少了线程池调度。
            但如果事件处理逻辑较慢，或者需要发起新的 IO 请求，比如需要查询数据库，则必须派发到线程池，否则 IO 线程阻塞，将导致不能接收其它请求。
            如果用 IO 线程处理事件，又在事件处理过程中发起新的 IO 请求，比如在连接事件中发起登录请求，会报“可能引发死锁”异常，但不会真死锁。
            因此，需要通过不同的派发策略和不同的线程池配置的组合来应对不同的场景:
            
            <dubbo:protocol name="dubbo" dispatcher="all" threadpool="fixed" threads="100" />
            
            Dispatcher
                all 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。
                direct 所有消息都不派发到线程池，全部在 IO 线程上直接执行。
                message 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行。
                execution 只请求消息派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在 IO 线程上执行。
                connection 在 IO 线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池。
            
            ThreadPool
                fixed 固定大小线程池，启动时建立线程，不关闭，一直持有。(缺省)
                cached 缓存线程池，空闲一分钟自动删除，需要时重建。
                limited 可伸缩线程池，但池中的线程数只会增长不会收缩。只增长不收缩的目的是为了避免收缩时突然来了大流量引起的性能问题。
                eager 优先创建Worker线程池。在任务数量大于corePoolSize但是小于maximumPoolSize时，优先创建Worker来处理任务。
                当任务数量大于maximumPoolSize时，将任务放入阻塞队列中。阻塞队列充满时抛出RejectedExecutionException。
                (相比于cached:cached在任务数量超过maximumPoolSize时直接抛出异常而不是将任务放入阻塞队列)

            直连提供者
                在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直连方式，
                将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表。
                如果是线上需求需要点对点，可在 <dubbo:reference> 中配置 url 指向提供者，将绕过注册中心，多个地址用分号隔开，配置如下
                <dubbo:reference id="xxxService" interface="com.alibaba.xxx.XxxService" url="dubbo://localhost:20890" />
                
            只订阅
                为方便开发测试，经常会在线下共用一个所有服务可用的注册中心，这时，如果一个正在开发中的服务提供者注册，可能会影响消费者不能正常运行。
                可以让服务提供者开发方，只订阅服务(开发的服务可能依赖其它服务)，而不注册正在开发的服务，通过直连测试正在开发的服务
                <dubbo:registry address="10.20.153.10:9090" register="false" />
                或者
                <dubbo:registry address="10.20.153.10:9090?register=false" />
                
            只注册
                如果有两个镜像环境，两个注册中心，有一个服务只在其中一个注册中心有部署，另一个注册中心还没来得及部署，而两个注册中心的其它应用都需要依赖此服务。这个时候，可以让服务提供者方只注册服务到另一注册中心，而不从另一注册中心订阅服务。
                禁用订阅配置
                
                <dubbo:registry id="hzRegistry" address="10.20.153.10:9090" />
                <dubbo:registry id="qdRegistry" address="10.20.141.150:9090" subscribe="false" />
                或者
                <dubbo:registry id="hzRegistry" address="10.20.153.10:9090" />
                <dubbo:registry id="qdRegistry" address="10.20.141.150:9090?subscribe=false" />
                
                
            静态服务
                有时候希望人工管理服务提供者的上线和下线，此时需将注册中心标识为非动态管理模式。
                <dubbo:registry address="10.20.141.150:9090" dynamic="false" />
                或者
                <dubbo:registry address="10.20.141.150:9090?dynamic=false" />
                服务提供者初次注册时为禁用状态，需人工启用。断线时，将不会被自动删除，需人工禁用。
            
            多协议
                Dubbo 允许配置多协议，在不同服务上支持不同协议或者同一服务上同时支持多种协议。
                
                不同服务不同协议
                不同服务在性能上适用不同协议进行传输，比如大数据用短连接协议，小数据大并发用长连接协议
                
                <?xml version="1.0" encoding="UTF-8"?>
                <beans xmlns="http://www.springframework.org/schema/beans"
                    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                    xmlns:dubbo="http://dubbo.apache.org/schema/dubbo"
                    xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd"> 
                    <dubbo:application name="world"  />
                    <dubbo:registry id="registry" address="10.20.141.150:9090" username="admin" password="hello1234" />
                    <!-- 多协议配置 -->
                    <dubbo:protocol name="dubbo" port="20880" />
                    <dubbo:protocol name="rmi" port="1099" />
                    <!-- 使用dubbo协议暴露服务 -->
                    <dubbo:service interface="com.alibaba.hello.api.HelloService" version="1.0.0" ref="helloService" protocol="dubbo" />
                    <!-- 使用rmi协议暴露服务 -->
                    <dubbo:service interface="com.alibaba.hello.api.DemoService" version="1.0.0" ref="demoService" protocol="rmi" /> 
                </beans>
                多协议暴露服务
                需要与 http 客户端互操作
                
                <?xml version="1.0" encoding="UTF-8"?>
                <beans xmlns="http://www.springframework.org/schema/beans"
                    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                    xmlns:dubbo="http://dubbo.apache.org/schema/dubbo"
                    xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd">
                    <dubbo:application name="world"  />
                    <dubbo:registry id="registry" address="10.20.141.150:9090" username="admin" password="hello1234" />
                    <!-- 多协议配置 -->
                    <dubbo:protocol name="dubbo" port="20880" />
                    <dubbo:protocol name="hessian" port="8080" />
                    <!-- 使用多个协议暴露服务 -->
                    <dubbo:service id="helloService" interface="com.alibaba.hello.api.HelloService" version="1.0.0" protocol="dubbo,hessian" />
                </beans>


            多注册中心
                Dubbo 支持同一服务向多注册中心同时注册，或者不同服务分别注册到不同的注册中心上去，甚至可以同时引用注册在不同注册中心上的同名服务。另外，注册中心是支持自定义扩展的 [1]。
            
            服务分组
                当一个接口有多种实现时，可以用 group 区分。
                
                服务
                <dubbo:service group="feedback" interface="com.xxx.IndexService" />
                <dubbo:service group="member" interface="com.xxx.IndexService" />
                引用
                <dubbo:reference id="feedbackIndexService" group="feedback" interface="com.xxx.IndexService" />
                <dubbo:reference id="memberIndexService" group="member" interface="com.xxx.IndexService" />
                
            多版本
                当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。
                可以按照以下的步骤进行版本迁移：
                0.在低压力时间段，先升级一半提供者为新版本
                1.再将所有消费者升级为新版本
                2.然后将剩下的一半提供者升级为新版本
                
                
            分组聚合
                按组合并返回结果 [1]，比如菜单服务，接口一样，但有多种实现，用group区分，现在消费方需从每种group中调用一次返回结果，
                合并结果返回，这样就可以实现聚合菜单项。
                
            结果缓存
                结果缓存 [1]，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量 [2]。
                
                缓存类型
                lru 基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。
                threadlocal 当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。
                jcache 与 JSR107 集成，可以桥接各种缓存实现。
                缓存类型可扩展，参见：缓存扩展
                
                配置
                <dubbo:reference interface="com.foo.BarService" cache="lru" />
                或：
                
                <dubbo:reference interface="com.foo.BarService">
                    <dubbo:method name="findBar" cache="lru" />
                </dubbo:reference>
                
    *整体设计
    http://dubbo.apache.org/zh-cn/docs/source_code_guide/service-invoking-process.html
        各层说明
            config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类
            proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory
            registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService
            cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance
            monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService
            protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter
            exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer
            transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec
            serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool
           
        关系说明
            在 RPC 中，Protocol 是核心层，也就是只要有 Protocol + Invoker + Exporter 就可以完成非透明的 RPC 调用，然后在 Invoker 的主过程上 Filter 拦截点。
            图中的 Consumer 和 Provider 是抽象概念，只是想让看图者更直观的了解哪些类分属于客户端与服务器端，不用 Client 和 Server 的原因是 Dubbo 在很多场景下都使用 Provider, Consumer, Registry, Monitor 划分逻辑拓普节点，保持统一概念。
            而 Cluster 是外围概念，所以 Cluster 的目的是将多个 Invoker 伪装成一个 Invoker，这样其它人只要关注 Protocol 层 Invoker 即可，加上 Cluster 或者去掉 Cluster 对其它层都不会造成影响，因为只有一个提供者时，是不需要 Cluster 的。
            Proxy 层封装了所有接口的透明化代理，而在其它层都以 Invoker 为中心，只有到了暴露给用户使用时，才用 Proxy 将 Invoker 转成接口，或将接口实现转成 Invoker，也就是去掉 Proxy 层 RPC 是可以 Run 的，只是不那么透明，不那么看起来像调本地服务一样调远程服务。
            而 Remoting 实现是 Dubbo 协议的实现，如果你选择 RMI 协议，整个 Remoting 都不会用上，Remoting 内部再划为 Transport 传输层和 Exchange 信息交换层，Transport 层只负责单向消息传输，是对 Mina, Netty, Grizzly 的抽象，它也可以扩展 UDP 传输，而 Exchange 层是在传输层之上封装了 Request-Response 语义。
            Registry 和 Monitor 实际上不算一层，而是一个独立的节点，只是为了全局概览，用层的方式画在一起。
        
        经历多次调用，到这里请求数据的发送过程就结束了，过程漫长。为了便于大家阅读代码，这里以 DemoService 为例，将 sayHello 方法的整个调用路径贴出来。
        
        proxy0#sayHello(String)
          —> InvokerInvocationHandler#invoke(Object, Method, Object[])
            —> MockClusterInvoker#invoke(Invocation)
              —> AbstractClusterInvoker#invoke(Invocation)
                —> FailoverClusterInvoker#doInvoke(Invocation, List<Invoker<T>>, LoadBalance)
                  —> Filter#invoke(Invoker, Invocation)  // 包含多个 Filter 调用
                    —> ListenerInvokerWrapper#invoke(Invocation) 
                      —> AbstractInvoker#invoke(Invocation) 
                        —> DubboInvoker#doInvoke(Invocation)
                          —> ReferenceCountExchangeClient#request(Object, int)
                            —> HeaderExchangeClient#request(Object, int)
                              —> HeaderExchangeChannel#request(Object, int)
                                —> AbstractPeer#send(Object)
                                  —> AbstractClient#send(Object, boolean)
                                    —> NettyChannel#send(Object, boolean)
                                      —> NioClientSocketChannel#write(Object)
                                      
        
        
        上面的方法通过反序列化将诸如 path、version、调用方法名、参数列表等信息依次解析出来，并设置到相应的字段中，
        最终得到一个具有完整调用信息的 DecodeableRpcInvocation 对象。
        
        到这里，请求数据解码的过程就分析完了。此时我们得到了一个 Request 对象，这个对象会被传送到下一个入站处理器中，
        我们继续往下看。
        
        调用服务
        解码器将数据包解析成 Request 对象后，NettyHandler 的 messageReceived 方法紧接着会收到这个对象，并将这个对象继续向下传递。
        这期间该对象会被依次传递给 NettyServer、MultiMessageHandler、HeartbeatHandler 以及 AllChannelHandler。
        最后由 AllChannelHandler 将该对象封装到 Runnable 实现类对象中，并将 Runnable 放入线程池中执行后续的调用逻辑。整个调用栈如下：
        
        NettyHandler#messageReceived(ChannelHandlerContext, MessageEvent)
          —> AbstractPeer#received(Channel, Object)
            —> MultiMessageHandler#received(Channel, Object)
              —> HeartbeatHandler#received(Channel, Object)
                —> AllChannelHandler#received(Channel, Object)
                  —> ExecutorService#execute(Runnable)    // 由线程池执行后续的调用逻辑
                  
                  
        线程派发模型
        Dubbo 将底层通信框架中接收请求的线程称为 IO 线程。如果一些事件处理逻辑可以很快执行完，比如只在内存打一个标记，此时直接在
        IO 线程上执行该段逻辑即可。但如果事件的处理逻辑比较耗时，比如该段逻辑会发起数据库查询或者 HTTP 请求。此时我们就不应该让
        事件处理逻辑在 IO 线程上执行，而是应该派发到线程池中去执行。原因也很简单，IO 线程主要用于接收请求，如果 IO 线程被占满，
        将导致它不能接收新的请求。
        

        到这里，整个服务调用过程就分析完了。最后把调用过程贴出来，如下：
        
        ChannelEventRunnable#run()
          —> DecodeHandler#received(Channel, Object)
            —> HeaderExchangeHandler#received(Channel, Object)
              —> HeaderExchangeHandler#handleRequest(ExchangeChannel, Request)
                —> DubboProtocol.requestHandler#reply(ExchangeChannel, Object)
                  —> Filter#invoke(Invoker, Invocation)
                    —> AbstractProxyInvoker#invoke(Invocation)
                      —> Wrapper0#invokeMethod(Object, String, Class[], Object[])
                        —> DemoServiceImpl#sayHello(String)
                        
    dubbo 熔断，限流，降级
        超时（timeout）
            在接口调用过程中，consumer调用provider的时候，provider在响应的时候，有可能会慢，如果provider 10s响应，那么consumer也
            会至少10s才响应。如果这种情况频度很高，那么就会整体降低consumer端服务的性能。
            这种响应时间慢的症状，就会像一层一层波浪一样，从底层系统一直涌到最上层，造成整个链路的超时。
            所以，consumer不可能无限制地等待provider接口的返回，会设置一个时间阈值，如果超过了这个时间阈值，就不继续等待。
            这个超时时间选取，一般看provider正常响应时间是多少，再追加一个buffer即可。
        重试（retry）
            但是也有可能provider只是偶尔抖动，那么超时后直接放弃，不做后续处理，就会导致当前请求错误，也会带来业务方面的损失。
            那么，对于这种偶尔抖动，可以在超时后重试一下，重试如果正常返回了，那么这次请求就被挽救了，能够正常给前端返回数据，只不过比原来响应慢一点。
            重试时的一些细化策略：
            重试可以考虑切换一台机器来进行调用，因为原来机器可能由于临时负载高而性能下降，重试会更加剧其性能问题，而换一台机器，
            得到更快返回的概率也更大一些。
            
            幂等(idempotent)
                如果允许consumer重试，那么provider就要能够做到幂等。
                即，同一个请求被consumer多次调用，对provider产生的影响(这里的影响一般是指某些写入相关的操作) 是一致的。
                而且这个幂等应该是服务级别的，而不是某台机器层面的，重试调用任何一台机器，都应该做到幂等。
                
        熔断（circuit break）
            如果是一个不那么重要的服务，却因为这个服务一直响应时间长导致consumer里面的核心服务也拖慢，那么就得不偿失了。
            单纯超时也解决不了这种情况了，因为一般超时时间，都比平均响应时间长一些，现在所有的打到provider的请求都超时了，
            那么consumer请求provider的平均响应时间就等于超时时间了，负载也被拖下来了。
            而重试则会加重这种问题，使consumer的可用性变得更差。
            因此就出现了熔断的逻辑，也就是，如果检查出来频繁超时，就把consumer调用provider的请求，直接短路掉，不实际调用，
            而是直接返回一个mock的值。
            等provider服务恢复稳定之后，重新调用。
            
        限流(current limiting)
            上面几个策略都是consumer针对provider出现各种情况而设计的。
            而provider有时候也要防范来自consumer的流量突变问题。
            这样一个场景，provider是一个核心服务，给N个consumer提供服务，突然某个consumer抽风，流量飙升，占用了provider大部分
            机器时间，导致其他可能更重要的consumer不能被正常服务。
            所以，provider端，需要根据consumer的重要程度，以及平时的QPS大小，来给每个consumer设置一个流量上线，同一时间内
            只会给A consumer提供N个线程支持，超过限制则等待或者直接拒绝。
        
            资源隔离
                provider可以对consumer来的流量进行限流，防止provider被拖垮。 
                同样，consumer 也需要对调用provider的线程资源进行隔离。 这样可以确保调用某个provider逻辑不会耗光整个consumer的线程池资源。
        
        服务降级
            降级服务既可以代码自动判断，也可以人工根据突发情况切换。
        
            consumer 端
                consumer 如果发现某个provider出现异常情况，比如，经常超时(可能是熔断引起的降级)，数据错误，这是，consumer可以采取一定的
                策略，降级provider的逻辑，基本的有直接返回固定的数据。
            provider 端
                当provider 发现流量激增的时候，为了保护自身的稳定性，也可能考虑降级服务。 
                比如，1，直接给consumer返回固定数据，2，需要实时写入数据库的，先缓存到队列里，异步写入数据库。
                
##netty
    https://juejin.im/post/5a228cc15188254cc067aef8 (必看)
    
    Netty提供了高性能与易用性，它具有以下特点：
        1.拥有设计良好且统一的API，支持NIO与OIO（阻塞IO）等多种传输类型，支持真正的无连接UDP Socket。
        2.简单而强大的线程模型，可高度定制线程（池）。
        3.良好的模块化与解耦，支持可扩展和灵活的事件模型，可以很轻松地分离关注点以复用逻辑组件（可插拔的,解耦）。
        4.性能高效，拥有比Java核心API更高的吞吐量，通过zero-copy功能以实现最少的内存复制消耗。
        5.内置了许多常用的协议编解码器，如HTTP、SSL、WebScoket等常见协议可以通过Netty做到开箱即用。
        用户也可以利用Netty简单方便地实现自己的应用层协议。
        
    NIO可以称为New IO也可以称为Non-blocking IO，它比Java旧的阻塞IO在性能上要高效许多（如果让每一个连接中的IO操作都单独创建一个线程，
    那么阻塞IO并不会比NIO在性能上落后，但不可能创建无限多的线程，在连接数非常多的情况下会很糟糕）。
    
        1.ByteBuffer：NIO的数据传输是基于缓冲区的，ByteBuffer正是NIO数据传输中所使用的缓冲区抽象。ByteBuffer支持在堆外分配内存，
        并且尝试避免在执行I/O操作中的多余复制。一般的I/O操作都需要进行系统调用，这样会先切换到内核态，内核态要先从文件读取数据到它的缓冲区，
        只有等数据准备完毕后，才会从内核态把数据写到用户态，所谓的阻塞IO其实就是说的在等待数据准备好的这段时间内进行阻塞。
        如果想要避免这个额外的内核操作，可以通过使用mmap（虚拟内存映射）的方式来让用户态直接操作文件。
          
        2.Channel：它类似于文件描述符，简单地来说它代表了一个实体（如一个硬件设备、文件、Socket或者一个能够执行一个
        或多个不同的I/O操作的程序组件）。你可以从一个Channel中读取数据到缓冲区，也可以将一个缓冲区中的数据写入到Channel。
        
        3.Selector：选择器是NIO实现的关键，NIO采用的是I/O多路复用的方式来实现非阻塞，Selector通过在一个线程中监听每个Channel的IO事件
        来确定有哪些已经准备好进行IO操作的Channel，因此可以在任何时间检查任意的读操作或写操作的完成状态。这种方式避免了等待IO操作准备
        数据时的阻塞，使用较少的线程便可以处理许多连接，减少了线程切换与维护的开销。
        
    网络传输的基本单位是字节，在Java NIO中提供了ByteBuffer作为字节缓冲区容器，但该类的API使用起来不太方便，所以Netty实现了ByteBuf
    作为其替代品，下面是使用ByteBuf的优点：
        相比ByteBuffer使用起来更加简单。
        通过内置的复合缓冲区类型实现了透明的zero-copy。
        容量可以按需增长。
        读和写使用了不同的索引指针。
        支持链式调用。
        支持引用计数与池化。
        可以被用户自定义的缓冲区类型扩展。
        
    *ByteBuf
    
    在讨论ByteBuf之前，我们先需要了解一下ByteBuffer的实现，这样才能比较深刻地明白它们之间的区别。
    ByteBuffer继承于abstract class Buffer（所以还有LongBuffer、IntBuffer等其他类型的实现），
        本质上它只是一个有限的线性的元素序列，包含了三个重要的属性。
            1.Capacity：缓冲区中元素的容量大小，你只能将capacity个数量的元素写入缓冲区，一旦缓冲区已满就需要清理缓冲区才能继续写数据。
            2.Position：指向下一个写入数据位置的索引指针，初始位置为0，最大为capacity-1。当写模式转换为读模式时，position需要被重置为0。
            3.Limit：在写模式中，limit是可以写入缓冲区的最大索引，也就是说它在写模式中等价于缓冲区的容量。在读模式中，limit表示可以读取数据的最大索引。
        
        由于Buffer中只维护了position一个索引指针，所以它在读写模式之间的切换需要调用一个flip()方法来重置指针。使用Buffer的流程一般如下：
            写入数据到缓冲区。
            调用flip()方法。
            从缓冲区中读取数据
            调用buffer.clear()或者buffer.compact()清理缓冲区，以便下次写入数据。
        
        Java NIO中的Buffer API操作的麻烦之处就在于读写转换需要手动重置指针。而ByteBuf没有这种繁琐性，它维护了两个不同的索引，
        一个用于读取，一个用于写入。当你从ByteBuf读取数据时，它的readerIndex将会被递增已经被读取的字节数，同样的，
        当你写入数据时，writerIndex则会递增。readerIndex的最大范围在writerIndex的所在位置，如果试图移动readerIndex超过该值则会触发异常。
        ByteBuf中名称以read或write开头的方法将会递增它们其对应的索引，而名称以get或set开头的方法则不会。
        ByteBuf同样可以指定一个最大容量，试图移动writerIndex超过该值则会触发异常。
        
        1.ByteBuf同样支持在堆内和堆外进行分配。在堆内分配也被称为支撑数组模式，它能在没有使用池化的情况下提供快速的分配和释放。
        2.另一种模式为堆外分配，Java NIO ByteBuffer类在JDK1.4时就已经允许JVM实现通过JNI调用来在堆外分配内存（
        调用malloc()函数在JVM堆外分配内存），这主要是为了避免额外的缓冲区复制操作。
        3.ByteBuf还支持第三种模式，它被称为复合缓冲区，为多个ByteBuf提供了一个聚合视图。在这个视图中，
        你可以根据需要添加或者删除ByteBuf实例，ByteBuf的子类CompositeByteBuf实现了该模式。
            一个适合使用复合缓冲区的场景是HTTP协议，通过HTTP协议传输的消息都会被分成两部分——头部和主体，
            如果这两部分由应用程序的不同模块产生，将在消息发送时进行组装，并且该应用程序还会为多个消息复用相同的消息主体，
            这样对于每个消息都将会创建一个新的头部，产生了很多不必要的内存操作。使用CompositeByteBuf是一个很好的选择，
            它消除了这些额外的复制，以帮助你复用这些消息。
            
            CompositeByteBuf透明的实现了zero-copy，zero-copy其实就是避免数据在两个内存区域中来回的复制。从操作系统层面上来讲，
            zero-copy指的是避免在内核态与用户态之间的数据缓冲区复制（通过mmap避免），而Netty中的zero-copy更偏向于在用户态中的
            数据操作的优化，就像使用CompositeByteBuf来复用多个ByteBuf以避免额外的复制，也可以使用wrap()方法来将一个字节数组
            包装成ByteBuf，又或者使用ByteBuf的slice()方法把它分割为多个共享同一内存区域的ByteBuf，这些都是为了优化内存的使用率
            
    CompositeByteBuf实现零拷贝  
        zero copy
            OS层面
                即所谓的 Zero-copy, 就是在操作数据时, 不需要将数据 buffer 从一个内存区域拷贝到另一个内存区域. 因为少了一次内存的拷贝, 
                因此 CPU 的效率就得到的提升.
                在 OS 层面上的 Zero-copy 通常指避免在 用户态(User-space) 与 内核态(Kernel-space) 之间来回拷贝数据.
                 例如 Linux 提供的 mmap 系统调用, 它可以将一段用户空间内存映射到内核空间, 当映射成功后, 用户对这段内存区域的修改
                 可以直接反映到内核空间; 同样地, 内核空间对这段区域的修改也直接反映用户空间. 正因为有这样的映射关系, 我们就不需要在 
                 用户态(User-space) 与 内核态(Kernel-space) 之间拷贝数据, 提高了数据传输的效率.
                而需要注意的是, Netty 中的 Zero-copy 与上面我们所提到到 OS 层面上的 Zero-copy 不太一样,
                 Netty的 Zero-coyp 完全是在用户态(Java 层面)的, 它的 Zero-copy 的更多的是偏向于 优化数据操作 这样的概念.
            
            Netty 的 Zero-copy 体现在如下几个个方面:
                Netty 提供了 CompositeByteBuf 类, 它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf, 避免了各个 ByteBuf 之间的拷贝.
                通过 wrap 操作, 我们可以将 byte[] 数组、ByteBuf、ByteBuffer等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作.
                ByteBuf 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf, 避免了内存的拷贝.
                通过 FileRegion 包装的FileChannel.tranferTo 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel,
                 避免了传统通过循环 write 方式导致的内存拷贝问题.

        不过需要注意的是, 虽然看起来 CompositeByteBuf 是由两个 ByteBuf 组合而成的, 不过在 CompositeByteBuf 内部, 这两个 
        ByteBuf 都是单独存在的, CompositeByteBuf 只是逻辑上是一个整体.
        那么其实 compositeByteBuf 的 writeIndex 仍然是0, 因此此时我们就不可能从 compositeByteBuf 中读取到数据, 
        这一点希望大家要特别注意.
        除了上面直接使用 CompositeByteBuf 类外, 我们还可以使用 Unpooled.wrappedBuffer 方法, 它底层封装了 CompositeByteBuf 操作,
        因此使用起来更加方便:
        
        通过 slice 操作实现零拷贝
            
        为了优化内存使用率，Netty提供了一套手动的方式来追踪不活跃对象，像UnpooledHeapByteBuf这种分配在堆内的对象得益于JVM的GC管理，
        无需额外操心，而UnpooledDirectByteBuf是在堆外分配的，它的内部基于DirectByteBuffer，DirectByteBuffer会先向Bits类申请
        一个额度（Bits还拥有一个全局变量totalCapacity，记录了所有DirectByteBuffer总大小），每次申请前都会查看是否已经超过
        -XX:MaxDirectMemorySize所设置的上限，如果超限就会尝试调用Sytem.gc()，以试图回收一部分内存，然后休眠100毫秒，
        如果内存还是不足，则只能抛出OOM异常。堆外内存的回收虽然有了这么一层保障，但为了提高性能与使用率，主动回收也是很有必要的。
        由于Netty还实现了ByteBuf的池化，像PooledHeapByteBuf和PooledDirectByteBuf就必须依赖于手动的方式来进行回收（放回池中）。
        
        Netty使用了引用计数器的方式来追踪那些不活跃的对象。引用计数的接口为ReferenceCounted，它的思想很简单，只要ByteBuf对象的引用计数大于0，
        就保证该对象不会被释放回收，可以通过手动调用release()与retain()方法来操作该对象的引用计数值递减或递增。用户也可以通过自定义
        一个ReferenceCounted的实现类，以满足自定义的规则。
        
    *Channel
        Netty中的Channel与Java NIO的概念一样，都是对一个实体或连接的抽象，但Netty提供了一套更加通用的API。
        
        每个Channel最终都会被分配一个ChannelPipeline和ChannelConfig，前者持有所有负责处理入站与出站数据以及事件的ChannelHandler，
        后者包含了该Channel的所有配置设置，并且支持热更新，由于不同的传输类型可能具有其特别的配置，所以该类可能会实现为ChannelConfig的
        不同子类。
        
        Channel是线程安全的（与之后要讲的线程模型有关），因此你完全可以在多个线程中复用同一个Channel，
        
    *ChannelHandler
        ChannelHandler充当了处理入站和出站数据的应用程序逻辑的容器，该类是基于事件驱动的，它会响应相关的事件然后去调用其关联的回调函数，
        例如当一个新的连接被建立时，ChannelHandler的channelActive()方法将会被调用。
        
        ChannelHandler的主要用途包括：
            对入站与出站数据的业务逻辑处理
            记录日志
            将数据从一种格式转换为另一种格式，实现编解码器。以一次HTTP协议（或者其他应用层协议）的流程为例，数据在网络传输时的单位为字节，
            当客户端发送请求到服务器时，服务器需要通过解码器（处理入站消息）将字节解码为协议的消息内容，服务器在发送响应的时候（处理出站消息），
            还需要通过编码器将消息内容编码为字节。
            捕获异常
            提供Channel生命周期内的通知，如Channel活动时与非活动时
        
        Netty中到处都充满了异步与事件驱动，而回调函数正是用于响应事件之后的操作。由于异步会直接返回一个结果，
        所以Netty提供了ChannelFuture（实现了java.util.concurrent.Future）来作为异步调用返回的占位符，
        真正的结果会在未来的某个时刻完成，到时候就可以通过ChannelFuture对其进行访问，每个Netty的出站I/O操作
        都将会返回一个ChannelFuture。
        Netty还提供了ChannelFutureListener接口来监听ChannelFuture是否成功，并采取对应的操作。
        Channel channel = ...
        ChannelFuture future = channel.connect(new InetSocketAddress("192.168.0.1",6666));
        // 注册一个监听器
        future.addListener(new ChannelFutureListener() {
        	@Override
        	public void operationComplete(ChannelFuture future) {
        		if (future.isSuccess()) {
        			// do something....
        		} else {
        			// 输出错误信息
        			Throwable cause = future.cause();
        			cause.printStackTrace();
        			// do something....
        		}
        	}
        });
        
        
        入站消息与出站消息由其对应的接口ChannelInboundHandler与ChannelOutboundHandler负责，
        这两个接口定义了监听Channel的生命周期的状态改变事件的回调函数。
        
        对于处理入站消息，另外一种选择是继承SimpleChannelInboundHandler，它是Netty的一个继承于ChannelInboundHandlerAdapter的抽象类，
        并在其之上实现了自动释放资源的功能。
        我们在了解ByteBuf时就已经知道了Netty使用了一套自己实现的引用计数算法来主动释放资源，假设你的ChannelHandler继承于
        ChannelInboundHandlerAdapter或ChannelOutboundHandlerAdapter，那么你就有责任去管理你所分配的ByteBuf，
        一般来说，一个消息对象（ByteBuf）已经被消费（或丢弃）了，并且不会传递给ChannelHandler链中的下一个处理器
        （如果该消息到达了实际的传输层，那么当它被写入或Channel关闭时，都会被自动释放），那么你就需要去手动释放它。通过一个简单的工具类
        ReferenceCountUtil的release方法，就可以做到这一点。
        
    *ChannelPipeline
        
        为了模块化与解耦合，不可能由一个ChannelHandler来完成所有应用逻辑，所以Netty采用了拦截器链的设计。ChannelPipeline就是用来管理
        ChannelHandler实例链的容器，它的职责就是保证实例链的流动。
        每一个新创建的Channel都将会被分配一个新的ChannelPipeline，这种关联关系是永久性的，一个Channel一生只能对应一个ChannelPipeline。
        
        一个入站事件被触发时，它会先从ChannelPipeline的最左端（头部）开始一直传播到ChannelPipeline的最右端（尾部），
        而出站事件正好与入站事件顺序相反（从最右端一直传播到最左端）。这个顺序是定死的，Netty总是将ChannelPipeline的入站口作为头部，
        而将出站口作为尾部。在事件传播的过程中，ChannelPipeline会判断下一个ChannelHandler的类型是否和事件的运动方向相匹配，如果不匹配，
        就跳过该ChannelHandler并继续检查下一个（保证入站事件只会被ChannelInboundHandler处理），一个ChannelHandler也可以同时实现
        ChannelInboundHandler与ChannelOutboundHandler，它在入站事件与出站事件中都会被调用。
        
        在阅读ChannelHandler的源码时，发现很多方法需要一个ChannelHandlerContext类型的参数，该接口是ChannelPipeline与
        ChannelHandler之间相关联的关键。ChannelHandlerContext可以通知ChannelPipeline中的当前ChannelHandler的下一个ChannelHandler，
        还可以动态地改变当前ChannelHandler在ChannelPipeline中的位置（通过调用ChannelPipeline中的各种方法来修改）。
        ChannelHandlerContext负责了在同一个ChannelPipeline中的ChannelHandler与其他ChannelHandler之间的交互，
        每个ChannelHandlerContext都对应了一个ChannelHandler。在DefaultChannelPipeline的源码中，已经表现的很明显了。
        
    *EventLoop
        为了最大限度地提供高性能和可维护性，Netty设计了一套强大又易用的线程模型。在一个网络框架中，最重要的能力是能够快速高效地处理
        在连接的生命周期内发生的各种事件，与之相匹配的程序构造被称为事件循环，Netty定义了接口EventLoop来负责这项工作。
                
        如果是经常用Java进行多线程开发的童鞋想必经常会使用到线程池，也就是Executor这套API。Netty就是从Executor（java.util.concurrent）
        之上扩展了自己的EventExecutorGroup（io.netty.util.concurrent），同时为了与Channel的事件进行交互，还扩展了EventLoopGroup接口
        （io.netty.channel）。在io.netty.util.concurrent包下的EventExecutorXXX负责实现线程并发相关的工作，而在io.netty.channel包下
        的EventLoopXXX负责实现网络编程相关的工作（处理Channel中的事件）。
        
        在Netty的线程模型中，一个EventLoop将由一个永远不会改变的Thread驱动，而一个Channel一生只会使用一个EventLoop
        （但是一个EventLoop可能会被指派用于服务多个Channel），在Channel中的所有I/O操作和事件都由EventLoop中的线程处理，
        也就是说一个Channel的一生之中都只会使用到一个线程。不过在Netty3，只有入站事件会被EventLoop处理，所有出站事件都会由调用线程处理，
        这种设计导致了ChannelHandler的线程安全问题。Netty4简化了线程模型，通过在同一个线程处理所有事件，既解决了这个问题，
        还提供了一个更加简单的架构。
        
        为了确保一个Channel的整个生命周期中的I/O事件会被一个EventLoop负责，Netty通过inEventLoop()方法来判断当前执行的线程的身份，
        确定它是否是分配给当前Channel以及它的EventLoop的那一个线程。如果当前（调用）线程正是EventLoop中的线程，那么所提交的任务将会被直接执行，
        否则，EventLoop将调度该任务以便稍后执行，并将它放入内部的任务队列（每个EventLoop都有它自己的任务队列，从SingleThreadEventLoop
        的源码就能发现很多用于调度内部任务队列的方法），在下次处理它的事件时，将会执行队列中的那些任务。这种设计可以让任何线程与Channel直接交互，
        而无需在ChannelHandler中进行额外的同步。
        从性能上来考虑，千万不要将一个需要长时间来运行的任务放入到任务队列中，它会影响到该队列中的其他任务的执行。解决方案是使用一个专门的
        EventExecutor来执行它（ChannelPipeline提供了带有EventExecutorGroup参数的addXXX()方法，该方法可以将传入的ChannelHandler
        绑定到你传入的EventExecutor之中），这样它就会在另一条线程中执行，与其他任务隔离。
        
        EventLoopGroup负责管理和分配EventLoop（创建EventLoop和为每个新创建的Channel分配EventLoop），根据不同的传输类型，
        EventLoop的创建和分配方式也不同。例如，使用NIO传输类型，EventLoopGroup就会只使用较少的EventLoop（一个EventLoop服务于多个Channel），
        这是因为NIO基于I/O多路复用，一个线程可以处理多个连接，而如果使用的是OIO，那么新创建一个Channel（连接）就需要分配一个EventLoop（线程）。
        
        EventLoopGroup是一组EventLoop的抽象，由于Netty对Reactor模式进行了变种，实际上为更好的利用多核CPU资源，
        Netty实例中一般会有多个EventLoop同时工作，每个EventLoop维护着一个Selector实例，类似单线程Reactor模式地工作着。
        至于多少线程可有用户决定，Netty也根据实际上的处理器核数提供了一个默认的数字，我们也建议使用这个数字
        Runtime.getRuntime().availableProcessors() * 2
        
        EventLoopGroup提供next接口，可以总一组EventLoop里面按照一定规则获取其中一个EventLoop来处理任务，
        对于EventLoopGroup这里需要了解的是在Netty中，在Netty服务器编程中我们需要BossEventLoopGroup和WorkerEventLoopGroup
        两个EventLoopGroup来进行工作。通常一个服务端口即一个ServerSocketChannel对应一个Selector和一个EventLoop线程，
        也就是我们建议BossEventLoopGroup的线程数参数这是为1。BossEventLoop负责接收客户端的连接并将SocketChannel交给
        WorkerEventLoopGroup来进行IO处理。
        
        如上图，BossEventLoopGroup通常是一个单线程的EventLoop，EventLoop维护着一个注册了ServerSocketChannel的Selector实例，
        BoosEventLoop不断轮询Selector将连接事件分离出来，通常是OP_ACCEPT事件，然后将accept得到的SocketChannel交给WorkerEventLoopGroup，
        WorkerEventLoopGroup会由next选择其中一个EventLoopGroup来将这个SocketChannel注册到其维护的Selector并对其后续的IO事件进行处理。
        在Reactor模式中BossEventLoopGroup主要是对多线程的扩展，而每个EventLoop的实现涵盖IO事件的分离，和分发（Dispatcher）。
        
        关于IO密集型和CPU密集型的思考
        
        Netty基于单线程设计的EventLoop能够同时处理成千上万的客户端连接的IO事件，缺点是单线程不能够处理时间过长的任务，
        这样会阻塞使得IO事件的处理被阻塞，严重的时候回造成IO事件堆积，服务不能够高效响应客户端请求。所谓时间过长的任务通常是占用CPU资源
        比较长的任务，也即CPU密集型，对于业务应用也可能是业务代码的耗时。这点和Node是极其相似的，我可以认为这是基于单线程的EventLoop
        模型的通病，我们不能够将过长的任务交给这个单线程来处理，也就是不适合CPU密集型应用。那么问题怎么解决呢，参照Node的解决方案，
        当我们遇到需要处理时间很长的任务的时候，我们可以将它交给子线程来处理，主线程继续去EventLoop，当子线程计算完毕再讲结果交给主线程。
        这也是通常基于Netty的应用的解决方案，通常业务代码执行时间比较长，我们不能够把业务逻辑交给这个单线程来处理，因此我们需要额外的
        线程池来分配线程资源来专门处理耗时较长的业务逻辑，这是比较通用的设计方案。
        
    *Bootstrap
        在深入了解地Netty的核心组件之后，发现它们的设计都很模块化，如果想要实现你自己的应用程序，就需要将这些组件组装到一起。
        Netty通过Bootstrap类，以对一个Netty应用程序进行配置（组装各个组件），并最终使它运行起来。对于客户端程序和服务器程序所使用到的
        Bootstrap类是不同的，后者需要使用ServerBootstrap，这样设计是因为，在如TCP这样有连接的协议中，服务器程序往往需要一个以上的Channel，
        通过父Channel来接受来自客户端的连接，然后创建子Channel用于它们之间的通信，而像UDP这样无连接的协议，它不需要每个连接都创建子Channel，
        只需要一个Channel即可。
        
    (推荐阅读)
    https://www.cnblogs.com/heavenhome/articles/6554262.html
        
                
                
##spark
https://blog.csdn.net/pengzonglu7292/article/details/80554507   (面试题)
https://juejin.im/post/58d8daedac502e0058d9edf0 (整体介绍)

    spark背景
        什么是spark
            Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，
            Spark是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，
            允许用户将Spark部署在大量廉价硬件之上，形成集群。
        Spark与Hadoop
            Spark是一个计算框架,而Hadoop中包含计算框架MapReduce和分布式文件系统HDFS,Hadoop更广泛地说还包括在其生态系统上的其他系统.
        为什么使用Spark?
            Hadoop的MapReduce计算模型存在问题:
            Hadoop的MapReduce的核心是Shuffle(洗牌).在整个Shuffle的过程中,至少产生6次I/O流.基于MapReduce计算引擎通常会将结果
            输出到次盘上,进行存储和容错.另外,当一些查询(如:hive)翻译到MapReduce任务是,往往会产生多个Stage,而这些Stage有依赖底层
            文件系统来存储每一个Stage的输出结果,而I/O的效率往往较低,从而影响MapReduce的运行速度.
        Spark的特点: 快, 易用, 通用,兼容性
            快：与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。
                Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。
            易用：Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式
                的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。
            通用：Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、
                机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。
            兼容性：Spark 可以非常方便地与其他的开源产品进行融合。比如，Spark 可以使用Hadoop 的 YARN 和 Apache Mesos 作为它的
                资源管理和调度器.并且可以处理所有 Hadoop 支持的数据，包括 HDFS、HBase 和 Cassandra 等。这对于已经部署Hadoop 集群
                的用户特别重要，因为不需要做任何数据迁移就可以使用 Spark 的强大处理能力。Spark 也可以不依赖于第三方的资源管理和调度器，
                它实现了Standalone 作为其内置的资源管理和调度框架，这样进一步降低了 Spark 的使用门槛，使得所有人都可以非常容易地部署和使用
                 Spark。此外，Spark 还提供了在EC2 上部Standalone 的 Spark 集群的工具。
        Spark的生态系统
            1.Spark Streaming:
                Spark Streaming基于微批量方式的计算和处理,可以用于处理实时的流数据.它使用DStream,简单来说是一个弹性分布式数据集(RDD)系列,
                处理实时数据.数据可以从Kafka,Flume,Kinesis或TCP套接字等众多来源获取,并且可以使用由高级函数（如 map，reduce，join 和 window）
                开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。
            2.Spark SQL
                SPark SQL可以通过JDBC API将Spark数据集暴露出去,而且还可以用传统的BI和可视化工具在Spark数据上执行类似SQL的查询,
                用户哈可以用Spark SQL对不同格式的数据(如Json, Parque以及数据库等)执行ETl,将其转化,然后暴露特定的查询.
            3.Spark MLlib
                MLlib是一个可扩展的Spark机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。
            4.Spark Graphx:
                GraphX是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），
                一种顶点和边都带有属性的有向多重图，扩展了Spark RDD。为了支持图计算，GraphX暴露了一个基础操作符集合（如subgraph，
                joinVertices和aggregateMessages）和一个经过优化的Pregel API变体。此外，GraphX还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。
            5.Tachyon
                Tachyon是一个以内存为中心的分布式文件系统,能够提供内存级别速度的跨集群框架(如Spark和mapReduce)的可信文件共享.它将工作集文件缓存在内存中,从而避免到磁盘中加载需要经常读取的数据集,通过这一机制,不同的作业/查询和框架可以内存级的速度访问缓存文件.
                此外，还有一些用于与其他产品集成的适配器，如Cassandra（Spark Cassandra 连接器）和R（SparkR）。Cassandra Connector可用于访问存储在Cassandra数据库中的数据并在这些数据上执行数据分析。
            6.Mesos
                Mesos是一个资源管理框架
                提供类似于YARN的功能
                用户可以在其中插件式地运行Spark,MapReduce,Tez等计算框架任务
                Mesos对资源和任务进行隔离,并实现高效的资源任务调度
            7.BlinkDB
                BlinkDB是一个用于在海量数据上进行交互式SQL的近似查询引擎
                允许用户通过查询准确性和查询时间之间做出权衡,完成近似查询
                核心思想:通过一个自适应优化框架,随着时间的推移,从原始数据建立并维护一组多维样本,通过一个动态样本选择策略,选择一个适当大小的示例,然后基于查询的准确性和响应时间满足用户查询需求
    
        Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。
            Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；
            Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行.
        Spark集群部署后,需要在主节点和从节点分别启动master进程和Worker进程,对整个集群进行控制.在一个Spark应用的执行程序中.
            Driver和Worker是两个重要的角色.Driver程序是应用逻辑执行的起点，负责作业的调度,即Task任务的发布,而多个Worker用来管理计算
            节点和创建Executor并行处理任务.在执行阶段,Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器.
            同时Executor对相应数据分区的任务进行处理.
        Sparkde架构中的基本组件:
            ClusterManager:在standlone模式中即为Master(主节点),控制整个集群.监控Worker.在Yarn模式中为资源管理器.
            Worker:从节点,负责控制计算节点,启动Ex而粗投入或Driver
            NodeManager:负责计算节点的控制。
            Driver:运行Application的main() 函数并创建SparkContext
            Executor: 执行器,在worker node上执行任务组件,用于启动线程执行任务.每个Application拥有独立的一组Executors
            SparkContext: 整个应用的上下文,监控应用的生命周期
            RDD:弹性分布式集合,spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph
            DAG Scheduler: 根据作业(Job)构建基于Stage的DAG,并交给Stage给TaskScheduler
            TaskScheduler：将任务（Task）分发给Executor执行
            SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。SparkEnv内创建并包含如下一些重要组件的引用。
            MapOutPutTracker：负责Shuffle元信息的存储。
            BroadcastManager：负责广播变量的控制与元信息的存储。
            BlockManager：负责存储管理、创建和查找块。
            MetricsSystem：监控运行时性能指标信息。
            SparkConf：负责存储配置信息。
            *Spark的整体流程:client提交应用,Master找到一个Worker启动Driver,Driver向Master或者向资源管理器申请资源,之后将应用转化为RDD Graph，
            再由DAGScheduler将RDD Graph转化为Stage的有向无环图提交给TaskScheduler，由TaskScheduler提交任务给Executor执行。
            在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行。

        spark解决单点问题
            启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：http://node-1:8080/
            到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，
            并且启动至少两个Master节点来实现高可靠，配置方式比较简单：
            Spark集群规划：node-1，node-2是Master；node-3，node-4，node-5是Worker
            安装配置zk集群，并启动zk集群
            停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置
            export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 
            -Dspark.deploy.zookeeper.dir=/spark"
            1.在node1节点上修改slaves配置文件内容指定worker节点
            2.在node1上执行$SPARK_HOME/sbin/start-all.sh，然后在node2上执行$SPARK_HOME/sbin/start-master.sh启动第二个Master
    
    基本概念
        spark RDD
            RDD概述
                什么是RDD
                    RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、
                    里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时
                    显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。
                RDD的属性
                    一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。
                    用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。
                    一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数
                    会对迭代器进行复合，不需要保存每次计算的结果。
                    RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分
                    分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。
                    一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，
                    另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的
                    Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。
                    一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个
                    Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配
                    到其所要处理数据块的存储位置。
                RDD创建
                    一旦分布式数据集（distData）被创建好，它们将可以被并行操作。例如，我们可以调用distData.reduce(lambda a, b: a + b)
                    来将数组的元素相加。我们会在后续的分布式数据集运算中进一步描述。
                    并行集合的一个重要参数是slices，表示数据集切分的份数。Spark将会在集群上为每一份数据起一个任务。典型地，
                    你可以在集群的每个CPU上分布2-4个slices. 一般来说，Spark会尝试根据集群的状况，来自动设定slices的数目。然而，
                    你也可以通过传递给parallelize的第二个参数来进行手动设置。（例如：sc.parallelize(data, 10)).

        Spark 分区(Partition)的认识、理解和应用
        https://blog.csdn.net/zhangzeyuan56/article/details/80935034
            什么是分区以及为什么要分区?
                Spark RDD 是一种分布式的数据集，由于数据量很大，因此要它被切分并存储在各个结点的分区当中。从而当我们对RDD进行操作时，
                实际上是对每个分区中的数据并行操作。
            分区的3种方式
                1、HashPartitioner
                scala> val counts = sc.parallelize(List((1,'a'),(1,'aa'),(2,'b'),(2,'bb'),(3,'c')), 3)
                .partitionBy(new HashPartitioner(3))
                HashPartitioner确定分区的方式：partition = key.hashCode () % numPartitions
                
                2、RangePartitioner
                scala> val counts = sc.parallelize(List((1,'a'),(1,'aa'),(2,'b'),(2,'bb'),(3,'c')), 3)
                .partitionBy(new RangePartitioner(3,counts))
                RangePartitioner会对key值进行排序，然后将key值被划分成3份key值集合。
                
                3、CustomPartitioner
                CustomPartitioner可以根据自己具体的应用需求，自定义分区。
                class CustomPartitioner(numParts: Int) extends Partitioner {
                 override def numPartitions: Int = numParts
                 override def getPartition(key: Any): Int =
                 {
                       if(key==1)){
                	0
                       } else if (key==2){
                       1} else{ 
                       2 }
                  } 
                }
                scala> val counts = sc.parallelize(List((1,'a'),(1,'aa'),(2,'b'),(2,'bb'),(3,'c')), 3).partitionBy(new CustomPartitioner(3))

            *理解从HDFS读入文件默认是怎样分区的
                Spark从HDFS读入文件的分区数默认等于HDFS文件的块数(blocks)，HDFS中的block是分布式存储的最小单元。
                如果我们上传一个30GB的非压缩的文件到HDFS，HDFS默认的块容量大小128MB，因此该文件在HDFS上会被分为235块(30GB/128MB)；
                Spark读取SparkContext.textFile()读取该文件，默认分区数等于块数即235。
                
                
            如何设置合理的分区数
                1、分区数越多越好吗？
                不是的，分区数太多意味着任务数太多，每次调度任务也是很耗时的，所以分区数太多会导致总体耗时增多。
                2、分区数太少会有什么影响？
                分区数太少的话，会导致一些结点没有分配到任务；另一方面，分区数少则每个分区要处理的数据量就会增大，从而对每个结点的内存
                要求就会提高；还有分区数不合理，会导致数据倾斜问题。
                3、合理的分区数是多少？如何设置？
                总核数=executor-cores * num-executor 
                一般合理的分区数设置为总核数的2~3倍
                
        *spark内存管理
            下文介绍的内存模型全部指 Executor 端的内存模型， Driver 端的内存模型本文不做介绍。统一内存管理模块包括了堆内内存
            (On-heap Memory)和堆外内存(Off-heap Memory)两大区域，下面对这两块区域进行详细的说明
            
            *堆内内存(On-heap Memory)
                默认情况下，Spark 仅仅使用了堆内内存。Executor 端的堆内内存区域大致可以分为以下四大块：
                
                1.Execution 内存：主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据
                2.Storage 内存：主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据；
                3.用户内存（User Memory）：主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息。
                4.预留内存（Reserved Memory）：系统预留内存，会用来存储Spark内部对象。
                
                systemMemory = Runtime.getRuntime.maxMemory，其实就是通过参数 spark.executor.memory 或 --executor-memory 配置的。
                reservedMemory 在 Spark 2.2.1 中是写死的，其值等于 300MB，这个值是不能修改的（如果在测试环境下，我们可以通过 spark.testing.reservedMemory 参数进行修改）；
                usableMemory = systemMemory - reservedMemory，这个就是 Spark 可用内存；
                
            堆外内存(Off-heap Memory)
                默认情况下，堆外内存是关闭的，我们可以通过 spark.memory.offHeap.enabled 参数启用，并且通过 
                spark.memory.offHeap.size 设置堆外内存大小，单位为字节。如果堆外内存被启用，那么 Executor 
                内将同时存在堆内和堆外内存，两者的使用互补影响，这个时候 Executor 中的 Execution 内存是堆内的 
                Execution 内存和堆外的 Execution 内存之和，同理，Storage 内存也一样。相比堆内内存，堆外内存只区分 
                Execution 内存和 Storage 内存，其内存分布如下图所示：

            *Execution 内存和 Storage 内存动态调整
            https://www.iteblog.com/archives/2342.html
                细心的同学肯定看到上面两张图中的 Execution 内存和 Storage 内存之间存在一条虚线，这是为什么呢？
                
                用过 Spark 的同学应该知道，在 Spark 1.5 之前，Execution 内存和 Storage 内存分配是静态的，换句话说就是如果
                 Execution 内存不足，即使 Storage 内存有很大空闲程序也是无法利用到的；反之亦然。这就导致我们很难进行内存的调优工作，
                 我们必须非常清楚地了解 Execution 和 Storage 两块区域的内存分布。而目前 Execution 内存和 Storage 内存可以互相共享的。
                 也就是说，如果 Execution 内存不足，而 Storage 内存有空闲，那么 Execution 可以从 Storage 中申请空间；反之亦然。
                 所以上图中的虚线代表 Execution 内存和 Storage 内存是可以随着运作动态调整的，这样可以有效地利用内存资源。
                 Execution 内存和 Storage 内存之间的动态调整可以概括如下：
                    
                具体的实现逻辑如下：
                
                程序提交的时候我们都会设定基本的 Execution 内存和 Storage 内存区域（通过 spark.memory.storageFraction 参数设置）；
                在程序运行时，如果双方的空间都不足时，则存储到硬盘；将内存中的块存储到磁盘的策略是按照 LRU 规则进行的。
                若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）
                Execution 内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后"归还"借用的空间
                Storage 内存的空间被对方占用后，目前的实现是无法让对方"归还"，因为需要考虑 Shuffle 过程中的很多因素，
                实现起来较为复杂；而且 Shuffle 过程产生的文件在后面一定会被使用到，而 Cache 在内存的数据不一定在后面使用。
                注意，上面说的借用对方的内存需要借用方和被借用方的内存类型都一样，都是堆内内存或者都是堆外内存，
                不存在堆内内存不够去借用堆外内存的空间。
            
            *Task 之间内存分布
                为了更好地使用使用内存，Executor 内运行的 Task 之间共享着 Execution 内存。具体的，Spark 内部维护了一个 HashMap
                 用于记录每个 Task 占用的内存。当 Task 需要在 Execution 内存区域申请 numBytes 内存，其先判断 HashMap 
                 里面是否维护着这个 Task 的内存使用情况，如果没有，则将这个 Task 内存使用置为0，并且以 TaskId 为 key，内存使用为 
                 value 加入到 HashMap 里面。之后为这个 Task 申请 numBytes 内存，如果 Execution 内存区域正好有大于 numBytes 
                 的空闲内存，则在 HashMap 里面将当前 Task 使用的内存加上 numBytes，然后返回；如果当前 Execution 内存区域
                 无法申请到每个 Task 最小可申请的内存，则当前 Task 被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。
                 每个 Task 可以使用 Execution 内存大小范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的 Task 个数。一个 
                 Task 能够运行必须申请到最小内存为 (1/2N * Execution 内存)；当 N = 1 的时候，Task 可以使用全部的 Execution 内存。
                
                比如如果 Execution 内存大小为 10GB，当前 Executor 内正在运行的 Task 个数为5，则该 Task 可以申请的内存范围为
                 10 / (2 * 5) ~ 10 / 5，也就是 1GB ~ 2GB的范围。
        
        *shuffle
        https://blog.csdn.net/shujuelin/article/details/84100842    (比较详细,必看)
            Shuffle简介
                Shuffle描述着数据从map task输出到reduce task输入的这段过程。shuffle是连接Map和Reduce之间的桥梁，
                Map的输出要用到Reduce中必须经过shuffle这个环节，shuffle的性能高低直接影响了整个程序的性能和吞吐量。
                因为在分布式情况下，reduce task需要跨节点去拉取其它节点上的map task结果。这一过程将会产生网络资源消耗和内存，
                磁盘IO的消耗。通常shuffle分为两部分：Map阶段的数据准备和Reduce阶段的数据拷贝处理。一般将在map端的Shuffle称之
                为Shuffle Write，在Reduce端的Shuffle称之为Shuffle Read.
            Hadoop MapReduce Shuffle
                Apache Spark 的 Shuffle 过程与 Apache Hadoop 的 Shuffle 过程有着诸多类似，一些概念可直接套用，例如，
                Shuffle 过程中，提供数据的一端，被称作 Map 端，Map 端每个生成数据的任务称为 Mapper，对应的，接收数据的一端，
                被称作 Reduce 端，Reduce 端每个拉取数据的任务称为 Reducer，Shuffle 过程本质上都是将 Map 端获得的数据使用分区器
                进行划分，并将数据发送给对应的 Reducer 的过程。
        
            map端的Shuffle简述:
                1)input, 根据split输入数据，运行map任务;
                2)patition, 每个map task都有一个内存缓冲区，存储着map的输出结果;
                3)spill, 当缓冲区快满的时候需要将缓冲区的数据以临时文件的方式存放到磁盘;
                4)merge, 当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。
            reduce 端的Shuffle简述:
                reduce task在执行之前的工作就是不断地拉取当前job里每个map task的最终结果，然后对从不同地方拉取过来的数据不断地做merge，也最终形成一个文件作为reduce task的输入文件。
                1) Copy过程，拉取数据。
                2)Merge阶段，合并拉取来的小文件
                3)Reducer计算
                4)Output输出计算结果
        
            什么时候需要 shuffle writer
            https://www.cnblogs.com/itboys/p/9201750.html
                
                中间就涉及到shuffle 过程，前一个stage 的 ShuffleMapTask 进行 shuffle write， 把数据存储在 blockManager 上面，
                 并且把数据位置元信息上报到 driver 的 mapOutTrack 组件中， 下一个 stage 根据数据位置元信息， 进行 shuffle read，
                  拉取上个stage 的输出数据。
                这篇文章讲述的就是其中的 shuffle write 过程。
                
            shuffle 写
                版本二的优点:就是为了减少这么多小文件的生成 
                bucket的数量=cpu*resultTask的个数 
                版本二设计的原理:一个shuffleMapTask还是会写入resultTask对应个数的本地文件，但是当下一个shuffleMapTask运行的时候会直接把数据写到之前已经建立好的本地文件，这个文件可以复用，这种复用机制叫做consolidation机制 
                我们把这一组的shuffle文件称为shuffleGroup,每个文件中都存储了很多shuffleMapTask对应的数据，这个文件叫做segment,这个时候因为不同的shuffleMapTask都是存在一个文件中 
                所以建立索引文件，来标记shuffleMapTask在shuffleBlockFile的位置+偏移量，这样就可以在一个文件里面把不同的shuffleMaptask数据分出来 
                spark shuffle的版本三 
                版本三的优点：是通过排序建立索引，相比较于版本二，它只有一个临时文件，不管有多少个resultTask都只有一个临时文件， 
                缺点:这个排序操作是一个消耗CPU的操作，代价是会消耗很多的cpu 
                版本二占用内存多，打开文件多，但不需排序，速度快。版本三占用内存少，打开文件少，速度相对慢。实践证明使用第二种方案的应用场景更多些。 
                shuffle的读流程 
                
            shuffle 读流程 
                1.有一个类blockManager，封装了临时文件的位置信息,resultTask先通过blockManager,就知道我从哪个节点拿数据 
                如果是远程，它就是发起一次socket请求，创建一个socket链接。然后发起一次远程调用，告诉远程的读取程序，读取哪些数据。读到的内容再通过socket传过来。 
                2.一条条读数据和一块块读数据的优缺点？ 
                如果是一条条读取的话，实时性好，性能低下
                
                一块块读取的话性能高，但是实时性不好
                Shuffle读由reduce这边发起，它需要先到临时文件中读，一般这个临时文件和reduce不在一台节点上，它需要跨网络去读。但也不排除在一台服务器。不论如何它需要知道临时文件的位置， 
                这个是谁来告诉它的呢？它有一个BlockManager的类。这里就知道将来是从本地文件中读取，还是需要从远程服务器上读取。 
                读进来后再做join或者combine的运算。 
                这些临时文件的位置就记录在Map结构中。 
                可以这样理解分区partition是RDD存储数据的地方，实际是个逻辑单位，真正要取数据时，它就调用BlockManage去读，它是以数据块的方式来读。 
                比如一次读取32k还是64k。它不是一条一条读，一条一条读肯定性能低。它读时首先是看本地还是远程，如果是本地就直接读这个文件了， 
                如果是远程，它就是发起一次socket请求，创建一个socket链接。然后发起一次远程调用，告诉远程的读取程序，读取哪些数据。读到的内容再通过socket传过来。
            
            Spark中的shuffle是在干嘛？
                Shuffle在Spark中即是把父RDD中的KV对按照Key重新分区，从而得到一个新的RDD。也就是说原本同属于父RDD同一个分区的数据需要进入到子RDD的不同的分区。
                但这只是shuffle的过程，却不是shuffle的原因。为何需要shuffle呢？
                
            Shuffle和Stage
                在分布式计算框架中，比如map-reduce，数据本地化是一个很重要的考虑，即计算需要被分发到数据所在的位置，从而减少数据的移动，提高运行效率。
                Map-Reduce的输入数据通常是HDFS中的文件，所以数据本地化要求map任务尽量被调度到保存了输入文件的节点执行。但是，
                有一些计算逻辑是无法简单地获取本地数据的，reduce的逻辑都是如此。对于reduce来说，处理函数的输入是key相同的所有value，
                但是这些value所在的数据集(即map的输出)位于不同的节点上，因此需要对map的输出进行重新组织，使得同样的key进入相同的reducer。
                 shuffle移动了大量的数据，对计算、内存、网络和磁盘都有巨大的消耗，因此，只有确实需要shuffle的地方才应该进行shuffle。
                
            Stage的划分
                对于Spark来说，计算的逻辑存在于RDD的转换逻辑中。Spark的调度器也是在依据数据本地化在调度任务，只不过此处的“本地”不仅包括磁盘文件，
                也包括RDD的分区， Spark会使得数据尽量少地被移动，据此，DAGScheduler把一个job划分为多个Stage，在一个Stage内部，
                数据是不需要移动地，数据会在本地经过一系列函数的处理，直至确实需要shuffle的地方。
        
        spark数据读取 
            CSV
            TSV 
                val spark = SparkSession.builder().appName("fileRead").getOrCreate()
                        import spark.implicits._
                        val data1 = spark.read
                            //          推断数据类型
                            .option("inferSchema", true)
                            //          设置空值
                            .option("nullValue", "?")
                            //          表示有表头，若没有则为false
                            .option("header", true)
                            //          文件路径
                            .csv("ds/block_10.csv")
                            //          缓存
                            .cache()
                        //          打印数据格式
                        data1.printSchema()
                        //      显示数据,false参数为不要把数据截断
                        data1.show(false)
                    
            JSON文件
                val jsonpath = "/home/wmx/hive/warehouse/trail/sample40.json"
                val data3 = spark.read.json(jsonpath).cache()
                data3.printSchema()
                // 因为有点多只显示1条，不截断
                data3.show(1,false)
                
                
        列式存储和行式存储相比有哪些优势呢？
            可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。 
            压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。 
            只读取需要的列，支持向量运算，能够获取更好的扫描性能。
        
        spark 入门map reduce 最好的几个例子
        https://blog.csdn.net/u013851082/article/details/70142806
        
    广播变量和累加器
        1、能不能将一个RDD使用广播变量广播出去？
               不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。
        2、 广播变量只能在Driver端定义，不能在Executor端定义。
        3、 在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。
        4、如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本。
        5、如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本。
        如何更新广播变量
        通过unpersist()将老的广播变量删除，然后重新广播一遍新的广播变量
        
        累加器在Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。
        
    Spark job 的执行流程简介
        Spark job 的执行流程简介
        我们可以发现，Spark 应用程序在提交执行后，控制台会打印很多日志信息，这些信息看起来是杂乱无章的，但是却在一定程度上体现了一个被提交的 Spark job 在集群中是如何被调度执行的，那么在这一节，将会向大家介绍一个典型的 Spark job 是如何被调度执行的。
        
        我们先来了解以下几个概念：
        
        DAG: 即 Directed Acyclic Graph，有向无环图，这是一个图论中的概念。如果一个有向图无法从某个顶点出发经过若干条边回到该点，则这个图是一个有向无环图。
        
        Job：我们知道，Spark 的计算操作是 lazy 执行的，只有当碰到一个动作 (Action) 算子时才会触发真正的计算。一个 Job 就是由动作算子而产生包含一个或多个 Stage 的计算作业。
        
        Stage：Job 被确定后,Spark 的调度器 (DAGScheduler) 会根据该计算作业的计算步骤把作业划分成一个或者多个 Stage。Stage 又分为 ShuffleMapStage 和 ResultStage，前者以 shuffle 为输出边界，后者会直接输出结果，其边界可以是获取外部数据，也可以是以一个 ShuffleMapStage 的输出为边界。每一个 Stage 将包含一个 TaskSet。
        
        TaskSet： 代表一组相关联的没有 shuffle 依赖关系的任务组成任务集。一组任务会被一起提交到更加底层的 TaskScheduler。
        
        Task：代表单个数据分区上的最小处理单元。分为 ShuffleMapTask 和 ResultTask。ShuffleMapTask 执行任务并把任务的输出划分到 (基于 task 的对应的数据分区) 多个 bucket(ArrayBuffer) 中,ResultTask 执行任务并把任务的输出发送给驱动程序。
        
        Spark 的作业任务调度是复杂的，需要结合源码来进行较为详尽的分析，但是这已经超过本文的范围，所以这一节我们只是对大致的流程进行分析。
        
        Spark 应用程序被提交后，当某个动作算子触发了计算操作时，SparkContext 会向 DAGScheduler 提交一个作业，接着 DAGScheduler 会根据 RDD 生成的依赖关系划分 Stage，并决定各个 Stage 之间的依赖关系，Stage 之间的依赖关系就形成了 DAG。Stage 的划分是以 ShuffleDependency 为依据的，也就是说当某个 RDD 的运算需要将数据进行 Shuffle 时，这个包含了 Shuffle 依赖关系的 RDD 将被用来作为输入信息，进而构建一个新的 Stage。我们可以看到用这样的方式划分 Stage，能够保证有依赖关系的数据可以以正确的顺序执行。根据每个 Stage 所依赖的 RDD 数据的 partition 的分布，会产生出与 partition 数量相等的 Task，这些 Task 根据 partition 的位置进行分布。其次对于 finalStage 或是 mapStage 会产生不同的 Task，最后所有的 Task 会封装到 TaskSet 内提交到 TaskScheduler 去执行。有兴趣的读者可以通过阅读 DAGScheduler 和 TaskScheduler 的源码获取更详细的执行流程。
    
    数据源自并行集合
        调用 SparkContext 的 parallelize 方法，在一个已经存在的 Scala 集合上创建一个 Seq 对象
    
    外部数据源
        Spark支持任何 Hadoop InputFormat 格式的输入，如本地文件、HDFS上的文件、Hive表、HBase上的数据、Amazon S3、Hypertable等，
        以上都可以用来创建RDD。
        常用函数是 sc.textFile() ,参数是Path和最小分区数[可选]。Path是文件的 URI 地址，该地址可以是本地路径，或者 hdfs://、s3n:// 
        等 URL 地址。其次，使用本地文件时，如果在集群上运行要确保worker节点也能访问到文件
    
    提交应用的脚本和可选参数
        可以选择local模式下运行来测试程序，但要是在集群上运行还需要通过spark-submit脚本来完成。官方文档上的示例是这样写的（其中表明哪些是必要参数）：
        
        ./bin/spark-submit \
          --class <main-class> \
          --master <master-url> \
          --deploy-mode <deploy-mode> \
          --conf <key>=<value> \
          ... # other options
          <application-jar> \
          [application-arguments]
        常用参数如下：
        
        --master 参数来设置 SparkContext 要连接的集群，默认不写就是local[*]【可以不用在SparkContext中写死master信息】
        
        --jars 来设置需要添加到 classpath 中的 JAR 包，有多个 JAR 包使用逗号分割符连接
        
        --class 指定程序的类入口
        
        --deploy-mode 指定部署模式，是在 worker 节点（cluster）上还是在本地作为一个外部的客户端（client）部署您的 driver（默认 : client）
        
        这里顺便提一下yarn-client和yarn-cluster区别 cluster-client
        
        application-jar : 包括您的应用以及所有依赖的一个打包的 Jar 的路径。该Jar包的 URL 在您的集群上必须是全局可见的，例如，一个 hdfs:// path 或者一个 file:// path 在所有节点是可见的。
        
        application-arguments : 传递到您的 main class 的 main 方法的参数
        
        driver-memory是 driver 使用的内存，不可超过单机的最大可使用的
        
        num-executors是创建多少个 executor
        
        executor-memory是各个 executor 使用的最大内存，不可超过单机的最大可使用内存
        
        executor-cores是每个 executor 最大可并发执行的 Task 数目
        
        #如下是spark on yarn模式下运行计算Pi的测试程序
        # 有一点务必注意，每行最后换行时务必多敲个空格，否则解析该语句时就是和下一句相连的，不知道会爆些什么古怪的错误
        [hadoop@master spark-2.4.0-bin-hadoop2.6]$ ./bin/spark-submit \
        > --master yarn \
        > --class org.apache.spark.examples.SparkPi \
        > --deploy-mode client \
        > --driver-memory 1g \
        > --num-executors 2 \
        > --executor-memory 2g \
        > --executor-cores 2 \
        > examples/jars/spark-examples_2.11-2.4.0.jar \
        > 10
        每次提交都写这么多肯定麻烦，可以写个脚本
        
        从文件中加载配置
        spark-submit 脚本可以从一个 properties 文件加载默认的 Spark configuration values 并且传递它们到您的应用中去。默认情况下，它将从 Spark 目录下的 conf/spark-defaults.conf 读取配置。更多详细信息，请看 加载默认配置 部分。
        
        加载默认的 Spark 配置，这种方式可以消除某些标记到 spark-submit 的必要性。例如，如果 spark.master 属性被设置了，您可以在 spark-submit 中安全的省略。一般情况下，明确设置在 SparkConf 上的配置值的优先级最高，然后是传递给 spark-submit 的值，最后才是 default value（默认文件）中的值。
        
        如果您不是很清楚其中的配置设置来自哪里，您可以通过使用 --verbose 选项来运行 spark-submit 打印出细粒度的调试信息
        
        更多内容可参考文档：提交应用 ，Spark-Submit 参数设置说明和考虑
        
        配置参数优先级问题
        sparkConf中配置的参数优先级最高，其次是spark-submit脚本中，最后是默认属性文件（spark-defaults.conf）中的配置参数
        
        默认情况下，spark-submit也会从spark-defaults.conf中读取配置
    
    reduceByKey(func, numPartitions=None)
        也就是，reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。
    groupByKey(numPartitions=None)
        也就是，groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行
        aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey
        不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。

    Spark on Yarn模式下的不同之处
        之前提到过on Yarn有yarn-client和yarn-cluster两种模式，在spark-submit脚本中通过--master、--deploy-mode来区分以哪种方式运行 【具体可见：LearningSpark(2)spark-submit可选参数.md】
        其中，官方文档中所提及--deploy-mode 指定部署模式，是在 worker 节点（cluster）上还是在本地作为一个外部的客户端（client）部署您的 driver（默认 : client），这和接下来所提及的内容有关
        因为是运行在Yarn集群上，所有没有什么Master、Worker节点，取而代之是ResourceManager、NodeManager（下文会以RM、NM代替）
        
        yarn-cluster运行模式
            首先spark-submit提交Application后会向RM发送请求，请求启动ApplicationMaster（同standalone模式下的Master，但同时该节点也会运行Drive进程【这里和yarn-client有区别】）。RM就会分配container在某个NM上启动ApplicationMaster
            要执行task就得有Executor，所以ApplicationMaster要向RM申请container来启动Executor。RM分配一些container（就是一些NM节点）给ApplicationMaster用来启动Executor，ApplicationMaster就会连接这些NM（这里NM就如同Worker）。NM启动Executor后向ApplicationMaster注册
        
        yarn-client运行模式
            如上所提的，这种模式的不同在于Driver是部署在本地提交的那台机器上的。过程大致如yarn-cluster，不同在于ApplicationMaster实际上是ExecutorLauncher，而申请到的NodeManager所启动的Executor是要向本地的Driver注册的，而不是向ApplicationMaster注册

    
    *常见面试题   
        RDD五大特性？
            1、RDD是由一系列的分区组成。
            2、操作一个RDD实际上操作的是RDD的所有分区。
            3、RDD之间存在各种依赖关系。
            4、可选的特性，key-value型的RDD是通过hash进行分区。
            5、RDD的每一个分区在计算时会选择最佳的计算位置。
        
        什么是RDD？
            RDD产生的意义在于降低开发分布式应用程序的门槛和提高执行效率。RDD全称resilient distributed dataset（弹性分布式数据集），
            它是一个可以容错的不可变集合，集合中的元素可以进行并行化地处理，Spark是围绕RDDs的概念展开的。RDD可以通过有两种创建的方式，
            一种是通过已经存在的驱动程序中的集合进行创建，另一种是通引用外部存储系统中的数据集进行创建，这里的外部系统可以是像HDFS或HBase
            这样的共享文件系统，也可以是任何支持hadoop InputFormat的数据。
            在源码中，RDD是一个具备泛型的可序列化的抽象类。具备泛型意味着RDD内部存储的数据类型不定，大多数类型的数据都可以存储在RDD之中。
            RDD是一个抽象类则意味着RDD不能直接使用，我们使用的时候通常使用的是它的子类，如HadoopRDD,BlockRDD,JdbcRDD,MapPartitionsRDD,
            CheckpointRDD等。
            
        spark能都取代hadoop?
            Spark是一个计算框架，它没有自己的存储，它的存储还得借助于HDFS，所以说Spark不能取代Hadoop,要取代也是取代MapReduce
        
        Spark的特点？
            Apache Spark 是一个快速的处理大规模数据的通用工具。它是一个基于内存计算框架。它有以下的四个特点：
            1）快速：基于内存的计算比MapReduce快100倍，基于磁盘快10倍。
            2）易用：编写一个spark的应用程序可以使用 Java, Scala, Python, R，这就使得我们的开发非常地灵活。并且，对比于MapReduce,
            spark内置了80多个高级操作，这使得开发十分高效和简单。
            3）运行范围广：spark可以运行在local、yarn、mesos、standalone、kubernetes等多种平台之上。它可以访问诸如HDFS,
             Cassandra, HBase, S3等多种多样的数据源。
            4）通用：spark提供了SparkSQL、SparkStreaming、GraphX、MLlib等一系列的分析工具。
        
        大数据流式处理框架对比：Storm vs Spark Streaming
            a.Spark Streaming最低可在0.5秒~2秒内做一次处理，而Storm最快可达到0.1秒，在实时性和容错性上，Spark Streaming不如Strom.
            b.Spark Streaming的集成性优于Storm,可以通过RDD无缝对接Spark上的所有组件，还可以很容易的与kafka,flume等分布式框架进行集成。
            c.在数据吞吐量上，Spark Streaming要远远优于Storm。
            
            综上所诉，Spark Streaming更适用于大数据流式处理。
        
        cache()和persist()方法的区别？
            cache()在源码底层调用的是persist().

        spark中的join是宽依赖还是债依赖？
            如果JoinAPI之前被调用的RDD API是宽依赖(存在shuffle), 而且两个join的RDD的分区数量一致，join结果的rdd分区数量也一样，这个时候join api是窄依赖
            　　除此之外的，rdd 的join api是宽依赖
            
        Spark中的map和reduce和mapreduce有什么关系？
            首先了解一下Mapreduce，它最本质的两个过程就是Map和Reduce，Map的应用在于我们需要数据一对一的元素的映射转换，
            比如说进行截取，进行过滤，或者任何的转换操作，这些一对一的元素转换就称作是Map；Reduce主要就是元素的聚合，
            就是多个元素对一个元素的聚合，比如求Sum等，这就是Reduce。
            
            其实spark里面也可以实现Mapreduce，但是这里它并不是算法，只是提供了map阶段和reduce阶段，但是在两个阶段提供了很多算法。
            如Map阶段的map, flatMap, filter, keyBy，Reduce阶段的reduceByKey, sortByKey, mean, gourpBy, sort等。
            
        repartition和coalesce区别？
            他们两个都是RDD的分区进行重新划分
            一、repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区）
            二、 rdd.coalesce方法的作用是创建CoalescedRDD，
            假设RDD有N个分区，需要重新划分成M个分区
            1）如果N>M并且N和M相差不多，(假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，
            这时可以将shuff设置为false，在shuffl为false的情况下，如果M>N时，coalesce为无效的，不进行shuffle过程，
            父RDD和子RDD之间是窄依赖关系。
            2）如果N>M并且两者相差悬殊，这时如果将shuffle设置为false，父子ＲＤＤ是窄依赖关系，他们同处在一个Stage中，
            就可能造成spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，
            可以讲shuffle设置为true， 会增加一个shuffle的步骤。
            3）、N<M。一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，
            这时需要将shuffle设置为true。


        DataFrame vs RDD vs DataSet
            a.基于RDD的编程，不同语言性能是不一样的，而DataFrame是一样的，因为底层会有一个优化器先将代码进行优化。
            b.对于RDD，暴露给执行引擎的信息只有数据的类型，如RDD[Student]装的是Student,而对于DataFrame,对于外部可见
                的信息有字段类型，字段key,字段value等。
            c.RDD是一个数组，DataFrame是一个列式表。

        Spark1.x和2.x的区别。
            在${SPARK_HOME}/jars目录下有许多jar包，而在spark1.0版本中只有一个大的jar包
            
        SparkContext?
            目前在一个JVM进程中可以创建多个SparkContext，但是只能有一个active级别的。如果你需要创建一个新的SparkContext实例，
            必须先调用stop方法停掉当前active级别的SparkContext实例。
            初始化一个SparkContext之前你需要构建一个SparkConf对象，初始化后，就可以使用SparkContext对象所包含的各种方法来创建和
            操作RDD和共享变量，Spark shell会自动初始化一个SparkContext。

        宽依赖和窄依赖？
            窄依赖是指父RDD的一个分区至多被子RDD的分区使用一次。(与数据规模无关)
            宽依赖是指父RDD的一个分区至少被子RDD的分区使用两次。(与数据规模有关)
            窄依赖的函数有：map, filter, union, join(父RDD是hash-partitioned ), mapPartitions, mapValues 
            宽依赖的函数有：xxxByKey, join(父RDD不是hash-partitioned ), partitionBy.
        
        
    行转列和列转行
    https://www.cnblogs.com/ken-jl/p/8570518.html
       

    parquet
    http://www.cnblogs.com/ITtangtang/p/7681019.html
    
    常用的Transformation：
        转换	含义
        map(func)	返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成
        filter(func)	返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成
        flatMap(func)	类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）
        mapPartitions(func)	类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] => Iterator[U]
        mapPartitionsWithIndex(func)	类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是 (Int, Interator[T]) => Iterator[U]
        sample(withReplacement, fraction, seed)	根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子
        union(otherDataset)	对源RDD和参数RDD求并集后返回一个新的RDD
        intersection(otherDataset)	对源RDD和参数RDD求交集后返回一个新的RDD
        distinct([numTasks]))	对源RDD进行去重后返回一个新的RDD
        groupByKey([numTasks])	在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD
        reduceByKey(func, [numTasks])	在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置
        aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])	
        sortByKey([ascending], [numTasks])	在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD
        sortBy(func,[ascending], [numTasks])	与sortByKey类似，但是更灵活
        join(otherDataset, [numTasks])	在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD
        cogroup(otherDataset, [numTasks])	在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD
        cartesian(otherDataset)	笛卡尔积
        pipe(command, [envVars])	
        coalesce(numPartitions)	
        repartition(numPartitions)	
        repartitionAndSortWithinPartitions(partitioner)	
        
        Action
        动作	含义
        reduce(func)	通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的
        collect()	在驱动程序中，以数组的形式返回数据集的所有元素
        count()	返回RDD的元素个数
        first()	返回RDD的第一个元素（类似于take(1)）
        take(n)	返回一个由数据集的前n个元素组成的数组
        takeSample(withReplacement,num, [seed])	返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子
        takeOrdered(n, [ordering])	
        saveAsTextFile(path)	将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本
        saveAsSequenceFile(path)	将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。
        saveAsObjectFile(path)	
        countByKey()	针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。
        foreach(func)	在数据集的每一个元素上，运行函数func进行更新。

##drools
    Drools是一款基于Java的开源规则引擎
    　　实现了将业务决策从应用程序中分离出来。
    　　优点：
    　　　　1、简化系统架构，优化应用
    　　　　2、提高系统的可维护性和维护成本
    　　　　3、方便系统的整合
    　　　　4、减少编写“硬代码”业务规则的成本和风险
    
    Drools的基础语法：
        Drl文件内容：
            package：包路径，该路径是逻辑路径(可以随便写，但是不能不写，最好和文件目录同名，以(.)的方式隔开)，规则文件中永远是第一行
        　　rule：规则体，以rule开头，以end结尾，每个文件可以包含多个rule	，规则体分为3个部分：LHS，RHS，属性 三大部分
        　　LHS：(Left Hand Side)，条件部分，在一个规则当中“when”和“then”中间的部分就是LHS部分，在LHS当中，可以包含0~N个条件，如果
        　　　　LHS为空的话，那么引擎会自动添加一个eval(true)的条件，由于该条件总是返回true，所以LHS为空的规则总是返回true。
        　　RHS：(Right Hand Side)，在一个规则中“then”后面的部分就是RHS，只有在LHS的所有条件都满足的情况下，RHS部分才会执行。

    Drools的API调用
        <?xml version="1.0" encoding="UTF-8"?>
        <kmodule xmlns="http://jboss.org/kie/6.0.0/kmodule">
            <kbase name="rules" packages="rules.testword">
                <ksession name="session"/>
            </kbase>
        </kmodule>
        
    规则语言
    　　rule “name”
    　　　　attributes ---->属性
    　　　　when
    　　　　　　LHS ---->条件
    　　　　then
    　　　　　　RHS	---->结果
    　　end
    　　一个规则包含三部分：唯有attributes部分可选，其他都是必填信息
    　　　　定义当前规则执行的一些属性等，比如是否可被重复执行，过期时间，生效时间等
    　　　　LHS：定义当前规则的条件，如 when Message();判断当前workingMemory中是否存在Message对象	
    　　　　RHS：可以写java代码，即当前规则条件满足执行的操作，可以直接调用Fact对象的方法来操作应用

    *Drools 原理
        DRL 解释执行流程
             Drools 规则是在 Java 应用程序上运行的，其要执行的步骤顺序由代码确定。为了实现这一点，Drools 规则引擎将业务规则转换成执行树
            如上图所示，每个规则条件分为小块，在树结构中连接和重用。每次将数据添加到规则引擎中时，它将在与此类似的树中进行求值，
            并到达一个动作节点，在该节点处，它们将被标记为准备执行特定规则的数据。
        
        规则引擎工作方式
            规则引擎默认不会在规则评估时立即执行业务规则，除非我们强制指定。当我们到达一个事实(Fact)与规则相匹配的节点时，
            规则评估会将规则操作与触发数据添加到一个叫作议程(Agenda)的组件中，如果同一个事实(Fact)与多个规则相匹配，
            就认为这些规则是冲突的，议程(Agenda)使用冲突解决策略(Conflict Resolution strategy)管理这些冲突规则的执行顺序。
            整个生命周期中，规则评估与规则执行之间有着明确的分割。规则操作的执行可能会导致事实(Fact)的更新，从而与其它规则相匹配，
            导致它们的触发，称之为前向链接。

        规则引擎虽然非常强大，但并非所有场景都适用。一般来说，规则引擎适用的项目都具有以下一个或多个特征：
            1.存在一个非常复杂的场景，即使对于商业专家也难以完全定义
            2.没有已知或定义明确的算法解决方案
            3.有不稳定需求，需要经常更新
            4.需要快速做出决策，通常是基于部分数据量




##秒杀实现

    实现难点
        1. 超买超卖问题的解决。
        2. 订单持久化，多线程将订单信息写入数据库
    
    进阶方案
        1.访问量还是大。系统还是撑不住。
        2.防止用户刷新页面导致重复提交。
        3.脚本攻击
    
    解决思路：
        1.访问量还是过大的话，要看性能瓶颈在哪里，一般来说首先撑不住的是tomcat，考虑优化tomcat，单个tomcat经过实践并发量撑住1000是没有问题的。
        先搭建tomcat集群，如果瓶颈出现在redis上的话考虑集群redis，这时候消息队列也是必须的，至于采用哪种消息队列框架还是根据实际情况。
        2.问题2和问题3其实属于同一个问题。这个问题其实属于网络问题的范畴，和我们的秒杀系统不在一个层面上。因此不应该由我们来解决。
        很多交换机都有防止一个源IP发起过多请求的功能。开源软件也有不少能实现这点。如linux上的TC可以控制。流行的Web服务器Nginx
        （它也可以看做是一个七层软交换机）也可以通过配置做到这一点。一个IP，一秒钟我就允许你访问我2次，其他软件包直接给你丢了，你还能压垮我吗？
        交换机也不行了呢？
        可能你们的客户并发访问量实在太大了，交换机都撑不住了。 这也有办法。我们可以用多个交换机为我们的秒杀系统服务。
        原理就是DNS可以对一个域名返回多个IP，并且对不同的源IP，同一个域名返回不同的IP。如网通用户访问，就返回一个网通机房的IP；
        电信用户访问，就返回一个电信机房的IP。也就是用CDN了！ 我们可以部署多台交换机为不同的用户服务。 用户通过这些交换机访问后面数据中心的Redis Cluster进行秒杀作业。
    
    
    https://blog.csdn.net/qq_27631217/article/details/80657271  (redis缓存细节)
    
    https://www.jianshu.com/p/d789ea15d060  (整体架构)
    
    *秒杀系统的难点
        首先我们先看下秒杀场景的难点到底在哪？在秒杀场景中最大的问题在于容易产生大并发请求、产生超卖现象和性能问题，下面我们分别分析下下面这三个问题：
        1）瞬时大并发：一提到秒杀系统给人最深刻的印象是超大的瞬时并发，这时你可以联想到小米手机的抢购场景，在小米手机抢购的场景一般都会有
            10w＋的用户同时访问一个商品页面去抢购手机，这就是一个典型的瞬时大并发，如果系统没有经过限流或者熔断处理，那么系统瞬间就会崩掉，就好像被DDos攻击一样；
        2）超卖：秒杀除了大并发这样的难点，还有一个所有电商都会遇到的痛，那就是超卖，电商搞大促最怕什么？最怕的就是超卖，产生超卖了以后会影响
            到用户体验，会导致订单系统、库存系统、供应链等等，产生的问题是一系列的连锁反应，所以电商都不希望超卖发生，但是在大并发的场景最容易
            发生的就是超卖，不同线程读取到的当前库存数据可能下个毫秒就被其他线程修改了，如果没有一定的锁库存机制那么库存数据必然出错，
            都不用上万并发，几十并发就可以导致商品超卖；
        3）性能：当遇到大并发和超卖问题后，必然会引出另一个问题，那就是性能问题，如何保证在大并发请求下，系统能够有好的性能，让用户能够有
            更好的体验，不然每个用户都等几十秒才能知道结果，那体验必然是很糟糕的；
    
    *秒杀系统方案
        从整个秒杀系统的架构其实和一般的互联网系统架构本身没有太多的不同，核心理念还是通过缓存、异步、限流来保证系统的高并发和高可用。
        下面从一笔秒杀交易的流程来描述下秒杀系统架构设计的要点：
        
        
##算法

    实际中，我们一般仅考量算法在最坏情况下的运行情况，也就是对于规模为 n 的任何输入，算法的最长运行时间。这样做的理由是：
        1.一个算法的最坏情况运行时间是在任何输入下运行时间的一个上界（Upper Bound）。
        2.对于某些算法，最坏情况出现的较为频繁。
        3.大体上看，平均情况通常与最坏情况一样差。  
        
    算法分析要保持大局观（Big Idea），其基本思路：
        1.忽略掉那些依赖于机器的常量。
        2.关注运行时间的增长趋势  
        
    使用 O 记号法（Big O Notation）表示最坏运行情况的上界
        线性复杂度 O(n) 表示每个元素都要被处理一次。
        平方复杂度 O(n2) 表示每个元素都要被处理 n 次。
        
        复杂度                |  	标记符号	   |             描述
        -|-|-
        常量（Constant）	     |     O(1)        |  操作的数量为常数，与输入的数据的规模无关。n = 1,000,000 -> 1-2 operations 
        对数（Logarithmic）	 |     O(log2 n)   |  操作的数量与输入数据的规模 n 的比例是 log2 (n)。n = 1,000,000 -> 30 operations
        线性（Linear）	     |     O(n)        |  操作的数量与输入数据的规模 n 成正比。n = 10,000 -> 5000 operations
        平方（Quadratic）	     |     O(n2)	   |  操作的数量与输入数据的规模 n 的比例为二次平方。n = 500 -> 250,000 operations
        立方（Cubic）	         |     O(n3)       |  操作的数量与输入数据的规模 n 的比例为三次方。n = 200 -> 8,000,000 operations
        指数（Exponential）	 |O(2n),O(kn),O(n!)|  指数级的操作，快速的增长。n = 20 -> 1048576 operations
         
       
    注1：快速的数学回忆，logab = y 其实就是 ay = b。所以，log24 = 2，因为 22 = 4。同样 log28 = 3，因为 23 = 8。我们说，log2n 
        的增长速度要慢于 n，因为当 n = 8 时，log2n = 3。
    注2：通常将以 10 为底的对数叫做常用对数。为了简便，N 的常用对数 log10 N 简写做 lg N，例如 log10 5 记做 lg 5。
    注3：通常将以无理数 e 为底的对数叫做自然对数。为了方便，N 的自然对数 loge N 简写做 ln N，例如 loge 3 记做 ln 3。
    注4：在算法导论中，采用记号 lg n = log2 n ，也就是以 2 为底的对数。改变一个对数的底只是把对数的值改变了一个常数倍，所以当
        不在意这些常数因子时，我们将经常采用 "lg n"记号，就像使用 O 记号一样。计算机工作者常常认为对数的底取 2 最自然，因为很多算法和
        数据结构都涉及到对问题进行二分。 

    计算代码块的渐进运行时间的方法有如下步骤：
        1.确定决定算法运行时间的组成步骤。
        2.找到执行该步骤的代码，标记为 1。
        3.查看标记为 1 的代码的下一行代码。如果下一行代码是一个循环，则将标记 1 修改为 1 倍于循环的次数 1 * n。如果包含多个嵌套的循环，
            则将继续计算倍数，例如 1 * n * m。
        4.找到标记到的最大的值，就是运行时间的最大值，即算法复杂度描述的上界。
    
    
    时间复杂度
    https://juejin.im/post/58d15f1044d90400691834d4     (说的比较明白)
        时间频度一个算法执行所耗费的时间，从理论上是不能算出来的，必须上机运行测试才能知道。但我们不可能也没有必要对每个算法都上机测试，
        只需知道哪个算法花费的时间多，哪个算法花费的时间少就可以了。并且一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中
        语句执行次数多，它花费时间就多。一个算法中的语句执行次数称为语句频度或时间频度。记为T(n)。
        时间复杂度前面提到的时间频度T(n)中，n称为问题的规模，当n不断变化时，时间频度T(n)也会不断变化。但有时我们想知道它变化时呈现什么
        规律，为此我们引入时间复杂度的概念。一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助
        函数f(n)，使得当n趋近于无穷大时，T（n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数，记作T(n)=O(f(n))，它称
        为算法的渐进时间复杂度，简称时间复杂度。
    
    大O表示法
        像前面用O( )来体现算法时间复杂度的记法，我们称之为大O表示法。算法复杂度可以从最理想情况、平均情况和最坏情况三个角度来评估，
        由于平均情况大多和最坏情况持平，而且评估最坏情况也可以避免后顾之忧，因此一般情况下，我们设计算法时都要直接估算最坏情况的
        复杂度。大O表示法O(f(n)中的f(n)的值可以为1、n、logn、n²等，因此我们可以将O(1)、O(n)、O(logn)、O(n²)分别可以称为常数阶、
        线性阶、对数阶和平方阶，那么如何推导出f(n)的值呢？我们接着来看推导大O阶的方法。
        推导大O阶推导大O阶，我们可以按照如下的规则来进行推导，得到的结果就是大O表示法：
        1.用常数1来取代运行时间中所有加法常数。
        2.修改后的运行次数函数中，只保留最高阶项
        3.如果最高阶项存在且不是1，则去除与这个项相乘的常数。
    
    *常用的时间复杂度按照耗费的时间从小到大依次是：
    O(1)<O(logn)<O(n)<O(nlogn)<O(n²)<O(n³)<O(2ⁿ)<O(n!)
    
    
    
    *八种排序算法
    https://www.jianshu.com/p/5e171281a387  
    
        1.选择排序
            选择排序可以说是最简单的一种排序方法：
            1.找到数组中最小的那个元素
            2.将最小的这个元素和数组中第一个元素交换位置
            3.在剩下的元素中找到最小的的元素，与数组第二个元素交换位置
            时间复杂度O(N^2)
    
        2.冒泡排序：
            据说是八大排序中的其一，通俗的意思就是讲，在一组数据中，相邻元素依次比较大小，最大的放后面，最小的冒上来
            时间复杂度O(N^2)
            
        3.直接插入排序
            1.首先设定插入次数，即循环次数，for(int i=1;i<length;i++)，1个数的那次不用插入。
            2.设定插入数和得到已经排好序列的最后一个数的位数。insertNum和j=i-1。
            3.从最后一个数开始向前循环，如果插入数小于当前数，就将当前数向后移动一位。
            4.将当前数放置到空着的位置，即j+1。
            时间复杂度O(N^2)
            
        4.希尔排序
            https://www.cnblogs.com/edwinchen/p/4782179.html    (原理)
            对于直接插入排序问题，数据量巨大时。
            1.将数的个数设为n，取奇数k=n/2，将下标差值为k的书分为一组，构成有序序列。
            2.再取k=k/2 ，将下标差值为k的书分为一组，构成有序序列。
            3.重复第二步，直到k=1执行简单插入排序。
            
            （1）希尔排序（shell sort）这个排序方法又称为缩小增量排序，是1959年D·L·Shell提出来的。该方法的基本思想是：设待排序元素序列有n个元素，首先取一个整数increment（小于n）作为间隔将全部元素分为increment个子序列，所有距离为increment的元素放在同一个子序列中，在每一个子序列中分别实行直接插入排序。然后缩小间隔increment，重复上述子序列划分和排序工作。直到最后取increment=1，将所有元素放在同一个子序列中排序为止。 
            （2）由于开始时，increment的取值较大，每个子序列中的元素较少，排序速度较快，到排序后期increment取值逐渐变小，子序列中元素个数逐渐增多，但由于前面工作的基础，大多数元素已经基本有序，所以排序速度仍然很快。 
            时间复杂度为O(nlogn)。
            
        5.归并排序
        https://blog.csdn.net/weixin_38108266/article/details/80949412
            速度仅次于快排，内存少的时候使用，可以进行并行计算的时候使用。
            
            1.选择相邻两个数组成一个有序序列。
            2.选择相邻的两个有序序列组成一个有序序列。
            3.重复第二步，直到全部组成一个有序序列。
            时间复杂度为O(nlogn)。
            
            递归：
            	private void MergeSort(int[] a, int p, int r) {
            		if(p<r){
            			int q = (p+r)/2;
            //			p表示从子序列的哪个索引开始，q表示子序列中间的位置
            			MergeSort(a,p,q);
            			MergeSort(a,q+1,r);
            			Merge(a,p,q,r);
            		}
            	
        6.堆排序
        https://www.cnblogs.com/MOBIN/p/5374217.html    (讲得比较清楚)
        https://www.cnblogs.com/MOBIN/p/5374217.html
            对简单选择排序的优化。
            
            1.将序列构建成大顶堆。
            2.将根节点与最后一个节点交换，然后断开最后一个节点。
            3.重复第一、二步，直到所有节点断开。
            
            作为选择排序的改进版，堆排序可以把每一趟元素的比较结果保存下来，以便我们在选择最小/大元素时对已经比较过的元素做出相应的调整。
            堆排序是一种树形选择排序，在排序过程中可以把元素看成是一颗完全二叉树，每个节点都大（小）于它的两个子节点，当每个节点都大于
            等于它的两个子节点时，就称为大顶堆，也叫堆有序； 当每个节点都小于等于它的两个子节点时，就称为小顶堆。

            算法思想(以大顶堆为例)：
            1.将长度为n的待排序的数组进行堆有序化构造成一个大顶堆
            2.将根节点与尾节点交换并输出此时的尾节点
            3.将剩余的n -1个节点重新进行堆有序化
            4.重复步骤2，步骤3直至构造成一个有序序列
            假设待排序数组为[20,50,10,30,70,20,80]
             
    
        7.快速排序
            要求时间最快时。
            选择第一个数为p，小于p的数放在左边，大于p的数放在右边。
            递归的将p左边和右边的数都按照第一步进行，直到不能递归。
            
            基本思想
            随机找出一个数，可以随机取，也可以取固定位置，一般是取第一个或最后一个称为基准，然后就是比基准小的在左边，比基准大的放到右边，
            如何放做，就是和基准进行交换，这样交换完左边都是比基准小的，右边都是比较基准大的，这样就将一个数组分成了两个子数组，
            然后再按照同样的方法把子数组再分成更小的子数组，直到不能分解为止。
            
        8.基数排序
          用于大量数，很长的数进行排序时。
          将所有的数的个位数取出，按照个位数进行排序，构成一个序列。
          将新构成的所有的数的十位数取出，按照十位数进行排序，构成一个序列。
          
    
    
    *查找
    https://blog.csdn.net/weixin_39241397/article/details/79344179  (推荐)
        二分查找
            基本思想：也称为是折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，
            若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找
            到或查找结束发现表中没有这样的结点。
            望时间复杂度为O(logn)；
            
        二叉查找树（BinarySearch Tree，也叫二叉搜索树，或称二叉排序树Binary Sort Tree）或者是一棵空树，或者是具有下列性质的二叉树：
        
        　　1）若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；
        　　2）若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；
        　　3）任意节点的左、右子树也分别为二叉查找树。
        　　二叉查找树性质：对二叉查找树进行中序遍历，即可得到有序的数列。
        
            复杂度分析：它和二分查找一样，插入和查找的时间复杂度均为O(logn)，但是在最坏的情况下仍然会有O(n)的时间复杂度。
            原因在于插入和删除元素的时候，树没有保持平衡（比如，我们查找上图（b）中的“93”，我们需要进行n次查找操作）。我们追求的是在最坏
            的情况下仍然有较好的时间复杂度，这就是平衡查找树设计的初衷。

        平衡查找树之2-3查找树（2-3 Tree）
            基于二叉查找树进行优化，进而可以得到其他的树表查找算法，如平衡树、红黑树等高效算法
        　　2-3查找树定义：和二叉树不一样，2-3树运行每个节点保存1个或者两个的值。对于普通的2节点(2-node)，他保存1个key和左右两个
            自己点。对应3节点(3-node)，保存两个Key，2-3查找树的定义如下：
        　　1）要么为空，要么：
        　　2）对于2节点，该节点保存一个key及对应value，以及两个指向左右节点的节点，左节点也是一个2-3节点，所有的值都比key要小，
                右节点也是一个2-3节点，所有的值比key要大。
        　　3）对于3节点，该节点保存两个key及对应value，以及三个指向左中右的节点。左节点也是一个2-3节点，所有的值均比两个key中
                的最小的key还要小；中间节点也是一个2-3节点，中间节点的key值在两个跟节点key值之间；右节点也是一个2-3节点，节点
                的所有key值比两个key中的最大的key还要大。
        
            2-3查找树的性质：
            　　1）如果中序遍历2-3查找树，就可以得到排好序的序列；
            　　2）在一个完全平衡的2-3查找树中，根节点到每一个为空节点的距离都相同。（这也是平衡树中“平衡”一词的概念，根节点到叶节点的
                最长距离对应于查找算法的最坏情况，而平衡树中根节点到叶节点的距离都一样，最坏情况也具有对数复杂度。）
                
        平衡查找树之红黑树（Red-Black Tree）
        　　2-3查找树能保证在插入元素之后能保持树的平衡状态，最坏情况下即所有的子节点都是2-node，树的高度为lgn，从而保证了最坏情况下
            的时间复杂度。但是2-3树实现起来比较复杂，于是就有了一种简单实现2-3树的数据结构，即红黑树（Red-Black Tree）。
        
        　　基本思想：红黑树的思想就是对2-3查找树进行编码，尤其是对2-3查找树中的3-nodes节点添加额外的信息。红黑树中将节点之间的链接
            分为两种不同类型，红色链接，他用来链接两个2-nodes节点来表示一个3-nodes节点。黑色链接用来链接普通的2-3节点。特别的，使用
            红色链接的两个2-nodes来表示一个3-nodes节点，并且向左倾斜，即一个2-node是另一个2-node的左子节点。这种做法的好处是查找的
            时候不用做任何修改，和普通的二叉查找树相同。  
               
            红黑树的定义：
            　　红黑树是一种具有红色和黑色链接的平衡查找树，同时满足：
                红色节点向左倾斜
                一个节点不可能有两个红色链接
                整个树完全黑色平衡，即从根节点到所以叶子结点的路径上，黑色链接的个数都相同。 
                
            红黑树的性质：整个树完全黑色平衡，即从根节点到所以叶子结点的路径上，黑色链接的个数都相同（2-3树的第2）性质，从根节点到叶子节点的距离都相等）。咋·    
            　　复杂度分析：最坏的情况就是，红黑树中除了最左侧路径全部是由3-node节点组成，即红黑相间的路径长度是全黑路径长度的2倍。
            
            红黑树这种数据结构应用十分广泛，在多种编程语言中被用作符号表的实现，如：
            Java中的java.util.TreeMap,java.util.TreeSet；
                
    二叉树遍历
        //前序遍历递归的方式
               public void preOrder(BinaryTreeNode root){
                   if(null!=root){
                       System.out.print(root.getData()+"\t");
                       preOrder(root.getLeft());
                       preOrder(root.getRight());
                  }
              }
              
              //前序遍历非递归的方式
              public void preOrderNonRecursive(BinaryTreeNode root){
                  Stack<BinaryTreeNode> stack=new Stack<BinaryTreeNode>();
                  while(true){
                      while(root!=null){
                          System.out.print(root.getData()+"\t");
                          stack.push(root);
                          root=root.getLeft();
                      }
                      if(stack.isEmpty()) break;
                      root=stack.pop();
                      root=root.getRight();
                  }
              }
    
    
    B+树
        B+树的定义十分复杂，因此只简要地介绍B+树：B+树是为磁盘或其他直接存取辅助设备而设计的一种平衡查找树，在B+树中，所有记录节点
        都是按键值的大小顺序存放在同一层的叶节点中，各叶节点指针进行连接。      
          
         
          
##设计模式
    单例模式(*)
        为啥要加volatile
            private volatile static Singleton2 singleton2;
            第一个if (instance == null)，只有instance为null的时候，才进入synchronized. 第二个if (instance == null)，
            是为了防止可能出现多个实例的情况。
            volatile: 主要在于singleton = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。
             　　1. 给 singleton 分配内存 　　
                 2. 调用 Singleton 的构造函数来初始化成员变量，形成实例 　　
                 3. 将singleton对象指向分配的内存空间（执行完这步 singleton才是非 null 了）但是在 JVM 的即时编译器中存在指令重排序的优化。 　　
             也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、
             2 未执行之前，被线程二抢占了 ，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，
             然后使用，然后顺理成章地报错。
          https://www.cnblogs.com/keeya/p/9260565.html  

    总体来说设计模式分为三大类：
        创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。
        结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。
        行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、
            中介者模式、解释器模式。
        其实还有两类：并发型模式和线程池模式。用一个图片来整体描述一下：
    
    设计模式的六大原则
        总原则：开闭原则（Open Close Principle）
        开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。
        所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的具体设计中我们会提到这点。
        
        1、单一职责原则
        不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。
        
        2、里氏替换原则（Liskov Substitution Principle）
        里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，
        子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也
        能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系
        就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。—— From Baidu 百科
        历史替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。
        
        3、依赖倒转原则（Dependence Inversion Principle）
        这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上
        层接口交互。
        
        4、接口隔离原则（Interface Segregation Principle）
        这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口
        （多个接口方法集合到一个的接口）要好。
        
        5、迪米特法则（最少知道原则）（Demeter Principle）
        就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供
        给外部。这样当被依赖的类变化时，才能最小的影响该类。
        最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。
        我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。
        
        6、合成复用原则（Composite Reuse Principle）
        原则是尽量首先使用合成/聚合的方式，而不是使用继承。
    
    设计模式
    http://www.cnblogs.com/geek6/p/3951677.html     (推荐,比较好理解)
        创建型
            静态工厂
            抽象工厂:一般一个抽象工厂里面包括多种维度的工厂,然后抽象工厂进行组装.
            单例模式
            建造者模式
            原型模式的适用环境：
            　　1：创建新对象成本较大（例如初始化时间长，占用CPU多或占太多网络资源），新对象可以通过复制已有对象来获得，如果相似对象，则可以对其成员变量稍作修改。
            　　2：系统要保存对象的状态，而对象的状态很小。
            　　3：需要避免使用分层次的工厂类来创建分层次的对象，并且类的实例对象只有一个或很少的组合状态，通过复制原型对象得到新实例可以比使用构造函数创建一个新实例更加方便。
        
        结构型
            代理
                静态代理    
                    缺点：我们得为每一个服务都得创建代理类，工作量太大，不易管理。同时接口一旦发生改变，代理类也得相应修改。
                动态代理
                    .CGLIB代理 JDK实现动态代理需要实现类通过接口定义业务方法，对于没有接口的类，如何实现动态代理呢，这就需要CGLib了。
                    CGLib采用了非常底层的字节码技术，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类
                    方法的调用，顺势织入横切逻辑。但因为采用的是继承，所以不能对final修饰的类进行代理。JDK动态代理与CGLib动态代理均
                    是实现Spring AOP的基础。
                CGLIB代理总结： CGLIB创建的动态代理对象比JDK创建的动态代理对象的性能更高，但是CGLIB创建代理对象时所花费的时间却比JDK
                多得多。所以对于单例的对象，因为无需频繁创建对象，用CGLIB合适，反之使用JDK方式要更为合适一些。同时由于CGLib由于是采用
                动态创建子类的方法，对于final修饰的方法无法进行代理。
          
            适配器
                适配器模式的优缺点
                　　优点：
                　　　　1：将目标类和适配者类解耦，通过引入一个适配器类来重用现有的适配者类，无需修改原有结构。
                　　　　2：增加了类的透明性和复用性，将具体的业务实现过程封装在适配者类中，对于客户端类而言是透明的，而且提高了适配者的复用性， 同一适配者类可以在多个不同的系统中复用。
                　　　　3：灵活性和扩展性都非常好，通过使用配置文件，可以很方便的更换适配器，也可以在不修改原有代码的基础上 增加新的适配器，完全复合开闭原则。
                　　缺点：
                　　　　1：一次最多只能适配一个适配者类，不能同时适配多个适配者。
                　　　　2：适配者类不能为最终类，在C#中不能为sealed类
                　　　　3：目标抽象类只能为接口，不能为类，其使用有一定的局限性。
          
            桥接模式
                桥接模式和抽象工厂有点类似
                
            装饰器模式
                和代理模式类似
            
            组合模式（Composite）
                组合模式有时又叫部分-整体模式在处理类似树形结构的问题时比较方便，看看关系图：
                    private String name;  
                    private TreeNode parent;  
                    private Vector<TreeNode> children = new Vector<TreeNode>();  
                使用场景：将多个对象组合在一起进行操作，常用于表示树形结构中，例如二叉树，数等。
                
            享元模式（Flyweight）
                FlyWeightFactory负责创建和管理享元单元，当一个客户端请求时，工厂需要检查当前对象池中是否有符合条件的对象，如果有，
                就返回已经存在的对象，如果没有，则创建一个新对象，FlyWeight是超类。一提到共享池，我们很容易联想到Java里面的JDBC连接池，
                想想每个连接的特点，我们不难总结出：适用于作共享的一些个对象，他们有一些共有的属性，就拿数据库连接池来说，url、
                driverClassName、username、password及dbname，这些属性对于每个连接来说都是一样的，所以就适合用享元模式来处理，
                建一个工厂类，将上述类似属性作为内部数据，其它的作为外部数据，在方法调用时，当做参数传进来，这样就节省了空间，减少了实例的数量。
                
        行为型
            *父类与子类关系
                策略模式（strategy）
                    这个模式涉及到三个角色：
                    　　●　　环境(Context)角色：持有一个Strategy的引用。
                    　　●　　抽象策略(Strategy)角色：这是一个抽象角色，通常由一个接口或抽象类实现。此角色给出所有的具体策略类所需的接口。
                    　　●　　具体策略(ConcreteStrategy)角色：包装了相关的算法或行为。

                模板方法模式（Template Method）
                    解决的问题 提高代码复用性 将相同部分的代码放在抽象的父类中，而将不同的代码放入不同的子类中 实现了反向控制 通过一个父类
                    调用其子类的操作，通过对子类的具体实现扩展不同的行为，实现了反向控制 & 符合“开闭原则”
            
            *类之间的关系
                观察者模式（Observer）
                    
                迭代子模式（Iterator）
                    
                责任链模式（Chain of Responsibility）
                    有多个对象，每个对象持有对下一个对象的引用，这样就会形成一条链，请求在这条链上传递，直到某一对象决定处理该请求。
                    但是发出者并不清楚到底最终那个对象会处理该请求，所以，责任链模式可以实现，在隐瞒客户端的情况下，对系统进行动态的调整。 
                
                    此处强调一点就是，链接上的请求可以是一条链，可以是一个树，还可以是一个环，模式本身不约束这个，需要我们自己去实现，
                    同时，在一个时刻，命令只允许由一个对象传给另一个对象，而不允许传给多个对象。
                    
                命令模式（Command）
                    命令模式很好理解，举个例子，司令员下令让士兵去干件事情，从整个事情的角度来考虑，司令员的作用是，发出口令，
                    口令经过传递，传到了士兵耳朵里，士兵去执行。这个过程好在，三者相互解耦，任何一方都不用去依赖其他人，只需要做好
                    自己的事儿就行，司令员要的是结果，不会去关注到底士兵是怎么实现的
                
                    Invoker是调用者（司令员），Receiver是被调用者（士兵），MyCommand是命令，实现了Command接口，持有接收对象
                    
            *类的状态
                备忘录模式（Memento）
                    主要目的是保存一个对象的某个状态，以便在适当的时候恢复对象，个人觉得叫备份模式更形象些，通俗的讲下：假设有原始类A，
                    A中有各种属性，A可以决定需要备份的属性，备忘录类B是用来存储A的一些内部状态，类C呢，就是一个用来存储备忘录的，
                    且只能存储，不能修改等操作
                    
                    简单描述下：新建原始类时，value被初始化为egg，后经过修改，将value的值置为niu，最后倒数第二行进行恢复状态，
                    结果成功恢复了。其实我觉得这个模式叫“备份-恢复”模式最形象
                
                状态模式（State)
                    根据这个特性，状态模式在日常开发中用的挺多的，尤其是做网站的时候，我们有时希望根据对象的某一属性，
                    区别开他们的一些功能，比如说简单的权限控制等。
                    public class Test {  
                      
                        public static void main(String[] args) {  
                              
                            State state = new State();  
                            Context context = new Context(state);  
                              
                            //设置第一种状态  
                            state.setValue("state1");  
                            context.method();  
                              
                            //设置第二种状态  
                            state.setValue("state2");  
                            context.method();  
                        }  
                    }  
                    
                    public class Context {  
                      
                        private State state;  
                      
                        public Context(State state) {  
                            this.state = state;  
                        }  
                      
                        public State getState() {  
                            return state;  
                        }  
                      
                        public void setState(State state) {  
                            this.state = state;  
                        }  
                      
                        public void method() {  
                            if (state.getValue().equals("state1")) {  
                                state.method1();  
                            } else if (state.getValue().equals("state2")) {  
                                state.method2();  
                            }  
                        }  
                    }  
                    
                    
            *通过中间类
                访问者模式（Visitor）
                    Subject类，accept方法，接受将要访问它的对象，getSubject()获取将要被访问的属性，
                
                中介者模式（Mediator）
                    中介者模式也是用来降低类类之间的耦合的，因为如果类类之间有依赖关系的话，不利于功能的拓展和维护，
                    因为只要修改一个对象，其它关联的对象都得进行修改。如果使用中介者模式，只需关心和Mediator类的关系，
                    具体类类之间的关系及调度交给Mediator就行，这有点像spring容器的作用
                
                解释器模式（Interpreter）
                    public static void main(String[] args) {  
                            // 计算9+2-8的值  
                            int result = new Minus().interpret((new Context(new Plus()  
                                    .interpret(new Context(9, 2)), 8)));  
                            System.out.println(result);  
                    } 
                
                    基本就这样，解释器模式用来做各种各样的解释器，如正则表达式等的解释器等等！
                


##限流、降级、熔断
    https://www.cnblogs.com/tietazhan/p/6249479.html
    
    限流
        一般开发高并发系统常见的限流有：限制总并发数（比如数据库连接池、线程池）、限制瞬时并发数（如nginx的limit_conn模块，
        用来限制瞬时并发连接数）、限制时间窗口内的平均速率（如Guava的RateLimiter、nginx的limit_req模块，限制每秒的平均速率）；
        其他还有如限制远程接口调用速率、限制MQ的消费速率。另外还可以根据网络连接数、网络流量、CPU或内存负载等来限流。
        那么接下来我们从限流算法、应用级限流、分布式限流、接入层限流来详细学习下限流技术手段。
        
        *限流算法
            常见的限流算法有：令牌桶、漏桶。计数器也可以进行粗暴限流实现。
            令牌桶算法
                令牌桶算法是一个存放固定容量令牌的桶，按照固定速率往桶里添加令牌。令牌桶算法的描述如下：
                
                    假设限制2r/s，则按照500毫秒的固定速率往桶中添加令牌；
                    桶中最多存放b个令牌，当桶满时，新添加的令牌被丢弃或拒绝；
                    当一个n个字节大小的数据包到达，将从桶中删除n个令牌，接着数据包被发送到网络上；
                    如果桶中的令牌不足n个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。 
                
            漏桶算法
                漏桶作为计量工具（The Leaky Bucket Algorithm as a Meter）时，可以用于流量整形（Traffic Shaping）和流量控制（TrafficPolicing），漏桶算法的描述如下：
                    一个固定容量的漏桶，按照常量固定速率流出水滴；
                    如果桶是空的，则不需流出水滴；
                    可以以任意速率流入水滴到漏桶；
                    如果流入水滴超出了桶的容量，则流入的水滴溢出了（被丢弃），而漏桶容量是不变的。
                
            令牌桶和漏桶对比：
            
                令牌桶是按照固定速率往桶中添加令牌，请求是否被处理需要看桶中令牌是否足够，当令牌数减为零时则拒绝新的请求；
                漏桶则是按照常量固定速率流出请求，流入请求速率任意，当流入的请求数累积到漏桶容量时，则新流入的请求被拒绝；
                令牌桶限制的是平均流入速率（允许突发请求，只要有令牌就可以处理，支持一次拿3个令牌，4个令牌），并允许一定程度突发流量；
                漏桶限制的是常量流出速率（即流出速率是一个固定常量值，比如都是1的速率流出，而不能一次是1，下次又是2），从而平滑突发流入速率；
                令牌桶允许一定程度的突发，而漏桶主要目的是平滑流入速率；
                两个算法实现可以一样，但是方向是相反的，对于相同的参数得到的限流效果是一样的。
                
                另外有时候我们还使用计数器来进行限流，主要用来限制总并发数，比如数据库连接池、线程池、秒杀的并发数；只要全局总请求数
                或者一定时间段的总请求数设定的阀值则进行限流，是简单粗暴的总数量限流，而不是平均速率限流。
                
                
            应用级限流
                限流总并发/连接/请求数
                
                    对于一个应用系统来说一定会有极限并发/请求数，即总有一个TPS/QPS阀值，如果超了阀值则系统就会不响应用户请求或响应
                    的非常慢，因此我们最好进行过载保护，防止大量请求涌入击垮系统。
                    如果你使用过Tomcat，其Connector其中一种配置有如下几个参数：
                    acceptCount：如果Tomcat的线程都忙于响应，新来的连接会进入队列排队，如果超出排队大小，则拒绝连接；
                    maxConnections：瞬时最大连接数，超出的会排队等待；
                    maxThreads：Tomcat能启动用来处理请求的最大线程数，如果请求处理量一直远远大于最大线程数则可能会僵死。
                    详细的配置请参考官方文档。另外如MySQL（如max_connections）、Redis（如tcp-backlog）都会有类似的限制连接数的配置。
                
                限流总资源数
                    如果有的资源是稀缺资源（如数据库连接、线程），而且可能有多个系统都会去使用它，那么需要限制应用；可以使用池化技术
                    来限制总资源数：连接池、线程池。比如分配给每个应用的数据库连接是100，那么本应用最多可以使用100个资源，超出了可
                    以等待或者抛异常。
                
                限流某个接口的总并发/请求数
                    如果接口可能会有突发访问情况，但又担心访问量太大造成崩溃，如抢购业务；这个时候就需要限制这个接口的总并发/请求数
                    总请求数了；因为粒度比较细，可以为每个接口都设置相应的阀值。可以使用Java中的AtomicLong进行限流：
                    
                    =================================
                    
                    try {
                        if(atomic.incrementAndGet() > 限流数) {
                            //拒绝请求
                        }
                        //处理请求
                    } finally {
                        atomic.decrementAndGet();
                    }
                    =================================
                    
                    适合对业务无损的服务或者需要过载保护的服务进行限流，如抢购业务，超出了大小要么让用户排队，要么告诉用户没货了，
                    对用户来说是可以接受的。而一些开放平台也会限制用户调用某个接口的试用请求量，也可以用这种计数器方式实现。这种
                    方式也是简单粗暴的限流，没有平滑处理，需要根据实际情况选择使用；
                    
                限流某个接口的时间窗请求数
                    即一个时间窗口内的请求数，如想限制某个接口/服务每秒/每分钟/每天的请求数/调用量。如一些基础服务会被很多其他系统调用，
                    比如商品详情页服务会调用基础商品服务调用，但是怕因为更新量比较大将基础服务打挂，这时我们要对每秒/每分钟的调用量进行
                    限速；一种实现方式如下所示：
                    =================================
                    LoadingCache<Long, AtomicLong> counter =
                            CacheBuilder.newBuilder()
                                    .expireAfterWrite(2, TimeUnit.SECONDS)
                                    .build(new CacheLoader<Long, AtomicLong>() {
                                        @Override
                                        public AtomicLong load(Long seconds) throws Exception {
                                            return new AtomicLong(0);
                                        }
                                    });
                    long limit = 1000;
                    while(true) {
                        //得到当前秒
                        long currentSeconds = System.currentTimeMillis() / 1000;
                        if(counter.get(currentSeconds).incrementAndGet() > limit) {
                            System.out.println("限流了:" + currentSeconds);
                            continue;
                        }
                        //业务处理
                    }
                    =================================
                    我们使用Guava的Cache来存储计数器，过期时间设置为2秒（保证1秒内的计数器是有的），然后我们获取当前时间戳然后取秒数来
                    作为KEY进行计数统计和限流，这种方式也是简单粗暴，刚才说的场景够用了。
                
                平滑限流某个接口的请求数
                    之前的限流方式都不能很好地应对突发请求，即瞬间请求可能都被允许从而导致一些问题；因此在一些场景中需要对突发请求进行
                    整形，整形为平均速率请求处理（比如5r/s，则每隔200毫秒处理一个请求，平滑了速率）。这个时候有两种算法满足我们的场景：
                    令牌桶和漏桶算法。Guava框架提供了令牌桶算法实现，可直接拿来使用。
                    Guava RateLimiter提供了令牌桶算法实现：平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)实现。
                     
                    
            分布式限流
                分布式限流最关键的是要将限流服务做成原子化，而解决方案可以使使用Redis+lua或者nginx+lua技术进行实现，通过这两种
                技术可以实现的高并发和高性能。
                首先我们来使用redis+lua实现时间窗内某个接口的请求数限流，实现了该功能后可以改造为限流总并发/请求数和限制总资源数。
                Lua本身就是一种编程语言，也可以使用它实现复杂的令牌桶或漏桶算法。
                

##安全
    todo
    1.单项三列算法
        MD5
        SHA
    2.对称加密
        DES
    3.非对称加密
        RSA
        HTTPS
    
    
##搜索引擎
    elasticSearch
       
                
                
                
                
                
                
