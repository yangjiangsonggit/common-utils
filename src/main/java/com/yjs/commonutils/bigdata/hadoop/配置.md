
守护进程	环境变量
资源管理器	YARN_RESOURCEMANAGER_HEAPSIZE
节点管理器	YARN_NODEMANAGER_HEAPSIZE
WebAppProxy	YARN_PROXYSERVER_HEAPSIZE
Map Reduce作业历史记录服务器	HADOOP_JOB_HISTORYSERVER_HEAPSIZE
配置Hadoop守护程序
本节介绍在给定的配置文件中指定的重要参数：

等/hadoop/core-site.xml
参数	值	笔记
fs.defaultFS	NameNode URI	hdfs：//主机：端口/
io.file.buffer.size	131072	SequenceFiles中使用的读/写缓冲区的大小。
等/hadoop/hdfs-site.xml

NameNode的配置：

参数	值	笔记
dfs.namenode.name.dir	NameNode永久存储名称空间和事务日志的本地文件系统上的路径。	如果这是用逗号分隔的目录列表，则将名称表复制到所有目录中，以实现冗余。
dfs.hosts / dfs.hosts.exclude	允许/排除的数据节点列表。	如有必要，请使用这些文件来控制允许的数据节点列表。
dfs.blocksize	268435456	大型文件系统的HDFS块大小为256MB。
dfs.namenode.handler.count	100	更多的NameNode服务器线程可处理来自大量DataNode的RPC。
DataNode的配置：
参数	值	笔记
dfs.datanode.data.dir	逗号分隔的DataNode本地文件系统上应存储其块的路径列表。	如果这是逗号分隔的目录列表，则数据将存储在所有命名的目录中，通常在不同的设备上。
etc / hadoop / yarn-site.xml

ResourceManager和NodeManager的配置：

参数	值	笔记
纱线使能	对 / 错	启用ACL？默认为false。
yarn.admin.acl	管理员ACL	用于在群集上设置管理员的ACL。ACL适用于逗号分隔的用户空间逗号分隔的组。默认为特殊值*，表示任何人。仅有空间的特殊价值意味着没有人可以使用。
yarn.log-aggregation-enable	假	启用或禁用日志聚合的配置
ResourceManager的配置：
参数	值	笔记
yarn.resourcemanager.address	ResourceManager host：port供客户端提交作业。	host：port  如果已设置，则将覆盖yarn.resourcemanager.hostname中设置的主机名。
yarn.resourcemanager.scheduler.address	ResourceManager host：ApplicationMaster的端口，可与Scheduler进行对话以获取资源。	host：port  如果已设置，则将覆盖yarn.resourcemanager.hostname中设置的主机名。
yarn.resourcemanager.resource-tracker.address	NodeManager的ResourceManager host：port。	host：port  如果已设置，则将覆盖yarn.resourcemanager.hostname中设置的主机名。
yarn.resourcemanager.admin.address	ResourceManager host：port用于管理命令。	host：port  如果已设置，则将覆盖yarn.resourcemanager.hostname中设置的主机名。
yarn.resourcemanager.webapp.address	ResourceManager Web用户界面主机：端口。	host：port  如果已设置，则将覆盖yarn.resourcemanager.hostname中设置的主机名。
yarn.resourcemanager。主机名	ResourceManager主机。	主机  可以设置单个主机名，以代替设置所有yarn.resourcemanager * address资源。为ResourceManager组件提供默认端口。
yarn.resourcemanager.scheduler.class	ResourceManager Scheduler类。	CapacityScheduler（推荐），FairScheduler（也推荐）或FifoScheduler。使用完全限定的类名，例如org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler。
纱线调度器最小分配MB	在资源管理器中分配给每个容器请求的最小内存限制。	以MB为单位
yarn.scheduler.maximum-allocation-mb	资源管理器中分配给每个容器请求的最大内存限制。	以MB为单位
yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path	允许/排除的NodeManager列表。	如有必要，请使用这些文件来控制允许的NodeManager列表。
NodeManager的配置：
参数	值	笔记
yarn.nodemanager.resource.memory-mb	资源，即给定NodeManager的可用物理内存，以MB为单位	定义NodeManager上的总可用资源以供运行中的容器使用
yarn.nodemanager.vmem-pmem-ratio	任务的虚拟内存使用率可能超过物理内存的最大比率	每个任务的虚拟内存使用量可能超出此物理内存限制的比率。NodeManager上的任务使用的虚拟内存总量可能超过此物理内存使用量的比例。
yarn.nodemanager.local-dirs	用逗号分隔的本地文件系统上写入中间数据的路径列表。	多个路径有助于传播磁盘I / O。
yarn.nodemanager.log目录	逗号分隔的本地文件系统上写入日志的路径列表。	多个路径有助于传播磁盘I / O。
yarn.nodemanager.log.retain-seconds	10800	在NodeManager上保留日志文件的默认时间（以秒为单位）仅在禁用日志聚合时才适用。
yarn.nodemanager.remote-app-log-dir	/日志	应用程序完成时将应用程序日志移动到的HDFS目录。需要设置适当的权限。仅在启用日志聚合的情况下适用。
yarn.nodemanager.remote-app-log-dir-后缀	日志	后缀附加到远程日志目录。日志将汇总到$ {yarn.nodemanager.remote-app-log-dir} / $ {user} / $ {thisParam}，仅在启用日志汇总的情况下适用。
yarn.nodemanager.aux-services	mapreduce_shuffle	需要为Map Reduce应用程序设置洗牌服务。
历史记录服务器的配置（需要移至其他位置）：
参数	值	笔记
yarn.log-aggregation.retain-seconds	-1	删除聚合日志前要保留多长时间。-1禁用。请注意，将此值设置得太小，您将向名称节点发送垃圾邮件。
yarn.log-aggregation.retain-check-interval-seconds	-1	检查聚合日志保留之间的时间。如果设置为0或负值，那么该值将被计算为聚合日志保留时间的十分之一。请注意，将此值设置得太小，您将向名称节点发送垃圾邮件。
等/hadoop/mapred-site.xml

MapReduce应用程序的配置：

参数	值	笔记
mapreduce.framework.name	纱	执行框架设置为Hadoop YARN。
mapreduce.map.memory.mb	1536	较大的地图资源限制。
mapreduce.map.java.opts	-Xmx1024M	地图的子jvm的较大堆大小。
mapreduce.reduce.memory.mb	3072	较大的资源限制以减少。
mapreduce.reduce.java.opts	-Xmx2560M	减少子jvm的堆大小。
mapreduce.task.io.sort.mb	512	更高的内存限制，同时对数据进行排序以提高效率。
mapreduce.task.io.sort.factor	100	排序文件时，更多流同时合并。
mapreduce.reduce.shuffle.parallelcopies	50	减少了运行的并行副本数量，从而从大量映射中获取输出。
MapReduce JobHistory服务器的配置：
参数	值	笔记
mapreduce.jobhistory.address	MapReduce JobHistory Server 主机：端口	默认端口为10020。
mapreduce.jobhistory.webapp.address	MapReduce JobHistory Server Web UI 主机：端口	默认端口是19888。
mapreduce.jobhistory.intermediate-done-dir	/先生历史/ tmp	MapReduce作业在其中写入历史文件的目录。
mapreduce.jobhistory.done-dir	/ mr-history /完成	历史记录文件由MR JobHistory服务器管理的目录。
监视NodeManager的运行状况
Hadoop提供了一种机制，管理员可以通过该机制将NodeManager配置为定期运行管理员提供的脚本，以确定节点是否正常。

管理员可以通过在脚本中执行对其选择的任何检查来确定节点是否处于正常状态。如果脚本检测到该节点处于不正常状态，则它必须在标准输出中打印一行，并以字符串ERROR开头。NodeManager会定期生成脚本并检查其输出。如上所述，如果脚本的输出包含字符串ERROR，则该节点的状态报告为不正常并且该节点被ResourceManager列入黑名单。没有其他任务将分配给该节点。但是，NodeManager会继续运行脚本，因此，如果该节点再次恢复正常，则它将自动从ResourceManager的黑名单节点中删除。节点的运行状况以及脚本的输出（如果运行状况不正常）对于管理员在ResourceManager Web界面中可用。Web界面上还会显示自节点运行状况良好以来的时间。

以下参数可用于控制etc / hadoop / yarn-site.xml中的节点运行状况监视脚本。

参数	值	笔记
yarn.nodemanager.health-checker.script.path	节点运行状况脚本	用于检查节点的运行状况的脚本。
yarn.nodemanager.health-checker.script.opts	节点运行状况脚本选项	用于检查节点运行状况的脚本选项。
yarn.nodemanager.health-checker.interval-ms	节点运行状况脚本间隔	运行状况脚本的时间间隔。
yarn.nodemanager.health-checker.script.timeout-ms	节点运行状况脚本超时间隔	运行状况脚本执行超时。
如果仅某些本地磁盘变坏，则运行状况检查器脚本不应给出错误。NodeManager能够定期检查本地磁盘的运行状况（特别是检查nodemanager-local-dirs和nodemanager-log-dirs），并在达到错误目录数量的阈值之后，根据配置属性yarn.nodemanager设置的值.disk-health-checker.min-healthy-disks，整个节点被标记为不正常，并且此信息也发送到资源管理器。启动磁盘被搜查，或者运行状况检查程序脚本标识了启动磁盘中的故障。